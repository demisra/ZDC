{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmisra/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uproot as ur\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for (path, dirnames, filenames) in os.walk('/mnt/scratch3/dmisra/zdcdata_current/'):\n",
    "    paths.extend(os.path.join(path, name) for name in filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "for path in paths:\n",
    "    with ur.open(path) as file:\n",
    "       tree = file[\"events\"]\n",
    "       samples[os.path.basename(f'{path}')] = tree.arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitExtract(n, k, p):  \n",
    "    return (((1 << k) - 1)  &  (n >> (p-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Energy Deposition in All ZDC Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_edep(data, count):\n",
    "    SiPix_edep = []\n",
    "    Crystal_edep = []\n",
    "    WSi_edep = []\n",
    "    PbSi_edep = []\n",
    "    PbScint_edep = []\n",
    "    energylabels = []\n",
    "    \n",
    "    for i in range(count):\n",
    "        SiPix_energies = np.array(data[\"ZDC_SiliconPix_Hits.energy\"][i])\n",
    "        SiPix_edep.append(sum(SiPix_energies))\n",
    "\n",
    "        Crystal_energies = np.array(data[\"ZDCEcalHits.energy\"][i])\n",
    "        Crystal_edep.append(sum(Crystal_energies))\n",
    "\n",
    "        WSi_energies = np.array(data[\"ZDC_WSi_Hits.energy\"][i])\n",
    "        WSi_edep.append(sum(WSi_energies))\n",
    "\n",
    "        PbSi_energies = np.array(data[\"ZDC_PbSi_Hits.energy\"][i])\n",
    "        PbSi_edep.append(sum(PbSi_energies))\n",
    "\n",
    "        PbScint_energies = np.array(data[\"ZDCHcalHits.energy\"][i])\n",
    "        PbScint_edep.append(sum(PbScint_energies))\n",
    "\n",
    "        label = np.sqrt(data[\"MCParticles.momentum.x\"][0,0]**2 + data[\"MCParticles.momentum.y\"][0,0]**2 + data[\"MCParticles.momentum.z\"][0,0]**2)\n",
    "        energylabels.append(label)\n",
    "\n",
    "    return pd.DataFrame([SiPix_edep, Crystal_edep, WSi_edep, PbSi_edep, PbScint_edep, energylabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for linear regression\n",
    "data = [components_edep(samples[key],10000) for key in samples]\n",
    "data_df = pd.concat(data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe4bf534a10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor_10GeV = torch.from_numpy(data[3].values).T.float()\n",
    "data_tensor_50GeV = torch.from_numpy(data[0].values).T.float()\n",
    "data_tensor_100GeV = torch.from_numpy(data[1].values).T.float()\n",
    "data_tensor_200GeV = torch.from_numpy(data[2].values).T.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.from_numpy(data_df.values).T.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tensor):\n",
    "    return tensor[:,:5]\n",
    "\n",
    "def labels(tensor):\n",
    "    return tensor[:,5].unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features(data_tensor)\n",
    "y = labels(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train/test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "input_size = 5\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = LinearRegression(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_0.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.fill_(1)\n",
    "         layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MSE Train Loss: 12972.11328125 | MSE Test Loss: 11719.4755859375\n",
      "Epoch: 100 | MSE Train Loss: 3227.372802734375 | MSE Test Loss: 3177.5615234375\n",
      "Epoch: 200 | MSE Train Loss: 2208.994384765625 | MSE Test Loss: 2204.013671875\n",
      "Epoch: 300 | MSE Train Loss: 1913.6846923828125 | MSE Test Loss: 1921.810302734375\n",
      "Epoch: 400 | MSE Train Loss: 1734.6279296875 | MSE Test Loss: 1748.1256103515625\n",
      "Epoch: 500 | MSE Train Loss: 1596.8577880859375 | MSE Test Loss: 1613.15771484375\n",
      "Epoch: 600 | MSE Train Loss: 1484.8660888671875 | MSE Test Loss: 1502.8677978515625\n",
      "Epoch: 700 | MSE Train Loss: 1391.8262939453125 | MSE Test Loss: 1410.9361572265625\n",
      "Epoch: 800 | MSE Train Loss: 1313.3978271484375 | MSE Test Loss: 1333.2313232421875\n",
      "Epoch: 900 | MSE Train Loss: 1246.5269775390625 | MSE Test Loss: 1266.80810546875\n",
      "Epoch: 1000 | MSE Train Loss: 1188.9749755859375 | MSE Test Loss: 1209.4964599609375\n",
      "Epoch: 1100 | MSE Train Loss: 1139.05126953125 | MSE Test Loss: 1159.656005859375\n",
      "Epoch: 1200 | MSE Train Loss: 1095.44775390625 | MSE Test Loss: 1116.0162353515625\n",
      "Epoch: 1300 | MSE Train Loss: 1057.127197265625 | MSE Test Loss: 1077.5697021484375\n",
      "Epoch: 1400 | MSE Train Loss: 1023.2545166015625 | MSE Test Loss: 1043.50439453125\n",
      "Epoch: 1500 | MSE Train Loss: 993.14599609375 | MSE Test Loss: 1013.155029296875\n",
      "Epoch: 1600 | MSE Train Loss: 966.236328125 | MSE Test Loss: 985.9697265625\n",
      "Epoch: 1700 | MSE Train Loss: 942.0545043945312 | MSE Test Loss: 961.4886474609375\n",
      "Epoch: 1800 | MSE Train Loss: 920.2040405273438 | MSE Test Loss: 939.3235473632812\n",
      "Epoch: 1900 | MSE Train Loss: 900.3518676757812 | MSE Test Loss: 919.1472778320312\n",
      "Epoch: 2000 | MSE Train Loss: 882.215087890625 | MSE Test Loss: 900.6817626953125\n",
      "Epoch: 2100 | MSE Train Loss: 865.554443359375 | MSE Test Loss: 883.691162109375\n",
      "Epoch: 2200 | MSE Train Loss: 850.1654663085938 | MSE Test Loss: 867.9736328125\n",
      "Epoch: 2300 | MSE Train Loss: 835.874755859375 | MSE Test Loss: 853.3577880859375\n",
      "Epoch: 2400 | MSE Train Loss: 822.5350341796875 | MSE Test Loss: 839.6972045898438\n",
      "Epoch: 2500 | MSE Train Loss: 810.0200805664062 | MSE Test Loss: 826.8666381835938\n",
      "Epoch: 2600 | MSE Train Loss: 798.22216796875 | MSE Test Loss: 814.7591552734375\n",
      "Epoch: 2700 | MSE Train Loss: 787.0494995117188 | MSE Test Loss: 803.2828979492188\n",
      "Epoch: 2800 | MSE Train Loss: 776.4237670898438 | MSE Test Loss: 792.3599853515625\n",
      "Epoch: 2900 | MSE Train Loss: 766.2776489257812 | MSE Test Loss: 781.9229736328125\n",
      "Epoch: 3000 | MSE Train Loss: 756.5540161132812 | MSE Test Loss: 771.914794921875\n",
      "Epoch: 3100 | MSE Train Loss: 747.2034912109375 | MSE Test Loss: 762.2860107421875\n",
      "Epoch: 3200 | MSE Train Loss: 738.1845092773438 | MSE Test Loss: 752.9945678710938\n",
      "Epoch: 3300 | MSE Train Loss: 729.4607543945312 | MSE Test Loss: 744.0043334960938\n",
      "Epoch: 3400 | MSE Train Loss: 721.0014038085938 | MSE Test Loss: 735.2838745117188\n",
      "Epoch: 3500 | MSE Train Loss: 712.7800903320312 | MSE Test Loss: 726.8068237304688\n",
      "Epoch: 3600 | MSE Train Loss: 704.7737426757812 | MSE Test Loss: 718.5499267578125\n",
      "Epoch: 3700 | MSE Train Loss: 696.9632568359375 | MSE Test Loss: 710.4937133789062\n",
      "Epoch: 3800 | MSE Train Loss: 689.3316040039062 | MSE Test Loss: 702.6211547851562\n",
      "Epoch: 3900 | MSE Train Loss: 681.8642578125 | MSE Test Loss: 694.917236328125\n",
      "Epoch: 4000 | MSE Train Loss: 674.5488891601562 | MSE Test Loss: 687.3695068359375\n",
      "Epoch: 4100 | MSE Train Loss: 667.3745727539062 | MSE Test Loss: 679.967041015625\n",
      "Epoch: 4200 | MSE Train Loss: 660.3319702148438 | MSE Test Loss: 672.7001342773438\n",
      "Epoch: 4300 | MSE Train Loss: 653.4130859375 | MSE Test Loss: 665.5604248046875\n",
      "Epoch: 4400 | MSE Train Loss: 646.610595703125 | MSE Test Loss: 658.5411987304688\n",
      "Epoch: 4500 | MSE Train Loss: 639.9185791015625 | MSE Test Loss: 651.6353149414062\n",
      "Epoch: 4600 | MSE Train Loss: 633.3316040039062 | MSE Test Loss: 644.8380126953125\n",
      "Epoch: 4700 | MSE Train Loss: 626.8447875976562 | MSE Test Loss: 638.1439208984375\n",
      "Epoch: 4800 | MSE Train Loss: 620.4540405273438 | MSE Test Loss: 631.5487670898438\n",
      "Epoch: 4900 | MSE Train Loss: 614.15576171875 | MSE Test Loss: 625.049072265625\n",
      "Epoch: 5000 | MSE Train Loss: 607.9464721679688 | MSE Test Loss: 618.6412963867188\n",
      "Epoch: 5100 | MSE Train Loss: 601.8235473632812 | MSE Test Loss: 612.3225708007812\n",
      "Epoch: 5200 | MSE Train Loss: 595.7843627929688 | MSE Test Loss: 606.0905151367188\n",
      "Epoch: 5300 | MSE Train Loss: 589.826416015625 | MSE Test Loss: 599.9422607421875\n",
      "Epoch: 5400 | MSE Train Loss: 583.9476928710938 | MSE Test Loss: 593.8756103515625\n",
      "Epoch: 5500 | MSE Train Loss: 578.1461181640625 | MSE Test Loss: 587.8883666992188\n",
      "Epoch: 5600 | MSE Train Loss: 572.4199829101562 | MSE Test Loss: 581.9786987304688\n",
      "Epoch: 5700 | MSE Train Loss: 566.7678833007812 | MSE Test Loss: 576.1454467773438\n",
      "Epoch: 5800 | MSE Train Loss: 561.1879272460938 | MSE Test Loss: 570.386474609375\n",
      "Epoch: 5900 | MSE Train Loss: 555.6790161132812 | MSE Test Loss: 564.700439453125\n",
      "Epoch: 6000 | MSE Train Loss: 550.2396240234375 | MSE Test Loss: 559.0864868164062\n",
      "Epoch: 6100 | MSE Train Loss: 544.8687744140625 | MSE Test Loss: 553.5425415039062\n",
      "Epoch: 6200 | MSE Train Loss: 539.5650634765625 | MSE Test Loss: 548.068359375\n",
      "Epoch: 6300 | MSE Train Loss: 534.3275146484375 | MSE Test Loss: 542.6617431640625\n",
      "Epoch: 6400 | MSE Train Loss: 529.1552124023438 | MSE Test Loss: 537.3226318359375\n",
      "Epoch: 6500 | MSE Train Loss: 524.0469970703125 | MSE Test Loss: 532.0492553710938\n",
      "Epoch: 6600 | MSE Train Loss: 519.0020141601562 | MSE Test Loss: 526.8410034179688\n",
      "Epoch: 6700 | MSE Train Loss: 514.0194091796875 | MSE Test Loss: 521.6969604492188\n",
      "Epoch: 6800 | MSE Train Loss: 509.0981750488281 | MSE Test Loss: 516.6157836914062\n",
      "Epoch: 6900 | MSE Train Loss: 504.2374267578125 | MSE Test Loss: 511.59716796875\n",
      "Epoch: 7000 | MSE Train Loss: 499.43658447265625 | MSE Test Loss: 506.6398620605469\n",
      "Epoch: 7100 | MSE Train Loss: 494.6944885253906 | MSE Test Loss: 501.742919921875\n",
      "Epoch: 7200 | MSE Train Loss: 490.01055908203125 | MSE Test Loss: 496.9060363769531\n",
      "Epoch: 7300 | MSE Train Loss: 485.3839416503906 | MSE Test Loss: 492.12799072265625\n",
      "Epoch: 7400 | MSE Train Loss: 480.81414794921875 | MSE Test Loss: 487.4081115722656\n",
      "Epoch: 7500 | MSE Train Loss: 476.3002624511719 | MSE Test Loss: 482.7459716796875\n",
      "Epoch: 7600 | MSE Train Loss: 471.84149169921875 | MSE Test Loss: 478.1404724121094\n",
      "Epoch: 7700 | MSE Train Loss: 467.4372253417969 | MSE Test Loss: 473.5907897949219\n",
      "Epoch: 7800 | MSE Train Loss: 463.0867614746094 | MSE Test Loss: 469.09649658203125\n",
      "Epoch: 7900 | MSE Train Loss: 458.7893981933594 | MSE Test Loss: 464.65679931640625\n",
      "Epoch: 8000 | MSE Train Loss: 454.5444030761719 | MSE Test Loss: 460.27093505859375\n",
      "Epoch: 8100 | MSE Train Loss: 450.35137939453125 | MSE Test Loss: 455.9383239746094\n",
      "Epoch: 8200 | MSE Train Loss: 446.2095642089844 | MSE Test Loss: 451.65850830078125\n",
      "Epoch: 8300 | MSE Train Loss: 442.1181335449219 | MSE Test Loss: 447.4302978515625\n",
      "Epoch: 8400 | MSE Train Loss: 438.0766906738281 | MSE Test Loss: 443.2534484863281\n",
      "Epoch: 8500 | MSE Train Loss: 434.0843505859375 | MSE Test Loss: 439.1272277832031\n",
      "Epoch: 8600 | MSE Train Loss: 430.1408386230469 | MSE Test Loss: 435.0510559082031\n",
      "Epoch: 8700 | MSE Train Loss: 426.2453308105469 | MSE Test Loss: 431.0242614746094\n",
      "Epoch: 8800 | MSE Train Loss: 422.3973693847656 | MSE Test Loss: 427.04620361328125\n",
      "Epoch: 8900 | MSE Train Loss: 418.5963134765625 | MSE Test Loss: 423.11639404296875\n",
      "Epoch: 9000 | MSE Train Loss: 414.8415832519531 | MSE Test Loss: 419.2342224121094\n",
      "Epoch: 9100 | MSE Train Loss: 411.13262939453125 | MSE Test Loss: 415.3989562988281\n",
      "Epoch: 9200 | MSE Train Loss: 407.46893310546875 | MSE Test Loss: 411.6102600097656\n",
      "Epoch: 9300 | MSE Train Loss: 403.84967041015625 | MSE Test Loss: 407.8673400878906\n",
      "Epoch: 9400 | MSE Train Loss: 400.2746887207031 | MSE Test Loss: 404.16973876953125\n",
      "Epoch: 9500 | MSE Train Loss: 396.7432556152344 | MSE Test Loss: 400.5169372558594\n",
      "Epoch: 9600 | MSE Train Loss: 393.25469970703125 | MSE Test Loss: 396.9083251953125\n",
      "Epoch: 9700 | MSE Train Loss: 389.8087158203125 | MSE Test Loss: 393.3432922363281\n",
      "Epoch: 9800 | MSE Train Loss: 386.40484619140625 | MSE Test Loss: 389.8215026855469\n",
      "Epoch: 9900 | MSE Train Loss: 383.04229736328125 | MSE Test Loss: 386.34234619140625\n",
      "Epoch: 10000 | MSE Train Loss: 379.7208251953125 | MSE Test Loss: 382.90533447265625\n",
      "Epoch: 10100 | MSE Train Loss: 376.4398193359375 | MSE Test Loss: 379.5096740722656\n",
      "Epoch: 10200 | MSE Train Loss: 373.19873046875 | MSE Test Loss: 376.1554260253906\n",
      "Epoch: 10300 | MSE Train Loss: 369.9971618652344 | MSE Test Loss: 372.84161376953125\n",
      "Epoch: 10400 | MSE Train Loss: 366.8345642089844 | MSE Test Loss: 369.56781005859375\n",
      "Epoch: 10500 | MSE Train Loss: 363.7105407714844 | MSE Test Loss: 366.3338623046875\n",
      "Epoch: 10600 | MSE Train Loss: 360.6246032714844 | MSE Test Loss: 363.13885498046875\n",
      "Epoch: 10700 | MSE Train Loss: 357.57611083984375 | MSE Test Loss: 359.98223876953125\n",
      "Epoch: 10800 | MSE Train Loss: 354.56494140625 | MSE Test Loss: 356.8642272949219\n",
      "Epoch: 10900 | MSE Train Loss: 351.5903625488281 | MSE Test Loss: 353.78369140625\n",
      "Epoch: 11000 | MSE Train Loss: 348.6520690917969 | MSE Test Loss: 350.74041748046875\n",
      "Epoch: 11100 | MSE Train Loss: 345.7496032714844 | MSE Test Loss: 347.734130859375\n",
      "Epoch: 11200 | MSE Train Loss: 342.88238525390625 | MSE Test Loss: 344.76397705078125\n",
      "Epoch: 11300 | MSE Train Loss: 340.0499267578125 | MSE Test Loss: 341.8296203613281\n",
      "Epoch: 11400 | MSE Train Loss: 337.2523498535156 | MSE Test Loss: 338.9310607910156\n",
      "Epoch: 11500 | MSE Train Loss: 334.4886474609375 | MSE Test Loss: 336.0674133300781\n",
      "Epoch: 11600 | MSE Train Loss: 331.7586364746094 | MSE Test Loss: 333.2383117675781\n",
      "Epoch: 11700 | MSE Train Loss: 329.0619812011719 | MSE Test Loss: 330.443603515625\n",
      "Epoch: 11800 | MSE Train Loss: 326.3980712890625 | MSE Test Loss: 327.6824951171875\n",
      "Epoch: 11900 | MSE Train Loss: 323.7666931152344 | MSE Test Loss: 324.954833984375\n",
      "Epoch: 12000 | MSE Train Loss: 321.1673583984375 | MSE Test Loss: 322.2601623535156\n",
      "Epoch: 12100 | MSE Train Loss: 318.5994873046875 | MSE Test Loss: 319.5978088378906\n",
      "Epoch: 12200 | MSE Train Loss: 316.063232421875 | MSE Test Loss: 316.9681396484375\n",
      "Epoch: 12300 | MSE Train Loss: 313.55767822265625 | MSE Test Loss: 314.3697509765625\n",
      "Epoch: 12400 | MSE Train Loss: 311.082763671875 | MSE Test Loss: 311.80316162109375\n",
      "Epoch: 12500 | MSE Train Loss: 308.6379089355469 | MSE Test Loss: 309.267333984375\n",
      "Epoch: 12600 | MSE Train Loss: 306.22283935546875 | MSE Test Loss: 306.7620849609375\n",
      "Epoch: 12700 | MSE Train Loss: 303.8373107910156 | MSE Test Loss: 304.2874450683594\n",
      "Epoch: 12800 | MSE Train Loss: 301.4806823730469 | MSE Test Loss: 301.8424377441406\n",
      "Epoch: 12900 | MSE Train Loss: 299.15283203125 | MSE Test Loss: 299.427001953125\n",
      "Epoch: 13000 | MSE Train Loss: 296.8533630371094 | MSE Test Loss: 297.0408630371094\n",
      "Epoch: 13100 | MSE Train Loss: 294.58184814453125 | MSE Test Loss: 294.683349609375\n",
      "Epoch: 13200 | MSE Train Loss: 292.3380432128906 | MSE Test Loss: 292.3545227050781\n",
      "Epoch: 13300 | MSE Train Loss: 290.1216125488281 | MSE Test Loss: 290.0536193847656\n",
      "Epoch: 13400 | MSE Train Loss: 287.9321594238281 | MSE Test Loss: 287.78076171875\n",
      "Epoch: 13500 | MSE Train Loss: 285.76934814453125 | MSE Test Loss: 285.53515625\n",
      "Epoch: 13600 | MSE Train Loss: 283.6329345703125 | MSE Test Loss: 283.3167724609375\n",
      "Epoch: 13700 | MSE Train Loss: 281.5224914550781 | MSE Test Loss: 281.1251220703125\n",
      "Epoch: 13800 | MSE Train Loss: 279.4377746582031 | MSE Test Loss: 278.9598693847656\n",
      "Epoch: 13900 | MSE Train Loss: 277.3785095214844 | MSE Test Loss: 276.8208923339844\n",
      "Epoch: 14000 | MSE Train Loss: 275.34417724609375 | MSE Test Loss: 274.70758056640625\n",
      "Epoch: 14100 | MSE Train Loss: 273.33489990234375 | MSE Test Loss: 272.6200866699219\n",
      "Epoch: 14200 | MSE Train Loss: 271.3499450683594 | MSE Test Loss: 270.5575256347656\n",
      "Epoch: 14300 | MSE Train Loss: 269.38916015625 | MSE Test Loss: 268.52008056640625\n",
      "Epoch: 14400 | MSE Train Loss: 267.4523010253906 | MSE Test Loss: 266.5069885253906\n",
      "Epoch: 14500 | MSE Train Loss: 265.53900146484375 | MSE Test Loss: 264.51837158203125\n",
      "Epoch: 14600 | MSE Train Loss: 263.6490173339844 | MSE Test Loss: 262.5536804199219\n",
      "Epoch: 14700 | MSE Train Loss: 261.7820129394531 | MSE Test Loss: 260.6126708984375\n",
      "Epoch: 14800 | MSE Train Loss: 259.937744140625 | MSE Test Loss: 258.6950988769531\n",
      "Epoch: 14900 | MSE Train Loss: 258.1159973144531 | MSE Test Loss: 256.8006286621094\n",
      "Epoch: 15000 | MSE Train Loss: 256.3164978027344 | MSE Test Loss: 254.9291229248047\n",
      "Epoch: 15100 | MSE Train Loss: 254.53890991210938 | MSE Test Loss: 253.0802459716797\n",
      "Epoch: 15200 | MSE Train Loss: 252.7829132080078 | MSE Test Loss: 251.2536163330078\n",
      "Epoch: 15300 | MSE Train Loss: 251.0482177734375 | MSE Test Loss: 249.4488067626953\n",
      "Epoch: 15400 | MSE Train Loss: 249.33489990234375 | MSE Test Loss: 247.66612243652344\n",
      "Epoch: 15500 | MSE Train Loss: 247.6422882080078 | MSE Test Loss: 245.90475463867188\n",
      "Epoch: 15600 | MSE Train Loss: 245.9702911376953 | MSE Test Loss: 244.1646728515625\n",
      "Epoch: 15700 | MSE Train Loss: 244.31875610351562 | MSE Test Loss: 242.44557189941406\n",
      "Epoch: 15800 | MSE Train Loss: 242.687255859375 | MSE Test Loss: 240.74725341796875\n",
      "Epoch: 15900 | MSE Train Loss: 241.07554626464844 | MSE Test Loss: 239.06924438476562\n",
      "Epoch: 16000 | MSE Train Loss: 239.4834442138672 | MSE Test Loss: 237.41146850585938\n",
      "Epoch: 16100 | MSE Train Loss: 237.91114807128906 | MSE Test Loss: 235.77401733398438\n",
      "Epoch: 16200 | MSE Train Loss: 236.35768127441406 | MSE Test Loss: 234.15621948242188\n",
      "Epoch: 16300 | MSE Train Loss: 234.82315063476562 | MSE Test Loss: 232.55776977539062\n",
      "Epoch: 16400 | MSE Train Loss: 233.3072509765625 | MSE Test Loss: 230.97842407226562\n",
      "Epoch: 16500 | MSE Train Loss: 231.81021118164062 | MSE Test Loss: 229.418701171875\n",
      "Epoch: 16600 | MSE Train Loss: 230.3311309814453 | MSE Test Loss: 227.87741088867188\n",
      "Epoch: 16700 | MSE Train Loss: 228.86985778808594 | MSE Test Loss: 226.3545684814453\n",
      "Epoch: 16800 | MSE Train Loss: 227.4266815185547 | MSE Test Loss: 224.85025024414062\n",
      "Epoch: 16900 | MSE Train Loss: 226.0011444091797 | MSE Test Loss: 223.36424255371094\n",
      "Epoch: 17000 | MSE Train Loss: 224.5925750732422 | MSE Test Loss: 221.89561462402344\n",
      "Epoch: 17100 | MSE Train Loss: 223.20152282714844 | MSE Test Loss: 220.44517517089844\n",
      "Epoch: 17200 | MSE Train Loss: 221.82749938964844 | MSE Test Loss: 219.01214599609375\n",
      "Epoch: 17300 | MSE Train Loss: 220.46974182128906 | MSE Test Loss: 217.59616088867188\n",
      "Epoch: 17400 | MSE Train Loss: 219.12892150878906 | MSE Test Loss: 216.1974334716797\n",
      "Epoch: 17500 | MSE Train Loss: 217.80442810058594 | MSE Test Loss: 214.81558227539062\n",
      "Epoch: 17600 | MSE Train Loss: 216.4956512451172 | MSE Test Loss: 213.45001220703125\n",
      "Epoch: 17700 | MSE Train Loss: 215.20347595214844 | MSE Test Loss: 212.10150146484375\n",
      "Epoch: 17800 | MSE Train Loss: 213.9266357421875 | MSE Test Loss: 210.76895141601562\n",
      "Epoch: 17900 | MSE Train Loss: 212.66543579101562 | MSE Test Loss: 209.4522705078125\n",
      "Epoch: 18000 | MSE Train Loss: 211.41969299316406 | MSE Test Loss: 208.15187072753906\n",
      "Epoch: 18100 | MSE Train Loss: 210.18881225585938 | MSE Test Loss: 206.8665313720703\n",
      "Epoch: 18200 | MSE Train Loss: 208.97335815429688 | MSE Test Loss: 205.59735107421875\n",
      "Epoch: 18300 | MSE Train Loss: 207.77210998535156 | MSE Test Loss: 204.3426971435547\n",
      "Epoch: 18400 | MSE Train Loss: 206.58628845214844 | MSE Test Loss: 203.10394287109375\n",
      "Epoch: 18500 | MSE Train Loss: 205.41416931152344 | MSE Test Loss: 201.8794708251953\n",
      "Epoch: 18600 | MSE Train Loss: 204.25694274902344 | MSE Test Loss: 200.670166015625\n",
      "Epoch: 18700 | MSE Train Loss: 203.11325073242188 | MSE Test Loss: 199.47506713867188\n",
      "Epoch: 18800 | MSE Train Loss: 201.98402404785156 | MSE Test Loss: 198.2947235107422\n",
      "Epoch: 18900 | MSE Train Loss: 200.86798095703125 | MSE Test Loss: 197.12811279296875\n",
      "Epoch: 19000 | MSE Train Loss: 199.7661590576172 | MSE Test Loss: 195.9761505126953\n",
      "Epoch: 19100 | MSE Train Loss: 198.6770782470703 | MSE Test Loss: 194.83721923828125\n",
      "Epoch: 19200 | MSE Train Loss: 197.60198974609375 | MSE Test Loss: 193.71299743652344\n",
      "Epoch: 19300 | MSE Train Loss: 196.53933715820312 | MSE Test Loss: 192.60145568847656\n",
      "Epoch: 19400 | MSE Train Loss: 195.49026489257812 | MSE Test Loss: 191.50399780273438\n",
      "Epoch: 19500 | MSE Train Loss: 194.4534912109375 | MSE Test Loss: 190.4193115234375\n",
      "Epoch: 19600 | MSE Train Loss: 193.42955017089844 | MSE Test Loss: 189.34768676757812\n",
      "Epoch: 19700 | MSE Train Loss: 192.41812133789062 | MSE Test Loss: 188.28915405273438\n",
      "Epoch: 19800 | MSE Train Loss: 191.4186248779297 | MSE Test Loss: 187.2429656982422\n",
      "Epoch: 19900 | MSE Train Loss: 190.43191528320312 | MSE Test Loss: 186.2098388671875\n",
      "Epoch: 20000 | MSE Train Loss: 189.45672607421875 | MSE Test Loss: 185.18875122070312\n",
      "Epoch: 20100 | MSE Train Loss: 188.4937286376953 | MSE Test Loss: 184.18019104003906\n",
      "Epoch: 20200 | MSE Train Loss: 187.5424041748047 | MSE Test Loss: 183.18377685546875\n",
      "Epoch: 20300 | MSE Train Loss: 186.60232543945312 | MSE Test Loss: 182.19895935058594\n",
      "Epoch: 20400 | MSE Train Loss: 185.6742706298828 | MSE Test Loss: 181.22645568847656\n",
      "Epoch: 20500 | MSE Train Loss: 184.75723266601562 | MSE Test Loss: 180.2655792236328\n",
      "Epoch: 20600 | MSE Train Loss: 183.85116577148438 | MSE Test Loss: 179.31590270996094\n",
      "Epoch: 20700 | MSE Train Loss: 182.95660400390625 | MSE Test Loss: 178.37806701660156\n",
      "Epoch: 20800 | MSE Train Loss: 182.0726318359375 | MSE Test Loss: 177.45140075683594\n",
      "Epoch: 20900 | MSE Train Loss: 181.1992645263672 | MSE Test Loss: 176.53546142578125\n",
      "Epoch: 21000 | MSE Train Loss: 180.33700561523438 | MSE Test Loss: 175.6311492919922\n",
      "Epoch: 21100 | MSE Train Loss: 179.48501586914062 | MSE Test Loss: 174.7374725341797\n",
      "Epoch: 21200 | MSE Train Loss: 178.64312744140625 | MSE Test Loss: 173.85409545898438\n",
      "Epoch: 21300 | MSE Train Loss: 177.81198120117188 | MSE Test Loss: 172.9820098876953\n",
      "Epoch: 21400 | MSE Train Loss: 176.9908447265625 | MSE Test Loss: 172.1201934814453\n",
      "Epoch: 21500 | MSE Train Loss: 176.17945861816406 | MSE Test Loss: 171.26837158203125\n",
      "Epoch: 21600 | MSE Train Loss: 175.37794494628906 | MSE Test Loss: 170.42703247070312\n",
      "Epoch: 21700 | MSE Train Loss: 174.58656311035156 | MSE Test Loss: 169.5960693359375\n",
      "Epoch: 21800 | MSE Train Loss: 173.80467224121094 | MSE Test Loss: 168.77471923828125\n",
      "Epoch: 21900 | MSE Train Loss: 173.0321044921875 | MSE Test Loss: 167.96331787109375\n",
      "Epoch: 22000 | MSE Train Loss: 172.26889038085938 | MSE Test Loss: 167.16151428222656\n",
      "Epoch: 22100 | MSE Train Loss: 171.51539611816406 | MSE Test Loss: 166.36953735351562\n",
      "Epoch: 22200 | MSE Train Loss: 170.77093505859375 | MSE Test Loss: 165.58729553222656\n",
      "Epoch: 22300 | MSE Train Loss: 170.0354461669922 | MSE Test Loss: 164.81411743164062\n",
      "Epoch: 22400 | MSE Train Loss: 169.3087158203125 | MSE Test Loss: 164.04994201660156\n",
      "Epoch: 22500 | MSE Train Loss: 168.59103393554688 | MSE Test Loss: 163.2954864501953\n",
      "Epoch: 22600 | MSE Train Loss: 167.88235473632812 | MSE Test Loss: 162.55006408691406\n",
      "Epoch: 22700 | MSE Train Loss: 167.18214416503906 | MSE Test Loss: 161.81332397460938\n",
      "Epoch: 22800 | MSE Train Loss: 166.49038696289062 | MSE Test Loss: 161.08566284179688\n",
      "Epoch: 22900 | MSE Train Loss: 165.80697631835938 | MSE Test Loss: 160.36648559570312\n",
      "Epoch: 23000 | MSE Train Loss: 165.13182067871094 | MSE Test Loss: 159.6557159423828\n",
      "Epoch: 23100 | MSE Train Loss: 164.4651336669922 | MSE Test Loss: 158.9540252685547\n",
      "Epoch: 23200 | MSE Train Loss: 163.8065948486328 | MSE Test Loss: 158.26063537597656\n",
      "Epoch: 23300 | MSE Train Loss: 163.15606689453125 | MSE Test Loss: 157.57542419433594\n",
      "Epoch: 23400 | MSE Train Loss: 162.51333618164062 | MSE Test Loss: 156.8984832763672\n",
      "Epoch: 23500 | MSE Train Loss: 161.87847900390625 | MSE Test Loss: 156.22964477539062\n",
      "Epoch: 23600 | MSE Train Loss: 161.2512664794922 | MSE Test Loss: 155.5686492919922\n",
      "Epoch: 23700 | MSE Train Loss: 160.63168334960938 | MSE Test Loss: 154.91561889648438\n",
      "Epoch: 23800 | MSE Train Loss: 160.0196075439453 | MSE Test Loss: 154.27049255371094\n",
      "Epoch: 23900 | MSE Train Loss: 159.41522216796875 | MSE Test Loss: 153.6332244873047\n",
      "Epoch: 24000 | MSE Train Loss: 158.8181610107422 | MSE Test Loss: 153.0034637451172\n",
      "Epoch: 24100 | MSE Train Loss: 158.22837829589844 | MSE Test Loss: 152.38148498535156\n",
      "Epoch: 24200 | MSE Train Loss: 157.64578247070312 | MSE Test Loss: 151.76683044433594\n",
      "Epoch: 24300 | MSE Train Loss: 157.07022094726562 | MSE Test Loss: 151.15940856933594\n",
      "Epoch: 24400 | MSE Train Loss: 156.50169372558594 | MSE Test Loss: 150.55938720703125\n",
      "Epoch: 24500 | MSE Train Loss: 155.94004821777344 | MSE Test Loss: 149.9666290283203\n",
      "Epoch: 24600 | MSE Train Loss: 155.38526916503906 | MSE Test Loss: 149.3807373046875\n",
      "Epoch: 24700 | MSE Train Loss: 154.83718872070312 | MSE Test Loss: 148.80184936523438\n",
      "Epoch: 24800 | MSE Train Loss: 154.2958526611328 | MSE Test Loss: 148.2301788330078\n",
      "Epoch: 24900 | MSE Train Loss: 153.76109313964844 | MSE Test Loss: 147.66513061523438\n",
      "Epoch: 25000 | MSE Train Loss: 153.23281860351562 | MSE Test Loss: 147.10682678222656\n",
      "Epoch: 25100 | MSE Train Loss: 152.71099853515625 | MSE Test Loss: 146.55531311035156\n",
      "Epoch: 25200 | MSE Train Loss: 152.19552612304688 | MSE Test Loss: 146.01039123535156\n",
      "Epoch: 25300 | MSE Train Loss: 151.68634033203125 | MSE Test Loss: 145.47190856933594\n",
      "Epoch: 25400 | MSE Train Loss: 151.18338012695312 | MSE Test Loss: 144.9398651123047\n",
      "Epoch: 25500 | MSE Train Loss: 150.68653869628906 | MSE Test Loss: 144.4143524169922\n",
      "Epoch: 25600 | MSE Train Loss: 150.19577026367188 | MSE Test Loss: 143.8950653076172\n",
      "Epoch: 25700 | MSE Train Loss: 149.71099853515625 | MSE Test Loss: 143.38197326660156\n",
      "Epoch: 25800 | MSE Train Loss: 149.23204040527344 | MSE Test Loss: 142.87493896484375\n",
      "Epoch: 25900 | MSE Train Loss: 148.759033203125 | MSE Test Loss: 142.37416076660156\n",
      "Epoch: 26000 | MSE Train Loss: 148.29173278808594 | MSE Test Loss: 141.8793182373047\n",
      "Epoch: 26100 | MSE Train Loss: 147.8301544189453 | MSE Test Loss: 141.3902587890625\n",
      "Epoch: 26200 | MSE Train Loss: 147.37416076660156 | MSE Test Loss: 140.90708923339844\n",
      "Epoch: 26300 | MSE Train Loss: 146.9237518310547 | MSE Test Loss: 140.42982482910156\n",
      "Epoch: 26400 | MSE Train Loss: 146.47877502441406 | MSE Test Loss: 139.9582061767578\n",
      "Epoch: 26500 | MSE Train Loss: 146.0392303466797 | MSE Test Loss: 139.4920196533203\n",
      "Epoch: 26600 | MSE Train Loss: 145.6050262451172 | MSE Test Loss: 139.03155517578125\n",
      "Epoch: 26700 | MSE Train Loss: 145.1760711669922 | MSE Test Loss: 138.57669067382812\n",
      "Epoch: 26800 | MSE Train Loss: 144.75233459472656 | MSE Test Loss: 138.12709045410156\n",
      "Epoch: 26900 | MSE Train Loss: 144.33367919921875 | MSE Test Loss: 137.6826934814453\n",
      "Epoch: 27000 | MSE Train Loss: 143.92037963867188 | MSE Test Loss: 137.24395751953125\n",
      "Epoch: 27100 | MSE Train Loss: 143.51214599609375 | MSE Test Loss: 136.81056213378906\n",
      "Epoch: 27200 | MSE Train Loss: 143.1088409423828 | MSE Test Loss: 136.38221740722656\n",
      "Epoch: 27300 | MSE Train Loss: 142.71038818359375 | MSE Test Loss: 135.95887756347656\n",
      "Epoch: 27400 | MSE Train Loss: 142.3168487548828 | MSE Test Loss: 135.54071044921875\n",
      "Epoch: 27500 | MSE Train Loss: 141.92799377441406 | MSE Test Loss: 135.1275634765625\n",
      "Epoch: 27600 | MSE Train Loss: 141.54380798339844 | MSE Test Loss: 134.71913146972656\n",
      "Epoch: 27700 | MSE Train Loss: 141.16432189941406 | MSE Test Loss: 134.31553649902344\n",
      "Epoch: 27800 | MSE Train Loss: 140.7893524169922 | MSE Test Loss: 133.91671752929688\n",
      "Epoch: 27900 | MSE Train Loss: 140.41932678222656 | MSE Test Loss: 133.52313232421875\n",
      "Epoch: 28000 | MSE Train Loss: 140.0537567138672 | MSE Test Loss: 133.1341094970703\n",
      "Epoch: 28100 | MSE Train Loss: 139.6925811767578 | MSE Test Loss: 132.74960327148438\n",
      "Epoch: 28200 | MSE Train Loss: 139.33572387695312 | MSE Test Loss: 132.3695831298828\n",
      "Epoch: 28300 | MSE Train Loss: 138.9831085205078 | MSE Test Loss: 131.99420166015625\n",
      "Epoch: 28400 | MSE Train Loss: 138.6347198486328 | MSE Test Loss: 131.62319946289062\n",
      "Epoch: 28500 | MSE Train Loss: 138.2908172607422 | MSE Test Loss: 131.25665283203125\n",
      "Epoch: 28600 | MSE Train Loss: 137.95118713378906 | MSE Test Loss: 130.89462280273438\n",
      "Epoch: 28700 | MSE Train Loss: 137.61563110351562 | MSE Test Loss: 130.5369110107422\n",
      "Epoch: 28800 | MSE Train Loss: 137.28407287597656 | MSE Test Loss: 130.1834716796875\n",
      "Epoch: 28900 | MSE Train Loss: 136.9564208984375 | MSE Test Loss: 129.83401489257812\n",
      "Epoch: 29000 | MSE Train Loss: 136.63275146484375 | MSE Test Loss: 129.48858642578125\n",
      "Epoch: 29100 | MSE Train Loss: 136.31336975097656 | MSE Test Loss: 129.1476287841797\n",
      "Epoch: 29200 | MSE Train Loss: 135.99778747558594 | MSE Test Loss: 128.81089782714844\n",
      "Epoch: 29300 | MSE Train Loss: 135.68592834472656 | MSE Test Loss: 128.47796630859375\n",
      "Epoch: 29400 | MSE Train Loss: 135.37770080566406 | MSE Test Loss: 128.1487274169922\n",
      "Epoch: 29500 | MSE Train Loss: 135.07337951660156 | MSE Test Loss: 127.82344818115234\n",
      "Epoch: 29600 | MSE Train Loss: 134.77297973632812 | MSE Test Loss: 127.50247192382812\n",
      "Epoch: 29700 | MSE Train Loss: 134.4761505126953 | MSE Test Loss: 127.18524932861328\n",
      "Epoch: 29800 | MSE Train Loss: 134.18272399902344 | MSE Test Loss: 126.87152862548828\n",
      "Epoch: 29900 | MSE Train Loss: 133.89276123046875 | MSE Test Loss: 126.56134796142578\n",
      "Epoch: 30000 | MSE Train Loss: 133.60684204101562 | MSE Test Loss: 126.25536346435547\n",
      "Epoch: 30100 | MSE Train Loss: 133.32421875 | MSE Test Loss: 125.95297241210938\n",
      "Epoch: 30200 | MSE Train Loss: 133.04483032226562 | MSE Test Loss: 125.65398406982422\n",
      "Epoch: 30300 | MSE Train Loss: 132.76878356933594 | MSE Test Loss: 125.35832977294922\n",
      "Epoch: 30400 | MSE Train Loss: 132.4965362548828 | MSE Test Loss: 125.06671905517578\n",
      "Epoch: 30500 | MSE Train Loss: 132.22738647460938 | MSE Test Loss: 124.77825927734375\n",
      "Epoch: 30600 | MSE Train Loss: 131.96127319335938 | MSE Test Loss: 124.49317169189453\n",
      "Epoch: 30700 | MSE Train Loss: 131.69857788085938 | MSE Test Loss: 124.21156311035156\n",
      "Epoch: 30800 | MSE Train Loss: 131.43931579589844 | MSE Test Loss: 123.93352508544922\n",
      "Epoch: 30900 | MSE Train Loss: 131.18296813964844 | MSE Test Loss: 123.65857696533203\n",
      "Epoch: 31000 | MSE Train Loss: 130.9294891357422 | MSE Test Loss: 123.3864974975586\n",
      "Epoch: 31100 | MSE Train Loss: 130.67965698242188 | MSE Test Loss: 123.11837768554688\n",
      "Epoch: 31200 | MSE Train Loss: 130.4326629638672 | MSE Test Loss: 122.85323333740234\n",
      "Epoch: 31300 | MSE Train Loss: 130.1884307861328 | MSE Test Loss: 122.59100341796875\n",
      "Epoch: 31400 | MSE Train Loss: 129.9474334716797 | MSE Test Loss: 122.33206176757812\n",
      "Epoch: 31500 | MSE Train Loss: 129.70944213867188 | MSE Test Loss: 122.07621002197266\n",
      "Epoch: 31600 | MSE Train Loss: 129.47402954101562 | MSE Test Loss: 121.82313537597656\n",
      "Epoch: 31700 | MSE Train Loss: 129.24160766601562 | MSE Test Loss: 121.57330322265625\n",
      "Epoch: 31800 | MSE Train Loss: 129.0122833251953 | MSE Test Loss: 121.32669830322266\n",
      "Epoch: 31900 | MSE Train Loss: 128.7854461669922 | MSE Test Loss: 121.08260345458984\n",
      "Epoch: 32000 | MSE Train Loss: 128.56137084960938 | MSE Test Loss: 120.84130096435547\n",
      "Epoch: 32100 | MSE Train Loss: 128.34030151367188 | MSE Test Loss: 120.60328674316406\n",
      "Epoch: 32200 | MSE Train Loss: 128.12164306640625 | MSE Test Loss: 120.3678207397461\n",
      "Epoch: 32300 | MSE Train Loss: 127.90569305419922 | MSE Test Loss: 120.13509368896484\n",
      "Epoch: 32400 | MSE Train Loss: 127.6926498413086 | MSE Test Loss: 119.90550231933594\n",
      "Epoch: 32500 | MSE Train Loss: 127.48177337646484 | MSE Test Loss: 119.67814636230469\n",
      "Epoch: 32600 | MSE Train Loss: 127.2737045288086 | MSE Test Loss: 119.4537353515625\n",
      "Epoch: 32700 | MSE Train Loss: 127.06829833984375 | MSE Test Loss: 119.23218536376953\n",
      "Epoch: 32800 | MSE Train Loss: 126.8649673461914 | MSE Test Loss: 119.01278686523438\n",
      "Epoch: 32900 | MSE Train Loss: 126.66456604003906 | MSE Test Loss: 118.79643249511719\n",
      "Epoch: 33000 | MSE Train Loss: 126.46646881103516 | MSE Test Loss: 118.58247375488281\n",
      "Epoch: 33100 | MSE Train Loss: 126.27044677734375 | MSE Test Loss: 118.37069702148438\n",
      "Epoch: 33200 | MSE Train Loss: 126.07743072509766 | MSE Test Loss: 118.16216278076172\n",
      "Epoch: 33300 | MSE Train Loss: 125.88630676269531 | MSE Test Loss: 117.95559692382812\n",
      "Epoch: 33400 | MSE Train Loss: 125.69762420654297 | MSE Test Loss: 117.75150299072266\n",
      "Epoch: 33500 | MSE Train Loss: 125.51139068603516 | MSE Test Loss: 117.55005645751953\n",
      "Epoch: 33600 | MSE Train Loss: 125.32697296142578 | MSE Test Loss: 117.35047912597656\n",
      "Epoch: 33700 | MSE Train Loss: 125.14542388916016 | MSE Test Loss: 117.15394592285156\n",
      "Epoch: 33800 | MSE Train Loss: 124.96573638916016 | MSE Test Loss: 116.95940399169922\n",
      "Epoch: 33900 | MSE Train Loss: 124.7882308959961 | MSE Test Loss: 116.76715087890625\n",
      "Epoch: 34000 | MSE Train Loss: 124.61307525634766 | MSE Test Loss: 116.5772705078125\n",
      "Epoch: 34100 | MSE Train Loss: 124.43954467773438 | MSE Test Loss: 116.38922119140625\n",
      "Epoch: 34200 | MSE Train Loss: 124.2688980102539 | MSE Test Loss: 116.2041244506836\n",
      "Epoch: 34300 | MSE Train Loss: 124.09980010986328 | MSE Test Loss: 116.02068328857422\n",
      "Epoch: 34400 | MSE Train Loss: 123.9329833984375 | MSE Test Loss: 115.83965301513672\n",
      "Epoch: 34500 | MSE Train Loss: 123.76813507080078 | MSE Test Loss: 115.66072082519531\n",
      "Epoch: 34600 | MSE Train Loss: 123.60511016845703 | MSE Test Loss: 115.4836196899414\n",
      "Epoch: 34700 | MSE Train Loss: 123.44445037841797 | MSE Test Loss: 115.3090591430664\n",
      "Epoch: 34800 | MSE Train Loss: 123.28518676757812 | MSE Test Loss: 115.13606262207031\n",
      "Epoch: 34900 | MSE Train Loss: 123.1285629272461 | MSE Test Loss: 114.9657211303711\n",
      "Epoch: 35000 | MSE Train Loss: 122.9732894897461 | MSE Test Loss: 114.79682922363281\n",
      "Epoch: 35100 | MSE Train Loss: 122.82028198242188 | MSE Test Loss: 114.63033294677734\n",
      "Epoch: 35200 | MSE Train Loss: 122.66887664794922 | MSE Test Loss: 114.46553039550781\n",
      "Epoch: 35300 | MSE Train Loss: 122.51942443847656 | MSE Test Loss: 114.30274963378906\n",
      "Epoch: 35400 | MSE Train Loss: 122.3718490600586 | MSE Test Loss: 114.14204406738281\n",
      "Epoch: 35500 | MSE Train Loss: 122.22587585449219 | MSE Test Loss: 113.98296356201172\n",
      "Epoch: 35600 | MSE Train Loss: 122.08196258544922 | MSE Test Loss: 113.82605743408203\n",
      "Epoch: 35700 | MSE Train Loss: 121.93941497802734 | MSE Test Loss: 113.67056274414062\n",
      "Epoch: 35800 | MSE Train Loss: 121.79906463623047 | MSE Test Loss: 113.5174560546875\n",
      "Epoch: 35900 | MSE Train Loss: 121.65987396240234 | MSE Test Loss: 113.36556243896484\n",
      "Epoch: 36000 | MSE Train Loss: 121.52303314208984 | MSE Test Loss: 113.21611022949219\n",
      "Epoch: 36100 | MSE Train Loss: 121.38722229003906 | MSE Test Loss: 113.06780242919922\n",
      "Epoch: 36200 | MSE Train Loss: 121.2536849975586 | MSE Test Loss: 112.92186737060547\n",
      "Epoch: 36300 | MSE Train Loss: 121.12114715576172 | MSE Test Loss: 112.77693176269531\n",
      "Epoch: 36400 | MSE Train Loss: 120.9908447265625 | MSE Test Loss: 112.63436126708984\n",
      "Epoch: 36500 | MSE Train Loss: 120.86153411865234 | MSE Test Loss: 112.49299621582031\n",
      "Epoch: 36600 | MSE Train Loss: 120.734375 | MSE Test Loss: 112.353759765625\n",
      "Epoch: 36700 | MSE Train Loss: 120.60819244384766 | MSE Test Loss: 112.2155990600586\n",
      "Epoch: 36800 | MSE Train Loss: 120.48409271240234 | MSE Test Loss: 112.07967376708984\n",
      "Epoch: 36900 | MSE Train Loss: 120.36099243164062 | MSE Test Loss: 111.9447250366211\n",
      "Epoch: 37000 | MSE Train Loss: 120.23992156982422 | MSE Test Loss: 111.81190490722656\n",
      "Epoch: 37100 | MSE Train Loss: 120.11970520019531 | MSE Test Loss: 111.68004608154297\n",
      "Epoch: 37200 | MSE Train Loss: 120.00162506103516 | MSE Test Loss: 111.55049896240234\n",
      "Epoch: 37300 | MSE Train Loss: 119.88428497314453 | MSE Test Loss: 111.42174530029297\n",
      "Epoch: 37400 | MSE Train Loss: 119.76913452148438 | MSE Test Loss: 111.29517364501953\n",
      "Epoch: 37500 | MSE Train Loss: 119.65464782714844 | MSE Test Loss: 111.16940307617188\n",
      "Epoch: 37600 | MSE Train Loss: 119.5422134399414 | MSE Test Loss: 111.04576873779297\n",
      "Epoch: 37700 | MSE Train Loss: 119.4305648803711 | MSE Test Loss: 110.9229736328125\n",
      "Epoch: 37800 | MSE Train Loss: 119.32079315185547 | MSE Test Loss: 110.80216979980469\n",
      "Epoch: 37900 | MSE Train Loss: 119.21194458007812 | MSE Test Loss: 110.68240356445312\n",
      "Epoch: 38000 | MSE Train Loss: 119.10475158691406 | MSE Test Loss: 110.56434631347656\n",
      "Epoch: 38100 | MSE Train Loss: 118.99864196777344 | MSE Test Loss: 110.44744110107422\n",
      "Epoch: 38200 | MSE Train Loss: 118.89385223388672 | MSE Test Loss: 110.33192443847656\n",
      "Epoch: 38300 | MSE Train Loss: 118.79045104980469 | MSE Test Loss: 110.21788024902344\n",
      "Epoch: 38400 | MSE Train Loss: 118.68807220458984 | MSE Test Loss: 110.10493469238281\n",
      "Epoch: 38500 | MSE Train Loss: 118.58731079101562 | MSE Test Loss: 109.9937515258789\n",
      "Epoch: 38600 | MSE Train Loss: 118.48725128173828 | MSE Test Loss: 109.88324737548828\n",
      "Epoch: 38700 | MSE Train Loss: 118.3890609741211 | MSE Test Loss: 109.77473449707031\n",
      "Epoch: 38800 | MSE Train Loss: 118.29149627685547 | MSE Test Loss: 109.66692352294922\n",
      "Epoch: 38900 | MSE Train Loss: 118.195556640625 | MSE Test Loss: 109.56082153320312\n",
      "Epoch: 39000 | MSE Train Loss: 118.10050201416016 | MSE Test Loss: 109.4555892944336\n",
      "Epoch: 39100 | MSE Train Loss: 118.00665283203125 | MSE Test Loss: 109.3516845703125\n",
      "Epoch: 39200 | MSE Train Loss: 117.91410064697266 | MSE Test Loss: 109.249267578125\n",
      "Epoch: 39300 | MSE Train Loss: 117.82229614257812 | MSE Test Loss: 109.14759826660156\n",
      "Epoch: 39400 | MSE Train Loss: 117.73218536376953 | MSE Test Loss: 109.04764556884766\n",
      "Epoch: 39500 | MSE Train Loss: 117.64259338378906 | MSE Test Loss: 108.94828796386719\n",
      "Epoch: 39600 | MSE Train Loss: 117.5545654296875 | MSE Test Loss: 108.85057067871094\n",
      "Epoch: 39700 | MSE Train Loss: 117.46734619140625 | MSE Test Loss: 108.75375366210938\n",
      "Epoch: 39800 | MSE Train Loss: 117.38117218017578 | MSE Test Loss: 108.65801239013672\n",
      "Epoch: 39900 | MSE Train Loss: 117.29627227783203 | MSE Test Loss: 108.56365966796875\n",
      "Epoch: 40000 | MSE Train Loss: 117.21192932128906 | MSE Test Loss: 108.46994018554688\n",
      "Epoch: 40100 | MSE Train Loss: 117.12926483154297 | MSE Test Loss: 108.37798309326172\n",
      "Epoch: 40200 | MSE Train Loss: 117.04720306396484 | MSE Test Loss: 108.2866439819336\n",
      "Epoch: 40300 | MSE Train Loss: 116.96620178222656 | MSE Test Loss: 108.19641876220703\n",
      "Epoch: 40400 | MSE Train Loss: 116.88633728027344 | MSE Test Loss: 108.10739135742188\n",
      "Epoch: 40500 | MSE Train Loss: 116.80696105957031 | MSE Test Loss: 108.01895141601562\n",
      "Epoch: 40600 | MSE Train Loss: 116.7292251586914 | MSE Test Loss: 107.93223571777344\n",
      "Epoch: 40700 | MSE Train Loss: 116.65203857421875 | MSE Test Loss: 107.84613800048828\n",
      "Epoch: 40800 | MSE Train Loss: 116.57581329345703 | MSE Test Loss: 107.76103973388672\n",
      "Epoch: 40900 | MSE Train Loss: 116.50076293945312 | MSE Test Loss: 107.67716979980469\n",
      "Epoch: 41000 | MSE Train Loss: 116.4261245727539 | MSE Test Loss: 107.59386444091797\n",
      "Epoch: 41100 | MSE Train Loss: 116.35296630859375 | MSE Test Loss: 107.51201629638672\n",
      "Epoch: 41200 | MSE Train Loss: 116.28042602539062 | MSE Test Loss: 107.43084716796875\n",
      "Epoch: 41300 | MSE Train Loss: 116.20858001708984 | MSE Test Loss: 107.350341796875\n",
      "Epoch: 41400 | MSE Train Loss: 116.1380844116211 | MSE Test Loss: 107.2713623046875\n",
      "Epoch: 41500 | MSE Train Loss: 116.06800842285156 | MSE Test Loss: 107.19292449951172\n",
      "Epoch: 41600 | MSE Train Loss: 115.9990005493164 | MSE Test Loss: 107.11557006835938\n",
      "Epoch: 41700 | MSE Train Loss: 115.93093872070312 | MSE Test Loss: 107.03916931152344\n",
      "Epoch: 41800 | MSE Train Loss: 115.8632583618164 | MSE Test Loss: 106.9632339477539\n",
      "Epoch: 41900 | MSE Train Loss: 115.79694366455078 | MSE Test Loss: 106.88867950439453\n",
      "Epoch: 42000 | MSE Train Loss: 115.73120880126953 | MSE Test Loss: 106.8148422241211\n",
      "Epoch: 42100 | MSE Train Loss: 115.66593170166016 | MSE Test Loss: 106.7414779663086\n",
      "Epoch: 42200 | MSE Train Loss: 115.60211181640625 | MSE Test Loss: 106.6695785522461\n",
      "Epoch: 42300 | MSE Train Loss: 115.5386734008789 | MSE Test Loss: 106.59812927246094\n",
      "Epoch: 42400 | MSE Train Loss: 115.47587585449219 | MSE Test Loss: 106.52745056152344\n",
      "Epoch: 42500 | MSE Train Loss: 115.41429138183594 | MSE Test Loss: 106.45807647705078\n",
      "Epoch: 42600 | MSE Train Loss: 115.35311126708984 | MSE Test Loss: 106.38912200927734\n",
      "Epoch: 42700 | MSE Train Loss: 115.29267883300781 | MSE Test Loss: 106.32093048095703\n",
      "Epoch: 42800 | MSE Train Loss: 115.23326873779297 | MSE Test Loss: 106.25379943847656\n",
      "Epoch: 42900 | MSE Train Loss: 115.17423248291016 | MSE Test Loss: 106.18712615966797\n",
      "Epoch: 43000 | MSE Train Loss: 115.11603546142578 | MSE Test Loss: 106.12134552001953\n",
      "Epoch: 43100 | MSE Train Loss: 115.05877685546875 | MSE Test Loss: 106.0565414428711\n",
      "Epoch: 43200 | MSE Train Loss: 115.00184631347656 | MSE Test Loss: 105.9920883178711\n",
      "Epoch: 43300 | MSE Train Loss: 114.9457778930664 | MSE Test Loss: 105.9286117553711\n",
      "Epoch: 43400 | MSE Train Loss: 114.89055633544922 | MSE Test Loss: 105.86604309082031\n",
      "Epoch: 43500 | MSE Train Loss: 114.83570098876953 | MSE Test Loss: 105.80393981933594\n",
      "Epoch: 43600 | MSE Train Loss: 114.78165435791016 | MSE Test Loss: 105.74261474609375\n",
      "Epoch: 43700 | MSE Train Loss: 114.72844696044922 | MSE Test Loss: 105.68220520019531\n",
      "Epoch: 43800 | MSE Train Loss: 114.67559051513672 | MSE Test Loss: 105.62214660644531\n",
      "Epoch: 43900 | MSE Train Loss: 114.62342071533203 | MSE Test Loss: 105.56279754638672\n",
      "Epoch: 44000 | MSE Train Loss: 114.57217407226562 | MSE Test Loss: 105.50450134277344\n",
      "Epoch: 44100 | MSE Train Loss: 114.52123260498047 | MSE Test Loss: 105.44654846191406\n",
      "Epoch: 44200 | MSE Train Loss: 114.47083282470703 | MSE Test Loss: 105.38921356201172\n",
      "Epoch: 44300 | MSE Train Loss: 114.42152404785156 | MSE Test Loss: 105.33300018310547\n",
      "Epoch: 44400 | MSE Train Loss: 114.3724594116211 | MSE Test Loss: 105.27710723876953\n",
      "Epoch: 44500 | MSE Train Loss: 114.32378387451172 | MSE Test Loss: 105.2215805053711\n",
      "Epoch: 44600 | MSE Train Loss: 114.27629089355469 | MSE Test Loss: 105.16729736328125\n",
      "Epoch: 44700 | MSE Train Loss: 114.22908782958984 | MSE Test Loss: 105.1134033203125\n",
      "Epoch: 44800 | MSE Train Loss: 114.18218994140625 | MSE Test Loss: 105.05982971191406\n",
      "Epoch: 44900 | MSE Train Loss: 114.13629913330078 | MSE Test Loss: 105.00724792480469\n",
      "Epoch: 45000 | MSE Train Loss: 114.09085845947266 | MSE Test Loss: 104.95521545410156\n",
      "Epoch: 45100 | MSE Train Loss: 114.04572296142578 | MSE Test Loss: 104.90349578857422\n",
      "Epoch: 45200 | MSE Train Loss: 114.00125122070312 | MSE Test Loss: 104.85252380371094\n",
      "Epoch: 45300 | MSE Train Loss: 113.95758056640625 | MSE Test Loss: 104.8024673461914\n",
      "Epoch: 45400 | MSE Train Loss: 113.9141845703125 | MSE Test Loss: 104.75270080566406\n",
      "Epoch: 45500 | MSE Train Loss: 113.87110900878906 | MSE Test Loss: 104.70327758789062\n",
      "Epoch: 45600 | MSE Train Loss: 113.82913208007812 | MSE Test Loss: 104.65496063232422\n",
      "Epoch: 45700 | MSE Train Loss: 113.7873764038086 | MSE Test Loss: 104.60691833496094\n",
      "Epoch: 45800 | MSE Train Loss: 113.74589538574219 | MSE Test Loss: 104.55917358398438\n",
      "Epoch: 45900 | MSE Train Loss: 113.70516967773438 | MSE Test Loss: 104.51228332519531\n",
      "Epoch: 46000 | MSE Train Loss: 113.66507720947266 | MSE Test Loss: 104.46604919433594\n",
      "Epoch: 46100 | MSE Train Loss: 113.62523651123047 | MSE Test Loss: 104.42008972167969\n",
      "Epoch: 46200 | MSE Train Loss: 113.58561706542969 | MSE Test Loss: 104.37437438964844\n",
      "Epoch: 46300 | MSE Train Loss: 113.547119140625 | MSE Test Loss: 104.32982635498047\n",
      "Epoch: 46400 | MSE Train Loss: 113.50884246826172 | MSE Test Loss: 104.28560638427734\n",
      "Epoch: 46500 | MSE Train Loss: 113.47078704833984 | MSE Test Loss: 104.24165344238281\n",
      "Epoch: 46600 | MSE Train Loss: 113.4333267211914 | MSE Test Loss: 104.19832611083984\n",
      "Epoch: 46700 | MSE Train Loss: 113.3966064453125 | MSE Test Loss: 104.15576934814453\n",
      "Epoch: 46800 | MSE Train Loss: 113.3600845336914 | MSE Test Loss: 104.11343383789062\n",
      "Epoch: 46900 | MSE Train Loss: 113.32377624511719 | MSE Test Loss: 104.07132720947266\n",
      "Epoch: 47000 | MSE Train Loss: 113.28826904296875 | MSE Test Loss: 104.0300521850586\n",
      "Epoch: 47100 | MSE Train Loss: 113.25324249267578 | MSE Test Loss: 103.98932647705078\n",
      "Epoch: 47200 | MSE Train Loss: 113.2183837890625 | MSE Test Loss: 103.94883728027344\n",
      "Epoch: 47300 | MSE Train Loss: 113.18378448486328 | MSE Test Loss: 103.90857696533203\n",
      "Epoch: 47400 | MSE Train Loss: 113.15008544921875 | MSE Test Loss: 103.8692855834961\n",
      "Epoch: 47500 | MSE Train Loss: 113.11670684814453 | MSE Test Loss: 103.83041381835938\n",
      "Epoch: 47600 | MSE Train Loss: 113.08352661132812 | MSE Test Loss: 103.79173278808594\n",
      "Epoch: 47700 | MSE Train Loss: 113.05052947998047 | MSE Test Loss: 103.7533187866211\n",
      "Epoch: 47800 | MSE Train Loss: 113.0185317993164 | MSE Test Loss: 103.7158432006836\n",
      "Epoch: 47900 | MSE Train Loss: 112.98673248291016 | MSE Test Loss: 103.67870330810547\n",
      "Epoch: 48000 | MSE Train Loss: 112.9551010131836 | MSE Test Loss: 103.64175415039062\n",
      "Epoch: 48100 | MSE Train Loss: 112.92372131347656 | MSE Test Loss: 103.60507202148438\n",
      "Epoch: 48200 | MSE Train Loss: 112.89326477050781 | MSE Test Loss: 103.56927490234375\n",
      "Epoch: 48300 | MSE Train Loss: 112.86295318603516 | MSE Test Loss: 103.53375244140625\n",
      "Epoch: 48400 | MSE Train Loss: 112.8328628540039 | MSE Test Loss: 103.49838256835938\n",
      "Epoch: 48500 | MSE Train Loss: 112.80294799804688 | MSE Test Loss: 103.46324157714844\n",
      "Epoch: 48600 | MSE Train Loss: 112.77389526367188 | MSE Test Loss: 103.42906188964844\n",
      "Epoch: 48700 | MSE Train Loss: 112.74508666992188 | MSE Test Loss: 103.39517211914062\n",
      "Epoch: 48800 | MSE Train Loss: 112.71646881103516 | MSE Test Loss: 103.36153411865234\n",
      "Epoch: 48900 | MSE Train Loss: 112.68800354003906 | MSE Test Loss: 103.32806396484375\n",
      "Epoch: 49000 | MSE Train Loss: 112.66027069091797 | MSE Test Loss: 103.29537200927734\n",
      "Epoch: 49100 | MSE Train Loss: 112.63288116455078 | MSE Test Loss: 103.26300048828125\n",
      "Epoch: 49200 | MSE Train Loss: 112.60564422607422 | MSE Test Loss: 103.23084259033203\n",
      "Epoch: 49300 | MSE Train Loss: 112.5785903930664 | MSE Test Loss: 103.19884490966797\n",
      "Epoch: 49400 | MSE Train Loss: 112.55204772949219 | MSE Test Loss: 103.16741943359375\n",
      "Epoch: 49500 | MSE Train Loss: 112.52600860595703 | MSE Test Loss: 103.1365966796875\n",
      "Epoch: 49600 | MSE Train Loss: 112.5001449584961 | MSE Test Loss: 103.10594177246094\n",
      "Epoch: 49700 | MSE Train Loss: 112.47443389892578 | MSE Test Loss: 103.07545471191406\n",
      "Epoch: 49800 | MSE Train Loss: 112.44893646240234 | MSE Test Loss: 103.04518127441406\n",
      "Epoch: 49900 | MSE Train Loss: 112.42420959472656 | MSE Test Loss: 103.01570892333984\n",
      "Epoch: 50000 | MSE Train Loss: 112.39964294433594 | MSE Test Loss: 102.9864501953125\n",
      "Epoch: 50100 | MSE Train Loss: 112.37523651123047 | MSE Test Loss: 102.95740509033203\n",
      "Epoch: 50200 | MSE Train Loss: 112.35096740722656 | MSE Test Loss: 102.92858123779297\n",
      "Epoch: 50300 | MSE Train Loss: 112.32721710205078 | MSE Test Loss: 102.9002914428711\n",
      "Epoch: 50400 | MSE Train Loss: 112.3039321899414 | MSE Test Loss: 102.87248229980469\n",
      "Epoch: 50500 | MSE Train Loss: 112.28077697753906 | MSE Test Loss: 102.8448486328125\n",
      "Epoch: 50600 | MSE Train Loss: 112.25775146484375 | MSE Test Loss: 102.81735229492188\n",
      "Epoch: 50700 | MSE Train Loss: 112.23489379882812 | MSE Test Loss: 102.7900161743164\n",
      "Epoch: 50800 | MSE Train Loss: 112.21273803710938 | MSE Test Loss: 102.76341247558594\n",
      "Epoch: 50900 | MSE Train Loss: 112.19078063964844 | MSE Test Loss: 102.7370376586914\n",
      "Epoch: 51000 | MSE Train Loss: 112.16895294189453 | MSE Test Loss: 102.71086120605469\n",
      "Epoch: 51100 | MSE Train Loss: 112.14727020263672 | MSE Test Loss: 102.6848373413086\n",
      "Epoch: 51200 | MSE Train Loss: 112.12576293945312 | MSE Test Loss: 102.65899658203125\n",
      "Epoch: 51300 | MSE Train Loss: 112.10498809814453 | MSE Test Loss: 102.63391876220703\n",
      "Epoch: 51400 | MSE Train Loss: 112.08429718017578 | MSE Test Loss: 102.6089859008789\n",
      "Epoch: 51500 | MSE Train Loss: 112.06375122070312 | MSE Test Loss: 102.5842056274414\n",
      "Epoch: 51600 | MSE Train Loss: 112.04335021972656 | MSE Test Loss: 102.55962371826172\n",
      "Epoch: 51700 | MSE Train Loss: 112.0231704711914 | MSE Test Loss: 102.53528594970703\n",
      "Epoch: 51800 | MSE Train Loss: 112.00360870361328 | MSE Test Loss: 102.5115966796875\n",
      "Epoch: 51900 | MSE Train Loss: 111.98419952392578 | MSE Test Loss: 102.48807525634766\n",
      "Epoch: 52000 | MSE Train Loss: 111.96485900878906 | MSE Test Loss: 102.46472930908203\n",
      "Epoch: 52100 | MSE Train Loss: 111.94567108154297 | MSE Test Loss: 102.44149017333984\n",
      "Epoch: 52200 | MSE Train Loss: 111.9266357421875 | MSE Test Loss: 102.41839599609375\n",
      "Epoch: 52300 | MSE Train Loss: 111.90826416015625 | MSE Test Loss: 102.39604187011719\n",
      "Epoch: 52400 | MSE Train Loss: 111.8899917602539 | MSE Test Loss: 102.37382507324219\n",
      "Epoch: 52500 | MSE Train Loss: 111.87185668945312 | MSE Test Loss: 102.35173797607422\n",
      "Epoch: 52600 | MSE Train Loss: 111.85379791259766 | MSE Test Loss: 102.32974243164062\n",
      "Epoch: 52700 | MSE Train Loss: 111.83589935302734 | MSE Test Loss: 102.30789947509766\n",
      "Epoch: 52800 | MSE Train Loss: 111.81851196289062 | MSE Test Loss: 102.28657531738281\n",
      "Epoch: 52900 | MSE Train Loss: 111.80137634277344 | MSE Test Loss: 102.265625\n",
      "Epoch: 53000 | MSE Train Loss: 111.78435516357422 | MSE Test Loss: 102.24478912353516\n",
      "Epoch: 53100 | MSE Train Loss: 111.76740264892578 | MSE Test Loss: 102.22405242919922\n",
      "Epoch: 53200 | MSE Train Loss: 111.75059509277344 | MSE Test Loss: 102.2034683227539\n",
      "Epoch: 53300 | MSE Train Loss: 111.73405456542969 | MSE Test Loss: 102.18321228027344\n",
      "Epoch: 53400 | MSE Train Loss: 111.71800231933594 | MSE Test Loss: 102.16346740722656\n",
      "Epoch: 53500 | MSE Train Loss: 111.70203399658203 | MSE Test Loss: 102.14384460449219\n",
      "Epoch: 53600 | MSE Train Loss: 111.6861572265625 | MSE Test Loss: 102.12432861328125\n",
      "Epoch: 53700 | MSE Train Loss: 111.67039489746094 | MSE Test Loss: 102.10488891601562\n",
      "Epoch: 53800 | MSE Train Loss: 111.65471649169922 | MSE Test Loss: 102.0855941772461\n",
      "Epoch: 53900 | MSE Train Loss: 111.6395034790039 | MSE Test Loss: 102.06678771972656\n",
      "Epoch: 54000 | MSE Train Loss: 111.62457275390625 | MSE Test Loss: 102.04830932617188\n",
      "Epoch: 54100 | MSE Train Loss: 111.60970306396484 | MSE Test Loss: 102.02994537353516\n",
      "Epoch: 54200 | MSE Train Loss: 111.59493255615234 | MSE Test Loss: 102.01166534423828\n",
      "Epoch: 54300 | MSE Train Loss: 111.58027648925781 | MSE Test Loss: 101.99346923828125\n",
      "Epoch: 54400 | MSE Train Loss: 111.56568908691406 | MSE Test Loss: 101.97535705566406\n",
      "Epoch: 54500 | MSE Train Loss: 111.55162811279297 | MSE Test Loss: 101.95785522460938\n",
      "Epoch: 54600 | MSE Train Loss: 111.5377426147461 | MSE Test Loss: 101.94049835205078\n",
      "Epoch: 54700 | MSE Train Loss: 111.52391815185547 | MSE Test Loss: 101.92324829101562\n",
      "Epoch: 54800 | MSE Train Loss: 111.51017761230469 | MSE Test Loss: 101.90609741210938\n",
      "Epoch: 54900 | MSE Train Loss: 111.49652862548828 | MSE Test Loss: 101.8890380859375\n",
      "Epoch: 55000 | MSE Train Loss: 111.48298645019531 | MSE Test Loss: 101.87210083007812\n",
      "Epoch: 55100 | MSE Train Loss: 111.46992492675781 | MSE Test Loss: 101.85578918457031\n",
      "Epoch: 55200 | MSE Train Loss: 111.45701599121094 | MSE Test Loss: 101.83966827392578\n",
      "Epoch: 55300 | MSE Train Loss: 111.44419860839844 | MSE Test Loss: 101.8236312866211\n",
      "Epoch: 55400 | MSE Train Loss: 111.43145751953125 | MSE Test Loss: 101.80766296386719\n",
      "Epoch: 55500 | MSE Train Loss: 111.41880798339844 | MSE Test Loss: 101.79177856445312\n",
      "Epoch: 55600 | MSE Train Loss: 111.40621948242188 | MSE Test Loss: 101.77599334716797\n",
      "Epoch: 55700 | MSE Train Loss: 111.39398956298828 | MSE Test Loss: 101.76056671142578\n",
      "Epoch: 55800 | MSE Train Loss: 111.38202667236328 | MSE Test Loss: 101.74543762207031\n",
      "Epoch: 55900 | MSE Train Loss: 111.3701400756836 | MSE Test Loss: 101.73040771484375\n",
      "Epoch: 56000 | MSE Train Loss: 111.35835266113281 | MSE Test Loss: 101.71543884277344\n",
      "Epoch: 56100 | MSE Train Loss: 111.34660339355469 | MSE Test Loss: 101.70059204101562\n",
      "Epoch: 56200 | MSE Train Loss: 111.3349380493164 | MSE Test Loss: 101.68582916259766\n",
      "Epoch: 56300 | MSE Train Loss: 111.32337188720703 | MSE Test Loss: 101.6711654663086\n",
      "Epoch: 56400 | MSE Train Loss: 111.31229400634766 | MSE Test Loss: 101.65705108642578\n",
      "Epoch: 56500 | MSE Train Loss: 111.30130004882812 | MSE Test Loss: 101.64303588867188\n",
      "Epoch: 56600 | MSE Train Loss: 111.29037475585938 | MSE Test Loss: 101.6291275024414\n",
      "Epoch: 56700 | MSE Train Loss: 111.27950286865234 | MSE Test Loss: 101.61526489257812\n",
      "Epoch: 56800 | MSE Train Loss: 111.26872253417969 | MSE Test Loss: 101.60144805908203\n",
      "Epoch: 56900 | MSE Train Loss: 111.25796508789062 | MSE Test Loss: 101.58773803710938\n",
      "Epoch: 57000 | MSE Train Loss: 111.24742126464844 | MSE Test Loss: 101.5741958618164\n",
      "Epoch: 57100 | MSE Train Loss: 111.23727416992188 | MSE Test Loss: 101.56117248535156\n",
      "Epoch: 57200 | MSE Train Loss: 111.22716522216797 | MSE Test Loss: 101.54820251464844\n",
      "Epoch: 57300 | MSE Train Loss: 111.2171401977539 | MSE Test Loss: 101.53534698486328\n",
      "Epoch: 57400 | MSE Train Loss: 111.20716857910156 | MSE Test Loss: 101.52255249023438\n",
      "Epoch: 57500 | MSE Train Loss: 111.197265625 | MSE Test Loss: 101.50983428955078\n",
      "Epoch: 57600 | MSE Train Loss: 111.18741607666016 | MSE Test Loss: 101.4971694946289\n",
      "Epoch: 57700 | MSE Train Loss: 111.1777114868164 | MSE Test Loss: 101.48467254638672\n",
      "Epoch: 57800 | MSE Train Loss: 111.16838836669922 | MSE Test Loss: 101.47260284423828\n",
      "Epoch: 57900 | MSE Train Loss: 111.15914916992188 | MSE Test Loss: 101.46060943603516\n",
      "Epoch: 58000 | MSE Train Loss: 111.14995574951172 | MSE Test Loss: 101.44869995117188\n",
      "Epoch: 58100 | MSE Train Loss: 111.14080810546875 | MSE Test Loss: 101.43684387207031\n",
      "Epoch: 58200 | MSE Train Loss: 111.13172149658203 | MSE Test Loss: 101.42504119873047\n",
      "Epoch: 58300 | MSE Train Loss: 111.1227035522461 | MSE Test Loss: 101.41331481933594\n",
      "Epoch: 58400 | MSE Train Loss: 111.11373138427734 | MSE Test Loss: 101.40164184570312\n",
      "Epoch: 58500 | MSE Train Loss: 111.10509490966797 | MSE Test Loss: 101.39031219482422\n",
      "Epoch: 58600 | MSE Train Loss: 111.09664916992188 | MSE Test Loss: 101.37920379638672\n",
      "Epoch: 58700 | MSE Train Loss: 111.0882339477539 | MSE Test Loss: 101.36820220947266\n",
      "Epoch: 58800 | MSE Train Loss: 111.07987213134766 | MSE Test Loss: 101.35724639892578\n",
      "Epoch: 58900 | MSE Train Loss: 111.07156372070312 | MSE Test Loss: 101.34635162353516\n",
      "Epoch: 59000 | MSE Train Loss: 111.06329345703125 | MSE Test Loss: 101.33549499511719\n",
      "Epoch: 59100 | MSE Train Loss: 111.05510711669922 | MSE Test Loss: 101.32471466064453\n",
      "Epoch: 59200 | MSE Train Loss: 111.04695129394531 | MSE Test Loss: 101.31395721435547\n",
      "Epoch: 59300 | MSE Train Loss: 111.03921508789062 | MSE Test Loss: 101.30366516113281\n",
      "Epoch: 59400 | MSE Train Loss: 111.03153228759766 | MSE Test Loss: 101.29358673095703\n",
      "Epoch: 59500 | MSE Train Loss: 111.02391815185547 | MSE Test Loss: 101.28353118896484\n",
      "Epoch: 59600 | MSE Train Loss: 111.01634216308594 | MSE Test Loss: 101.27351379394531\n",
      "Epoch: 59700 | MSE Train Loss: 111.00879669189453 | MSE Test Loss: 101.26356506347656\n",
      "Epoch: 59800 | MSE Train Loss: 111.00132751464844 | MSE Test Loss: 101.25362396240234\n",
      "Epoch: 59900 | MSE Train Loss: 110.99388122558594 | MSE Test Loss: 101.24376678466797\n",
      "Epoch: 60000 | MSE Train Loss: 110.98650360107422 | MSE Test Loss: 101.23395538330078\n",
      "Epoch: 60100 | MSE Train Loss: 110.97945404052734 | MSE Test Loss: 101.2244873046875\n",
      "Epoch: 60200 | MSE Train Loss: 110.97251892089844 | MSE Test Loss: 101.2152328491211\n",
      "Epoch: 60300 | MSE Train Loss: 110.96562194824219 | MSE Test Loss: 101.20600891113281\n",
      "Epoch: 60400 | MSE Train Loss: 110.9587631225586 | MSE Test Loss: 101.19685363769531\n",
      "Epoch: 60500 | MSE Train Loss: 110.95195007324219 | MSE Test Loss: 101.1877212524414\n",
      "Epoch: 60600 | MSE Train Loss: 110.9451904296875 | MSE Test Loss: 101.17864227294922\n",
      "Epoch: 60700 | MSE Train Loss: 110.9384536743164 | MSE Test Loss: 101.16958618164062\n",
      "Epoch: 60800 | MSE Train Loss: 110.93177032470703 | MSE Test Loss: 101.16059112548828\n",
      "Epoch: 60900 | MSE Train Loss: 110.92520904541016 | MSE Test Loss: 101.15174102783203\n",
      "Epoch: 61000 | MSE Train Loss: 110.9189453125 | MSE Test Loss: 101.14320373535156\n",
      "Epoch: 61100 | MSE Train Loss: 110.91271209716797 | MSE Test Loss: 101.134765625\n",
      "Epoch: 61200 | MSE Train Loss: 110.90652465820312 | MSE Test Loss: 101.12633514404297\n",
      "Epoch: 61300 | MSE Train Loss: 110.90038299560547 | MSE Test Loss: 101.11795806884766\n",
      "Epoch: 61400 | MSE Train Loss: 110.89427947998047 | MSE Test Loss: 101.10964965820312\n",
      "Epoch: 61500 | MSE Train Loss: 110.8882064819336 | MSE Test Loss: 101.10135650634766\n",
      "Epoch: 61600 | MSE Train Loss: 110.88217163085938 | MSE Test Loss: 101.09310913085938\n",
      "Epoch: 61700 | MSE Train Loss: 110.87615966796875 | MSE Test Loss: 101.08489227294922\n",
      "Epoch: 61800 | MSE Train Loss: 110.87022399902344 | MSE Test Loss: 101.0767822265625\n",
      "Epoch: 61900 | MSE Train Loss: 110.86463165283203 | MSE Test Loss: 101.06908416748047\n",
      "Epoch: 62000 | MSE Train Loss: 110.85907745361328 | MSE Test Loss: 101.06140899658203\n",
      "Epoch: 62100 | MSE Train Loss: 110.85355377197266 | MSE Test Loss: 101.05381774902344\n",
      "Epoch: 62200 | MSE Train Loss: 110.84805297851562 | MSE Test Loss: 101.04629516601562\n",
      "Epoch: 62300 | MSE Train Loss: 110.84259033203125 | MSE Test Loss: 101.0387954711914\n",
      "Epoch: 62400 | MSE Train Loss: 110.8371810913086 | MSE Test Loss: 101.03134155273438\n",
      "Epoch: 62500 | MSE Train Loss: 110.83177947998047 | MSE Test Loss: 101.02391052246094\n",
      "Epoch: 62600 | MSE Train Loss: 110.82642364501953 | MSE Test Loss: 101.01651763916016\n",
      "Epoch: 62700 | MSE Train Loss: 110.82109832763672 | MSE Test Loss: 101.0091552734375\n",
      "Epoch: 62800 | MSE Train Loss: 110.81598663330078 | MSE Test Loss: 101.00203704833984\n",
      "Epoch: 62900 | MSE Train Loss: 110.81105041503906 | MSE Test Loss: 100.9950942993164\n",
      "Epoch: 63000 | MSE Train Loss: 110.80611419677734 | MSE Test Loss: 100.98821258544922\n",
      "Epoch: 63100 | MSE Train Loss: 110.80121612548828 | MSE Test Loss: 100.98138427734375\n",
      "Epoch: 63200 | MSE Train Loss: 110.7963638305664 | MSE Test Loss: 100.97457885742188\n",
      "Epoch: 63300 | MSE Train Loss: 110.7915267944336 | MSE Test Loss: 100.9677963256836\n",
      "Epoch: 63400 | MSE Train Loss: 110.78672790527344 | MSE Test Loss: 100.96104431152344\n",
      "Epoch: 63500 | MSE Train Loss: 110.78195190429688 | MSE Test Loss: 100.95436096191406\n",
      "Epoch: 63600 | MSE Train Loss: 110.7771987915039 | MSE Test Loss: 100.94768524169922\n",
      "Epoch: 63700 | MSE Train Loss: 110.77247619628906 | MSE Test Loss: 100.9410629272461\n",
      "Epoch: 63800 | MSE Train Loss: 110.76790618896484 | MSE Test Loss: 100.9345703125\n",
      "Epoch: 63900 | MSE Train Loss: 110.7635498046875 | MSE Test Loss: 100.92835998535156\n",
      "Epoch: 64000 | MSE Train Loss: 110.75919342041016 | MSE Test Loss: 100.92221069335938\n",
      "Epoch: 64100 | MSE Train Loss: 110.75489044189453 | MSE Test Loss: 100.91606140136719\n",
      "Epoch: 64200 | MSE Train Loss: 110.75060272216797 | MSE Test Loss: 100.90995025634766\n",
      "Epoch: 64300 | MSE Train Loss: 110.74632263183594 | MSE Test Loss: 100.90387725830078\n",
      "Epoch: 64400 | MSE Train Loss: 110.74208068847656 | MSE Test Loss: 100.89781188964844\n",
      "Epoch: 64500 | MSE Train Loss: 110.7378921508789 | MSE Test Loss: 100.89176940917969\n",
      "Epoch: 64600 | MSE Train Loss: 110.73369598388672 | MSE Test Loss: 100.88577270507812\n",
      "Epoch: 64700 | MSE Train Loss: 110.72954559326172 | MSE Test Loss: 100.87979888916016\n",
      "Epoch: 64800 | MSE Train Loss: 110.72540283203125 | MSE Test Loss: 100.87384796142578\n",
      "Epoch: 64900 | MSE Train Loss: 110.72142791748047 | MSE Test Loss: 100.8680648803711\n",
      "Epoch: 65000 | MSE Train Loss: 110.7176284790039 | MSE Test Loss: 100.86247253417969\n",
      "Epoch: 65100 | MSE Train Loss: 110.71382904052734 | MSE Test Loss: 100.85694122314453\n",
      "Epoch: 65200 | MSE Train Loss: 110.71005249023438 | MSE Test Loss: 100.85145568847656\n",
      "Epoch: 65300 | MSE Train Loss: 110.70631408691406 | MSE Test Loss: 100.84603118896484\n",
      "Epoch: 65400 | MSE Train Loss: 110.70259094238281 | MSE Test Loss: 100.84063720703125\n",
      "Epoch: 65500 | MSE Train Loss: 110.69890594482422 | MSE Test Loss: 100.83528137207031\n",
      "Epoch: 65600 | MSE Train Loss: 110.69522857666016 | MSE Test Loss: 100.8299560546875\n",
      "Epoch: 65700 | MSE Train Loss: 110.69158172607422 | MSE Test Loss: 100.82463073730469\n",
      "Epoch: 65800 | MSE Train Loss: 110.68795013427734 | MSE Test Loss: 100.81934356689453\n",
      "Epoch: 65900 | MSE Train Loss: 110.68433380126953 | MSE Test Loss: 100.81407165527344\n",
      "Epoch: 66000 | MSE Train Loss: 110.68074798583984 | MSE Test Loss: 100.808837890625\n",
      "Epoch: 66100 | MSE Train Loss: 110.67740631103516 | MSE Test Loss: 100.80388641357422\n",
      "Epoch: 66200 | MSE Train Loss: 110.67412567138672 | MSE Test Loss: 100.79903411865234\n",
      "Epoch: 66300 | MSE Train Loss: 110.67084503173828 | MSE Test Loss: 100.79421997070312\n",
      "Epoch: 66400 | MSE Train Loss: 110.66759490966797 | MSE Test Loss: 100.7894058227539\n",
      "Epoch: 66500 | MSE Train Loss: 110.66439056396484 | MSE Test Loss: 100.78463745117188\n",
      "Epoch: 66600 | MSE Train Loss: 110.66117095947266 | MSE Test Loss: 100.7798843383789\n",
      "Epoch: 66700 | MSE Train Loss: 110.6579818725586 | MSE Test Loss: 100.775146484375\n",
      "Epoch: 66800 | MSE Train Loss: 110.65481567382812 | MSE Test Loss: 100.77043914794922\n",
      "Epoch: 66900 | MSE Train Loss: 110.65166473388672 | MSE Test Loss: 100.7657470703125\n",
      "Epoch: 67000 | MSE Train Loss: 110.64852905273438 | MSE Test Loss: 100.76106262207031\n",
      "Epoch: 67100 | MSE Train Loss: 110.64541625976562 | MSE Test Loss: 100.75640106201172\n",
      "Epoch: 67200 | MSE Train Loss: 110.64232635498047 | MSE Test Loss: 100.75177764892578\n",
      "Epoch: 67300 | MSE Train Loss: 110.63932800292969 | MSE Test Loss: 100.74723815917969\n",
      "Epoch: 67400 | MSE Train Loss: 110.63651275634766 | MSE Test Loss: 100.74290466308594\n",
      "Epoch: 67500 | MSE Train Loss: 110.63371276855469 | MSE Test Loss: 100.73859405517578\n",
      "Epoch: 67600 | MSE Train Loss: 110.63093566894531 | MSE Test Loss: 100.73435974121094\n",
      "Epoch: 67700 | MSE Train Loss: 110.62816619873047 | MSE Test Loss: 100.73014068603516\n",
      "Epoch: 67800 | MSE Train Loss: 110.62540435791016 | MSE Test Loss: 100.72594451904297\n",
      "Epoch: 67900 | MSE Train Loss: 110.62267303466797 | MSE Test Loss: 100.7217788696289\n",
      "Epoch: 68000 | MSE Train Loss: 110.61993408203125 | MSE Test Loss: 100.71759033203125\n",
      "Epoch: 68100 | MSE Train Loss: 110.61724090576172 | MSE Test Loss: 100.71347045898438\n",
      "Epoch: 68200 | MSE Train Loss: 110.61456298828125 | MSE Test Loss: 100.70935821533203\n",
      "Epoch: 68300 | MSE Train Loss: 110.61189270019531 | MSE Test Loss: 100.70524597167969\n",
      "Epoch: 68400 | MSE Train Loss: 110.60923767089844 | MSE Test Loss: 100.70112609863281\n",
      "Epoch: 68500 | MSE Train Loss: 110.6065902709961 | MSE Test Loss: 100.69705963134766\n",
      "Epoch: 68600 | MSE Train Loss: 110.60397338867188 | MSE Test Loss: 100.69300079345703\n",
      "Epoch: 68700 | MSE Train Loss: 110.60150146484375 | MSE Test Loss: 100.68913269042969\n",
      "Epoch: 68800 | MSE Train Loss: 110.5991439819336 | MSE Test Loss: 100.68537902832031\n",
      "Epoch: 68900 | MSE Train Loss: 110.59678649902344 | MSE Test Loss: 100.68167877197266\n",
      "Epoch: 69000 | MSE Train Loss: 110.59441375732422 | MSE Test Loss: 100.67797088623047\n",
      "Epoch: 69100 | MSE Train Loss: 110.59210968017578 | MSE Test Loss: 100.6742935180664\n",
      "Epoch: 69200 | MSE Train Loss: 110.58977508544922 | MSE Test Loss: 100.67070007324219\n",
      "Epoch: 69300 | MSE Train Loss: 110.58748626708984 | MSE Test Loss: 100.66709899902344\n",
      "Epoch: 69400 | MSE Train Loss: 110.58518981933594 | MSE Test Loss: 100.66352081298828\n",
      "Epoch: 69500 | MSE Train Loss: 110.5829086303711 | MSE Test Loss: 100.65995025634766\n",
      "Epoch: 69600 | MSE Train Loss: 110.58065032958984 | MSE Test Loss: 100.65640258789062\n",
      "Epoch: 69700 | MSE Train Loss: 110.57840728759766 | MSE Test Loss: 100.65286254882812\n",
      "Epoch: 69800 | MSE Train Loss: 110.576171875 | MSE Test Loss: 100.64932250976562\n",
      "Epoch: 69900 | MSE Train Loss: 110.57396697998047 | MSE Test Loss: 100.64582061767578\n",
      "Epoch: 70000 | MSE Train Loss: 110.5717544555664 | MSE Test Loss: 100.64230346679688\n",
      "Epoch: 70100 | MSE Train Loss: 110.569580078125 | MSE Test Loss: 100.63885498046875\n",
      "Epoch: 70200 | MSE Train Loss: 110.56753540039062 | MSE Test Loss: 100.63555908203125\n",
      "Epoch: 70300 | MSE Train Loss: 110.5655746459961 | MSE Test Loss: 100.63236236572266\n",
      "Epoch: 70400 | MSE Train Loss: 110.5636215209961 | MSE Test Loss: 100.62923431396484\n",
      "Epoch: 70500 | MSE Train Loss: 110.56167602539062 | MSE Test Loss: 100.62611389160156\n",
      "Epoch: 70600 | MSE Train Loss: 110.55977630615234 | MSE Test Loss: 100.6230239868164\n",
      "Epoch: 70700 | MSE Train Loss: 110.55784606933594 | MSE Test Loss: 100.61992645263672\n",
      "Epoch: 70800 | MSE Train Loss: 110.55593872070312 | MSE Test Loss: 100.61685180664062\n",
      "Epoch: 70900 | MSE Train Loss: 110.55403900146484 | MSE Test Loss: 100.61378479003906\n",
      "Epoch: 71000 | MSE Train Loss: 110.5521469116211 | MSE Test Loss: 100.61071014404297\n",
      "Epoch: 71100 | MSE Train Loss: 110.55026245117188 | MSE Test Loss: 100.60768127441406\n",
      "Epoch: 71200 | MSE Train Loss: 110.54841613769531 | MSE Test Loss: 100.60462188720703\n",
      "Epoch: 71300 | MSE Train Loss: 110.54655456542969 | MSE Test Loss: 100.60160827636719\n",
      "Epoch: 71400 | MSE Train Loss: 110.54472351074219 | MSE Test Loss: 100.59859466552734\n",
      "Epoch: 71500 | MSE Train Loss: 110.54289245605469 | MSE Test Loss: 100.59559631347656\n",
      "Epoch: 71600 | MSE Train Loss: 110.54108428955078 | MSE Test Loss: 100.59260559082031\n",
      "Epoch: 71700 | MSE Train Loss: 110.53929138183594 | MSE Test Loss: 100.58963012695312\n",
      "Epoch: 71800 | MSE Train Loss: 110.53759002685547 | MSE Test Loss: 100.5868148803711\n",
      "Epoch: 71900 | MSE Train Loss: 110.5359878540039 | MSE Test Loss: 100.58409118652344\n",
      "Epoch: 72000 | MSE Train Loss: 110.5344009399414 | MSE Test Loss: 100.5813980102539\n",
      "Epoch: 72100 | MSE Train Loss: 110.53279876708984 | MSE Test Loss: 100.5787353515625\n",
      "Epoch: 72200 | MSE Train Loss: 110.53121185302734 | MSE Test Loss: 100.57608795166016\n",
      "Epoch: 72300 | MSE Train Loss: 110.52967071533203 | MSE Test Loss: 100.57342529296875\n",
      "Epoch: 72400 | MSE Train Loss: 110.52809143066406 | MSE Test Loss: 100.5707778930664\n",
      "Epoch: 72500 | MSE Train Loss: 110.52655029296875 | MSE Test Loss: 100.56815338134766\n",
      "Epoch: 72600 | MSE Train Loss: 110.5250015258789 | MSE Test Loss: 100.56554412841797\n",
      "Epoch: 72700 | MSE Train Loss: 110.5234603881836 | MSE Test Loss: 100.56293487548828\n",
      "Epoch: 72800 | MSE Train Loss: 110.52194213867188 | MSE Test Loss: 100.56034088134766\n",
      "Epoch: 72900 | MSE Train Loss: 110.52043151855469 | MSE Test Loss: 100.55773162841797\n",
      "Epoch: 73000 | MSE Train Loss: 110.5189437866211 | MSE Test Loss: 100.55518341064453\n",
      "Epoch: 73100 | MSE Train Loss: 110.5174331665039 | MSE Test Loss: 100.55259704589844\n",
      "Epoch: 73200 | MSE Train Loss: 110.5159683227539 | MSE Test Loss: 100.550048828125\n",
      "Epoch: 73300 | MSE Train Loss: 110.51447296142578 | MSE Test Loss: 100.5474853515625\n",
      "Epoch: 73400 | MSE Train Loss: 110.51303100585938 | MSE Test Loss: 100.54493713378906\n",
      "Epoch: 73500 | MSE Train Loss: 110.51155090332031 | MSE Test Loss: 100.54242706298828\n",
      "Epoch: 73600 | MSE Train Loss: 110.5101547241211 | MSE Test Loss: 100.53998565673828\n",
      "Epoch: 73700 | MSE Train Loss: 110.5088882446289 | MSE Test Loss: 100.5377197265625\n",
      "Epoch: 73800 | MSE Train Loss: 110.50761413574219 | MSE Test Loss: 100.53546142578125\n",
      "Epoch: 73900 | MSE Train Loss: 110.50635528564453 | MSE Test Loss: 100.5332260131836\n",
      "Epoch: 74000 | MSE Train Loss: 110.5051040649414 | MSE Test Loss: 100.5309829711914\n",
      "Epoch: 74100 | MSE Train Loss: 110.50384521484375 | MSE Test Loss: 100.52877807617188\n",
      "Epoch: 74200 | MSE Train Loss: 110.50259399414062 | MSE Test Loss: 100.52656555175781\n",
      "Epoch: 74300 | MSE Train Loss: 110.5013656616211 | MSE Test Loss: 100.52435302734375\n",
      "Epoch: 74400 | MSE Train Loss: 110.50013732910156 | MSE Test Loss: 100.5221939086914\n",
      "Epoch: 74500 | MSE Train Loss: 110.49893951416016 | MSE Test Loss: 100.52001190185547\n",
      "Epoch: 74600 | MSE Train Loss: 110.4977035522461 | MSE Test Loss: 100.51789093017578\n",
      "Epoch: 74700 | MSE Train Loss: 110.49650573730469 | MSE Test Loss: 100.51575469970703\n",
      "Epoch: 74800 | MSE Train Loss: 110.49531555175781 | MSE Test Loss: 100.51362609863281\n",
      "Epoch: 74900 | MSE Train Loss: 110.49412536621094 | MSE Test Loss: 100.51151275634766\n",
      "Epoch: 75000 | MSE Train Loss: 110.49292755126953 | MSE Test Loss: 100.50939178466797\n",
      "Epoch: 75100 | MSE Train Loss: 110.49176025390625 | MSE Test Loss: 100.50728607177734\n",
      "Epoch: 75200 | MSE Train Loss: 110.49060821533203 | MSE Test Loss: 100.50519561767578\n",
      "Epoch: 75300 | MSE Train Loss: 110.48944091796875 | MSE Test Loss: 100.50311279296875\n",
      "Epoch: 75400 | MSE Train Loss: 110.48828887939453 | MSE Test Loss: 100.50101470947266\n",
      "Epoch: 75500 | MSE Train Loss: 110.48713684082031 | MSE Test Loss: 100.49894714355469\n",
      "Epoch: 75600 | MSE Train Loss: 110.48600769042969 | MSE Test Loss: 100.49688720703125\n",
      "Epoch: 75700 | MSE Train Loss: 110.48501586914062 | MSE Test Loss: 100.49504089355469\n",
      "Epoch: 75800 | MSE Train Loss: 110.48403930664062 | MSE Test Loss: 100.49322509765625\n",
      "Epoch: 75900 | MSE Train Loss: 110.48306274414062 | MSE Test Loss: 100.49142456054688\n",
      "Epoch: 76000 | MSE Train Loss: 110.48209381103516 | MSE Test Loss: 100.48963165283203\n",
      "Epoch: 76100 | MSE Train Loss: 110.48113250732422 | MSE Test Loss: 100.48783111572266\n",
      "Epoch: 76200 | MSE Train Loss: 110.48017120361328 | MSE Test Loss: 100.48604583740234\n",
      "Epoch: 76300 | MSE Train Loss: 110.47920227050781 | MSE Test Loss: 100.48426818847656\n",
      "Epoch: 76400 | MSE Train Loss: 110.4782485961914 | MSE Test Loss: 100.48249816894531\n",
      "Epoch: 76500 | MSE Train Loss: 110.4773178100586 | MSE Test Loss: 100.4807357788086\n",
      "Epoch: 76600 | MSE Train Loss: 110.47637939453125 | MSE Test Loss: 100.47897338867188\n",
      "Epoch: 76700 | MSE Train Loss: 110.47545623779297 | MSE Test Loss: 100.47722625732422\n",
      "Epoch: 76800 | MSE Train Loss: 110.47451782226562 | MSE Test Loss: 100.4754867553711\n",
      "Epoch: 76900 | MSE Train Loss: 110.4736099243164 | MSE Test Loss: 100.47373962402344\n",
      "Epoch: 77000 | MSE Train Loss: 110.4726791381836 | MSE Test Loss: 100.4720230102539\n",
      "Epoch: 77100 | MSE Train Loss: 110.47177124023438 | MSE Test Loss: 100.4703140258789\n",
      "Epoch: 77200 | MSE Train Loss: 110.47087860107422 | MSE Test Loss: 100.46857452392578\n",
      "Epoch: 77300 | MSE Train Loss: 110.469970703125 | MSE Test Loss: 100.46687316894531\n",
      "Epoch: 77400 | MSE Train Loss: 110.46906280517578 | MSE Test Loss: 100.46517181396484\n",
      "Epoch: 77500 | MSE Train Loss: 110.46817779541016 | MSE Test Loss: 100.4634780883789\n",
      "Epoch: 77600 | MSE Train Loss: 110.46731567382812 | MSE Test Loss: 100.4617919921875\n",
      "Epoch: 77700 | MSE Train Loss: 110.46643829345703 | MSE Test Loss: 100.46009063720703\n",
      "Epoch: 77800 | MSE Train Loss: 110.46556854248047 | MSE Test Loss: 100.45841217041016\n",
      "Epoch: 77900 | MSE Train Loss: 110.46469116210938 | MSE Test Loss: 100.45673370361328\n",
      "Epoch: 78000 | MSE Train Loss: 110.46393585205078 | MSE Test Loss: 100.45521545410156\n",
      "Epoch: 78100 | MSE Train Loss: 110.46320343017578 | MSE Test Loss: 100.45376586914062\n",
      "Epoch: 78200 | MSE Train Loss: 110.46247100830078 | MSE Test Loss: 100.45231628417969\n",
      "Epoch: 78300 | MSE Train Loss: 110.46174621582031 | MSE Test Loss: 100.45088195800781\n",
      "Epoch: 78400 | MSE Train Loss: 110.46102905273438 | MSE Test Loss: 100.44944763183594\n",
      "Epoch: 78500 | MSE Train Loss: 110.46031951904297 | MSE Test Loss: 100.44803619384766\n",
      "Epoch: 78600 | MSE Train Loss: 110.4595947265625 | MSE Test Loss: 100.44661712646484\n",
      "Epoch: 78700 | MSE Train Loss: 110.45890045166016 | MSE Test Loss: 100.44520568847656\n",
      "Epoch: 78800 | MSE Train Loss: 110.45819091796875 | MSE Test Loss: 100.44377899169922\n",
      "Epoch: 78900 | MSE Train Loss: 110.45748901367188 | MSE Test Loss: 100.44236755371094\n",
      "Epoch: 79000 | MSE Train Loss: 110.45679473876953 | MSE Test Loss: 100.44097137451172\n",
      "Epoch: 79100 | MSE Train Loss: 110.45610809326172 | MSE Test Loss: 100.43956756591797\n",
      "Epoch: 79200 | MSE Train Loss: 110.45540618896484 | MSE Test Loss: 100.43818664550781\n",
      "Epoch: 79300 | MSE Train Loss: 110.4547348022461 | MSE Test Loss: 100.4367904663086\n",
      "Epoch: 79400 | MSE Train Loss: 110.45407104492188 | MSE Test Loss: 100.43540954589844\n",
      "Epoch: 79500 | MSE Train Loss: 110.45337677001953 | MSE Test Loss: 100.43401336669922\n",
      "Epoch: 79600 | MSE Train Loss: 110.45272064208984 | MSE Test Loss: 100.43264770507812\n",
      "Epoch: 79700 | MSE Train Loss: 110.45205688476562 | MSE Test Loss: 100.43126678466797\n",
      "Epoch: 79800 | MSE Train Loss: 110.45138549804688 | MSE Test Loss: 100.4299087524414\n",
      "Epoch: 79900 | MSE Train Loss: 110.45072937011719 | MSE Test Loss: 100.42854309082031\n",
      "Epoch: 80000 | MSE Train Loss: 110.45008850097656 | MSE Test Loss: 100.42717742919922\n",
      "Epoch: 80100 | MSE Train Loss: 110.44942474365234 | MSE Test Loss: 100.42582702636719\n",
      "Epoch: 80200 | MSE Train Loss: 110.44878387451172 | MSE Test Loss: 100.42448425292969\n",
      "Epoch: 80300 | MSE Train Loss: 110.44815826416016 | MSE Test Loss: 100.4231185913086\n",
      "Epoch: 80400 | MSE Train Loss: 110.44749450683594 | MSE Test Loss: 100.4217758178711\n",
      "Epoch: 80500 | MSE Train Loss: 110.4468765258789 | MSE Test Loss: 100.42044067382812\n",
      "Epoch: 80600 | MSE Train Loss: 110.44625091552734 | MSE Test Loss: 100.41909790039062\n",
      "Epoch: 80700 | MSE Train Loss: 110.44567108154297 | MSE Test Loss: 100.4178466796875\n",
      "Epoch: 80800 | MSE Train Loss: 110.4451675415039 | MSE Test Loss: 100.41667175292969\n",
      "Epoch: 80900 | MSE Train Loss: 110.44464111328125 | MSE Test Loss: 100.41553497314453\n",
      "Epoch: 81000 | MSE Train Loss: 110.44412231445312 | MSE Test Loss: 100.41437530517578\n",
      "Epoch: 81100 | MSE Train Loss: 110.44361877441406 | MSE Test Loss: 100.41326141357422\n",
      "Epoch: 81200 | MSE Train Loss: 110.44310760498047 | MSE Test Loss: 100.41211700439453\n",
      "Epoch: 81300 | MSE Train Loss: 110.44261169433594 | MSE Test Loss: 100.4110107421875\n",
      "Epoch: 81400 | MSE Train Loss: 110.4421157836914 | MSE Test Loss: 100.40987396240234\n",
      "Epoch: 81500 | MSE Train Loss: 110.4416275024414 | MSE Test Loss: 100.40876770019531\n",
      "Epoch: 81600 | MSE Train Loss: 110.44112396240234 | MSE Test Loss: 100.40766906738281\n",
      "Epoch: 81700 | MSE Train Loss: 110.44062805175781 | MSE Test Loss: 100.40657806396484\n",
      "Epoch: 81800 | MSE Train Loss: 110.44014739990234 | MSE Test Loss: 100.405517578125\n",
      "Epoch: 81900 | MSE Train Loss: 110.43964385986328 | MSE Test Loss: 100.40444946289062\n",
      "Epoch: 82000 | MSE Train Loss: 110.43916320800781 | MSE Test Loss: 100.40337371826172\n",
      "Epoch: 82100 | MSE Train Loss: 110.43869018554688 | MSE Test Loss: 100.40233612060547\n",
      "Epoch: 82200 | MSE Train Loss: 110.43820190429688 | MSE Test Loss: 100.40128326416016\n",
      "Epoch: 82300 | MSE Train Loss: 110.43775177001953 | MSE Test Loss: 100.40021514892578\n",
      "Epoch: 82400 | MSE Train Loss: 110.43727111816406 | MSE Test Loss: 100.39917755126953\n",
      "Epoch: 82500 | MSE Train Loss: 110.43680572509766 | MSE Test Loss: 100.39813995361328\n",
      "Epoch: 82600 | MSE Train Loss: 110.43634033203125 | MSE Test Loss: 100.39710235595703\n",
      "Epoch: 82700 | MSE Train Loss: 110.43587493896484 | MSE Test Loss: 100.39605712890625\n",
      "Epoch: 82800 | MSE Train Loss: 110.43541717529297 | MSE Test Loss: 100.39502716064453\n",
      "Epoch: 82900 | MSE Train Loss: 110.4349594116211 | MSE Test Loss: 100.39398193359375\n",
      "Epoch: 83000 | MSE Train Loss: 110.43450927734375 | MSE Test Loss: 100.3929672241211\n",
      "Epoch: 83100 | MSE Train Loss: 110.43404388427734 | MSE Test Loss: 100.39193725585938\n",
      "Epoch: 83200 | MSE Train Loss: 110.4336166381836 | MSE Test Loss: 100.39090728759766\n",
      "Epoch: 83300 | MSE Train Loss: 110.43316650390625 | MSE Test Loss: 100.389892578125\n",
      "Epoch: 83400 | MSE Train Loss: 110.4327163696289 | MSE Test Loss: 100.38887786865234\n",
      "Epoch: 83500 | MSE Train Loss: 110.4322738647461 | MSE Test Loss: 100.38786315917969\n",
      "Epoch: 83600 | MSE Train Loss: 110.43184661865234 | MSE Test Loss: 100.38682556152344\n",
      "Epoch: 83700 | MSE Train Loss: 110.43140411376953 | MSE Test Loss: 100.3858413696289\n",
      "Epoch: 83800 | MSE Train Loss: 110.43098449707031 | MSE Test Loss: 100.38482666015625\n",
      "Epoch: 83900 | MSE Train Loss: 110.4305648803711 | MSE Test Loss: 100.38383483886719\n",
      "Epoch: 84000 | MSE Train Loss: 110.43019104003906 | MSE Test Loss: 100.38298034667969\n",
      "Epoch: 84100 | MSE Train Loss: 110.42985534667969 | MSE Test Loss: 100.38214874267578\n",
      "Epoch: 84200 | MSE Train Loss: 110.42951202392578 | MSE Test Loss: 100.38136291503906\n",
      "Epoch: 84300 | MSE Train Loss: 110.42919158935547 | MSE Test Loss: 100.38056182861328\n",
      "Epoch: 84400 | MSE Train Loss: 110.42884063720703 | MSE Test Loss: 100.37977600097656\n",
      "Epoch: 84500 | MSE Train Loss: 110.42851257324219 | MSE Test Loss: 100.37897491455078\n",
      "Epoch: 84600 | MSE Train Loss: 110.42817687988281 | MSE Test Loss: 100.37820434570312\n",
      "Epoch: 84700 | MSE Train Loss: 110.4278564453125 | MSE Test Loss: 100.37740325927734\n",
      "Epoch: 84800 | MSE Train Loss: 110.42753601074219 | MSE Test Loss: 100.37662506103516\n",
      "Epoch: 84900 | MSE Train Loss: 110.42720794677734 | MSE Test Loss: 100.3758316040039\n",
      "Epoch: 85000 | MSE Train Loss: 110.42691040039062 | MSE Test Loss: 100.37505340576172\n",
      "Epoch: 85100 | MSE Train Loss: 110.42657470703125 | MSE Test Loss: 100.37428283691406\n",
      "Epoch: 85200 | MSE Train Loss: 110.42625427246094 | MSE Test Loss: 100.37350463867188\n",
      "Epoch: 85300 | MSE Train Loss: 110.42595672607422 | MSE Test Loss: 100.37271881103516\n",
      "Epoch: 85400 | MSE Train Loss: 110.42561340332031 | MSE Test Loss: 100.37195587158203\n",
      "Epoch: 85500 | MSE Train Loss: 110.42532348632812 | MSE Test Loss: 100.37117004394531\n",
      "Epoch: 85600 | MSE Train Loss: 110.42499542236328 | MSE Test Loss: 100.37041473388672\n",
      "Epoch: 85700 | MSE Train Loss: 110.42469787597656 | MSE Test Loss: 100.36964416503906\n",
      "Epoch: 85800 | MSE Train Loss: 110.42439270019531 | MSE Test Loss: 100.3688735961914\n",
      "Epoch: 85900 | MSE Train Loss: 110.4240951538086 | MSE Test Loss: 100.36811065673828\n",
      "Epoch: 86000 | MSE Train Loss: 110.42378234863281 | MSE Test Loss: 100.36735534667969\n",
      "Epoch: 86100 | MSE Train Loss: 110.4234848022461 | MSE Test Loss: 100.36658477783203\n",
      "Epoch: 86200 | MSE Train Loss: 110.42317962646484 | MSE Test Loss: 100.36582946777344\n",
      "Epoch: 86300 | MSE Train Loss: 110.42288970947266 | MSE Test Loss: 100.36506652832031\n",
      "Epoch: 86400 | MSE Train Loss: 110.4225845336914 | MSE Test Loss: 100.36431121826172\n",
      "Epoch: 86500 | MSE Train Loss: 110.42230224609375 | MSE Test Loss: 100.36356353759766\n",
      "Epoch: 86600 | MSE Train Loss: 110.42201232910156 | MSE Test Loss: 100.3628158569336\n",
      "Epoch: 86700 | MSE Train Loss: 110.42172241210938 | MSE Test Loss: 100.362060546875\n",
      "Epoch: 86800 | MSE Train Loss: 110.42143249511719 | MSE Test Loss: 100.361328125\n",
      "Epoch: 86900 | MSE Train Loss: 110.421142578125 | MSE Test Loss: 100.36058044433594\n",
      "Epoch: 87000 | MSE Train Loss: 110.42086791992188 | MSE Test Loss: 100.35980987548828\n",
      "Epoch: 87100 | MSE Train Loss: 110.42057800292969 | MSE Test Loss: 100.35909271240234\n",
      "Epoch: 87200 | MSE Train Loss: 110.4203109741211 | MSE Test Loss: 100.35834503173828\n",
      "Epoch: 87300 | MSE Train Loss: 110.42001342773438 | MSE Test Loss: 100.35762786865234\n",
      "Epoch: 87400 | MSE Train Loss: 110.41974639892578 | MSE Test Loss: 100.35687255859375\n",
      "Epoch: 87500 | MSE Train Loss: 110.41947174072266 | MSE Test Loss: 100.35612487792969\n",
      "Epoch: 87600 | MSE Train Loss: 110.41920471191406 | MSE Test Loss: 100.35540008544922\n",
      "Epoch: 87700 | MSE Train Loss: 110.4189224243164 | MSE Test Loss: 100.35467529296875\n",
      "Epoch: 87800 | MSE Train Loss: 110.41864776611328 | MSE Test Loss: 100.35393524169922\n",
      "Epoch: 87900 | MSE Train Loss: 110.41838073730469 | MSE Test Loss: 100.35321807861328\n",
      "Epoch: 88000 | MSE Train Loss: 110.41815185546875 | MSE Test Loss: 100.35252380371094\n",
      "Epoch: 88100 | MSE Train Loss: 110.41793060302734 | MSE Test Loss: 100.35191345214844\n",
      "Epoch: 88200 | MSE Train Loss: 110.4177474975586 | MSE Test Loss: 100.3513412475586\n",
      "Epoch: 88300 | MSE Train Loss: 110.41755676269531 | MSE Test Loss: 100.35076904296875\n",
      "Epoch: 88400 | MSE Train Loss: 110.4173583984375 | MSE Test Loss: 100.35020446777344\n",
      "Epoch: 88500 | MSE Train Loss: 110.41716766357422 | MSE Test Loss: 100.34964752197266\n",
      "Epoch: 88600 | MSE Train Loss: 110.41696166992188 | MSE Test Loss: 100.34908294677734\n",
      "Epoch: 88700 | MSE Train Loss: 110.41676330566406 | MSE Test Loss: 100.34855651855469\n",
      "Epoch: 88800 | MSE Train Loss: 110.41658020019531 | MSE Test Loss: 100.3480224609375\n",
      "Epoch: 88900 | MSE Train Loss: 110.41638946533203 | MSE Test Loss: 100.34748077392578\n",
      "Epoch: 89000 | MSE Train Loss: 110.41621398925781 | MSE Test Loss: 100.3469467163086\n",
      "Epoch: 89100 | MSE Train Loss: 110.41602325439453 | MSE Test Loss: 100.34639739990234\n",
      "Epoch: 89200 | MSE Train Loss: 110.41581726074219 | MSE Test Loss: 100.34585571289062\n",
      "Epoch: 89300 | MSE Train Loss: 110.4156494140625 | MSE Test Loss: 100.3453369140625\n",
      "Epoch: 89400 | MSE Train Loss: 110.41545867919922 | MSE Test Loss: 100.34479522705078\n",
      "Epoch: 89500 | MSE Train Loss: 110.41526794433594 | MSE Test Loss: 100.3442611694336\n",
      "Epoch: 89600 | MSE Train Loss: 110.41509246826172 | MSE Test Loss: 100.3437271118164\n",
      "Epoch: 89700 | MSE Train Loss: 110.4149169921875 | MSE Test Loss: 100.34318542480469\n",
      "Epoch: 89800 | MSE Train Loss: 110.41472625732422 | MSE Test Loss: 100.34265899658203\n",
      "Epoch: 89900 | MSE Train Loss: 110.41454315185547 | MSE Test Loss: 100.3421401977539\n",
      "Epoch: 90000 | MSE Train Loss: 110.41436767578125 | MSE Test Loss: 100.34159851074219\n",
      "Epoch: 90100 | MSE Train Loss: 110.41417694091797 | MSE Test Loss: 100.341064453125\n",
      "Epoch: 90200 | MSE Train Loss: 110.41402435302734 | MSE Test Loss: 100.3405532836914\n",
      "Epoch: 90300 | MSE Train Loss: 110.41383361816406 | MSE Test Loss: 100.34001922607422\n",
      "Epoch: 90400 | MSE Train Loss: 110.41365051269531 | MSE Test Loss: 100.33949279785156\n",
      "Epoch: 90500 | MSE Train Loss: 110.41349029541016 | MSE Test Loss: 100.33897399902344\n",
      "Epoch: 90600 | MSE Train Loss: 110.4133071899414 | MSE Test Loss: 100.33844757080078\n",
      "Epoch: 90700 | MSE Train Loss: 110.41313171386719 | MSE Test Loss: 100.33792114257812\n",
      "Epoch: 90800 | MSE Train Loss: 110.4129867553711 | MSE Test Loss: 100.33740997314453\n",
      "Epoch: 90900 | MSE Train Loss: 110.41280364990234 | MSE Test Loss: 100.33687591552734\n",
      "Epoch: 91000 | MSE Train Loss: 110.41263580322266 | MSE Test Loss: 100.33637237548828\n",
      "Epoch: 91100 | MSE Train Loss: 110.41248321533203 | MSE Test Loss: 100.33586120605469\n",
      "Epoch: 91200 | MSE Train Loss: 110.41230010986328 | MSE Test Loss: 100.3353271484375\n",
      "Epoch: 91300 | MSE Train Loss: 110.41213989257812 | MSE Test Loss: 100.33483123779297\n",
      "Epoch: 91400 | MSE Train Loss: 110.41197204589844 | MSE Test Loss: 100.33430480957031\n",
      "Epoch: 91500 | MSE Train Loss: 110.41181182861328 | MSE Test Loss: 100.33378601074219\n",
      "Epoch: 91600 | MSE Train Loss: 110.41165161132812 | MSE Test Loss: 100.33328247070312\n",
      "Epoch: 91700 | MSE Train Loss: 110.41149139404297 | MSE Test Loss: 100.33277893066406\n",
      "Epoch: 91800 | MSE Train Loss: 110.41133117675781 | MSE Test Loss: 100.33226013183594\n",
      "Epoch: 91900 | MSE Train Loss: 110.41116333007812 | MSE Test Loss: 100.33174896240234\n",
      "Epoch: 92000 | MSE Train Loss: 110.41101837158203 | MSE Test Loss: 100.33125305175781\n",
      "Epoch: 92100 | MSE Train Loss: 110.41085815429688 | MSE Test Loss: 100.33072662353516\n",
      "Epoch: 92200 | MSE Train Loss: 110.41069793701172 | MSE Test Loss: 100.33024597167969\n",
      "Epoch: 92300 | MSE Train Loss: 110.4105453491211 | MSE Test Loss: 100.32972717285156\n",
      "Epoch: 92400 | MSE Train Loss: 110.410400390625 | MSE Test Loss: 100.3292236328125\n",
      "Epoch: 92500 | MSE Train Loss: 110.41022491455078 | MSE Test Loss: 100.32872009277344\n",
      "Epoch: 92600 | MSE Train Loss: 110.41009521484375 | MSE Test Loss: 100.32821655273438\n",
      "Epoch: 92700 | MSE Train Loss: 110.4099349975586 | MSE Test Loss: 100.32771301269531\n",
      "Epoch: 92800 | MSE Train Loss: 110.40978240966797 | MSE Test Loss: 100.32721710205078\n",
      "Epoch: 92900 | MSE Train Loss: 110.40963745117188 | MSE Test Loss: 100.32672119140625\n",
      "Epoch: 93000 | MSE Train Loss: 110.40949249267578 | MSE Test Loss: 100.32622528076172\n",
      "Epoch: 93100 | MSE Train Loss: 110.40934753417969 | MSE Test Loss: 100.32572937011719\n",
      "Epoch: 93200 | MSE Train Loss: 110.4092025756836 | MSE Test Loss: 100.32523345947266\n",
      "Epoch: 93300 | MSE Train Loss: 110.4090576171875 | MSE Test Loss: 100.3247299194336\n",
      "Epoch: 93400 | MSE Train Loss: 110.40890502929688 | MSE Test Loss: 100.32424926757812\n",
      "Epoch: 93500 | MSE Train Loss: 110.40879821777344 | MSE Test Loss: 100.32379913330078\n",
      "Epoch: 93600 | MSE Train Loss: 110.40869903564453 | MSE Test Loss: 100.32343292236328\n",
      "Epoch: 93700 | MSE Train Loss: 110.40860748291016 | MSE Test Loss: 100.32306671142578\n",
      "Epoch: 93800 | MSE Train Loss: 110.40850830078125 | MSE Test Loss: 100.32272338867188\n",
      "Epoch: 93900 | MSE Train Loss: 110.40841674804688 | MSE Test Loss: 100.3223876953125\n",
      "Epoch: 94000 | MSE Train Loss: 110.4083251953125 | MSE Test Loss: 100.322021484375\n",
      "Epoch: 94100 | MSE Train Loss: 110.40823364257812 | MSE Test Loss: 100.32169342041016\n",
      "Epoch: 94200 | MSE Train Loss: 110.40813446044922 | MSE Test Loss: 100.32134246826172\n",
      "Epoch: 94300 | MSE Train Loss: 110.40802764892578 | MSE Test Loss: 100.32100677490234\n",
      "Epoch: 94400 | MSE Train Loss: 110.407958984375 | MSE Test Loss: 100.3206558227539\n",
      "Epoch: 94500 | MSE Train Loss: 110.4078598022461 | MSE Test Loss: 100.3203125\n",
      "Epoch: 94600 | MSE Train Loss: 110.40777587890625 | MSE Test Loss: 100.3199691772461\n",
      "Epoch: 94700 | MSE Train Loss: 110.40768432617188 | MSE Test Loss: 100.31961822509766\n",
      "Epoch: 94800 | MSE Train Loss: 110.40760040283203 | MSE Test Loss: 100.31927490234375\n",
      "Epoch: 94900 | MSE Train Loss: 110.40750885009766 | MSE Test Loss: 100.31896209716797\n",
      "Epoch: 95000 | MSE Train Loss: 110.40740966796875 | MSE Test Loss: 100.31864929199219\n",
      "Epoch: 95100 | MSE Train Loss: 110.4073257446289 | MSE Test Loss: 100.3183364868164\n",
      "Epoch: 95200 | MSE Train Loss: 110.40724182128906 | MSE Test Loss: 100.3180160522461\n",
      "Epoch: 95300 | MSE Train Loss: 110.40716552734375 | MSE Test Loss: 100.31771087646484\n",
      "Epoch: 95400 | MSE Train Loss: 110.40707397460938 | MSE Test Loss: 100.31739044189453\n",
      "Epoch: 95500 | MSE Train Loss: 110.40699005126953 | MSE Test Loss: 100.31707000732422\n",
      "Epoch: 95600 | MSE Train Loss: 110.40689086914062 | MSE Test Loss: 100.31675720214844\n",
      "Epoch: 95700 | MSE Train Loss: 110.40680694580078 | MSE Test Loss: 100.31645965576172\n",
      "Epoch: 95800 | MSE Train Loss: 110.40673065185547 | MSE Test Loss: 100.31612396240234\n",
      "Epoch: 95900 | MSE Train Loss: 110.40664672851562 | MSE Test Loss: 100.3158187866211\n",
      "Epoch: 96000 | MSE Train Loss: 110.40656280517578 | MSE Test Loss: 100.31549835205078\n",
      "Epoch: 96100 | MSE Train Loss: 110.40647888183594 | MSE Test Loss: 100.3152084350586\n",
      "Epoch: 96200 | MSE Train Loss: 110.40641784667969 | MSE Test Loss: 100.31489562988281\n",
      "Epoch: 96300 | MSE Train Loss: 110.40631866455078 | MSE Test Loss: 100.3145751953125\n",
      "Epoch: 96400 | MSE Train Loss: 110.40623474121094 | MSE Test Loss: 100.31425476074219\n",
      "Epoch: 96500 | MSE Train Loss: 110.40615844726562 | MSE Test Loss: 100.31395721435547\n",
      "Epoch: 96600 | MSE Train Loss: 110.40607452392578 | MSE Test Loss: 100.31365966796875\n",
      "Epoch: 96700 | MSE Train Loss: 110.40599822998047 | MSE Test Loss: 100.31334686279297\n",
      "Epoch: 96800 | MSE Train Loss: 110.4059066772461 | MSE Test Loss: 100.31303405761719\n",
      "Epoch: 96900 | MSE Train Loss: 110.40583801269531 | MSE Test Loss: 100.3127212524414\n",
      "Epoch: 97000 | MSE Train Loss: 110.40574645996094 | MSE Test Loss: 100.31243133544922\n",
      "Epoch: 97100 | MSE Train Loss: 110.40567016601562 | MSE Test Loss: 100.3121109008789\n",
      "Epoch: 97200 | MSE Train Loss: 110.40559387207031 | MSE Test Loss: 100.31180572509766\n",
      "Epoch: 97300 | MSE Train Loss: 110.405517578125 | MSE Test Loss: 100.31149291992188\n",
      "Epoch: 97400 | MSE Train Loss: 110.40544128417969 | MSE Test Loss: 100.31120300292969\n",
      "Epoch: 97500 | MSE Train Loss: 110.4053726196289 | MSE Test Loss: 100.3108901977539\n",
      "Epoch: 97600 | MSE Train Loss: 110.40528869628906 | MSE Test Loss: 100.31057739257812\n",
      "Epoch: 97700 | MSE Train Loss: 110.40522003173828 | MSE Test Loss: 100.3102798461914\n",
      "Epoch: 97800 | MSE Train Loss: 110.40514373779297 | MSE Test Loss: 100.30996704101562\n",
      "Epoch: 97900 | MSE Train Loss: 110.40507507324219 | MSE Test Loss: 100.30967712402344\n",
      "Epoch: 98000 | MSE Train Loss: 110.40499877929688 | MSE Test Loss: 100.30937194824219\n",
      "Epoch: 98100 | MSE Train Loss: 110.40492248535156 | MSE Test Loss: 100.3090591430664\n",
      "Epoch: 98200 | MSE Train Loss: 110.40484619140625 | MSE Test Loss: 100.30876922607422\n",
      "Epoch: 98300 | MSE Train Loss: 110.4047622680664 | MSE Test Loss: 100.3084716796875\n",
      "Epoch: 98400 | MSE Train Loss: 110.40470123291016 | MSE Test Loss: 100.30816650390625\n",
      "Epoch: 98500 | MSE Train Loss: 110.40462493896484 | MSE Test Loss: 100.307861328125\n",
      "Epoch: 98600 | MSE Train Loss: 110.40455627441406 | MSE Test Loss: 100.30755615234375\n",
      "Epoch: 98700 | MSE Train Loss: 110.40448760986328 | MSE Test Loss: 100.30726623535156\n",
      "Epoch: 98800 | MSE Train Loss: 110.40442657470703 | MSE Test Loss: 100.30696868896484\n",
      "Epoch: 98900 | MSE Train Loss: 110.40434265136719 | MSE Test Loss: 100.30665588378906\n",
      "Epoch: 99000 | MSE Train Loss: 110.40426635742188 | MSE Test Loss: 100.30635833740234\n",
      "Epoch: 99100 | MSE Train Loss: 110.4041976928711 | MSE Test Loss: 100.30608367919922\n",
      "Epoch: 99200 | MSE Train Loss: 110.40415954589844 | MSE Test Loss: 100.30577850341797\n",
      "Epoch: 99300 | MSE Train Loss: 110.4040756225586 | MSE Test Loss: 100.30548095703125\n",
      "Epoch: 99400 | MSE Train Loss: 110.40399932861328 | MSE Test Loss: 100.30519104003906\n",
      "Epoch: 99500 | MSE Train Loss: 110.40393829345703 | MSE Test Loss: 100.30487823486328\n",
      "Epoch: 99600 | MSE Train Loss: 110.40387725830078 | MSE Test Loss: 100.30460357666016\n",
      "Epoch: 99700 | MSE Train Loss: 110.40381622314453 | MSE Test Loss: 100.30431365966797\n",
      "Epoch: 99800 | MSE Train Loss: 110.40372467041016 | MSE Test Loss: 100.30400848388672\n",
      "Epoch: 99900 | MSE Train Loss: 110.40365600585938 | MSE Test Loss: 100.30370330810547\n",
      "Epoch: 100000 | MSE Train Loss: 110.40360260009766 | MSE Test Loss: 100.30342102050781\n",
      "Epoch: 100100 | MSE Train Loss: 110.40354919433594 | MSE Test Loss: 100.3031234741211\n",
      "Epoch: 100200 | MSE Train Loss: 110.40348815917969 | MSE Test Loss: 100.30282592773438\n",
      "Epoch: 100300 | MSE Train Loss: 110.40341186523438 | MSE Test Loss: 100.30252838134766\n",
      "Epoch: 100400 | MSE Train Loss: 110.40335083007812 | MSE Test Loss: 100.30226135253906\n",
      "Epoch: 100500 | MSE Train Loss: 110.40330505371094 | MSE Test Loss: 100.30197143554688\n",
      "Epoch: 100600 | MSE Train Loss: 110.40323638916016 | MSE Test Loss: 100.30167388916016\n",
      "Epoch: 100700 | MSE Train Loss: 110.40315246582031 | MSE Test Loss: 100.30137634277344\n",
      "Epoch: 100800 | MSE Train Loss: 110.40309143066406 | MSE Test Loss: 100.30107879638672\n",
      "Epoch: 100900 | MSE Train Loss: 110.4030532836914 | MSE Test Loss: 100.3008041381836\n",
      "Epoch: 101000 | MSE Train Loss: 110.40298461914062 | MSE Test Loss: 100.3005142211914\n",
      "Epoch: 101100 | MSE Train Loss: 110.40292358398438 | MSE Test Loss: 100.30021667480469\n",
      "Epoch: 101200 | MSE Train Loss: 110.40287017822266 | MSE Test Loss: 100.29993438720703\n",
      "Epoch: 101300 | MSE Train Loss: 110.40279388427734 | MSE Test Loss: 100.2996597290039\n",
      "Epoch: 101400 | MSE Train Loss: 110.40274810791016 | MSE Test Loss: 100.29936981201172\n",
      "Epoch: 101500 | MSE Train Loss: 110.4026870727539 | MSE Test Loss: 100.29907989501953\n",
      "Epoch: 101600 | MSE Train Loss: 110.40264129638672 | MSE Test Loss: 100.29879760742188\n",
      "Epoch: 101700 | MSE Train Loss: 110.40261840820312 | MSE Test Loss: 100.29862213134766\n",
      "Epoch: 101800 | MSE Train Loss: 110.40258026123047 | MSE Test Loss: 100.29844665527344\n",
      "Epoch: 101900 | MSE Train Loss: 110.40254974365234 | MSE Test Loss: 100.29827880859375\n",
      "Epoch: 102000 | MSE Train Loss: 110.40251922607422 | MSE Test Loss: 100.29812622070312\n",
      "Epoch: 102100 | MSE Train Loss: 110.4024887084961 | MSE Test Loss: 100.29795837402344\n",
      "Epoch: 102200 | MSE Train Loss: 110.4024658203125 | MSE Test Loss: 100.29782104492188\n",
      "Epoch: 102300 | MSE Train Loss: 110.40243530273438 | MSE Test Loss: 100.29767608642578\n",
      "Epoch: 102400 | MSE Train Loss: 110.40239715576172 | MSE Test Loss: 100.29753112792969\n",
      "Epoch: 102500 | MSE Train Loss: 110.40237426757812 | MSE Test Loss: 100.2973861694336\n",
      "Epoch: 102600 | MSE Train Loss: 110.40235900878906 | MSE Test Loss: 100.29725646972656\n",
      "Epoch: 102700 | MSE Train Loss: 110.40233612060547 | MSE Test Loss: 100.297119140625\n",
      "Epoch: 102800 | MSE Train Loss: 110.40230560302734 | MSE Test Loss: 100.29696655273438\n",
      "Epoch: 102900 | MSE Train Loss: 110.40226745605469 | MSE Test Loss: 100.29682922363281\n",
      "Epoch: 103000 | MSE Train Loss: 110.40225219726562 | MSE Test Loss: 100.29668426513672\n",
      "Epoch: 103100 | MSE Train Loss: 110.4021987915039 | MSE Test Loss: 100.29653930664062\n",
      "Epoch: 103200 | MSE Train Loss: 110.4021987915039 | MSE Test Loss: 100.29639434814453\n",
      "Epoch: 103300 | MSE Train Loss: 110.40214538574219 | MSE Test Loss: 100.2962646484375\n",
      "Epoch: 103400 | MSE Train Loss: 110.40211486816406 | MSE Test Loss: 100.29612731933594\n",
      "Epoch: 103500 | MSE Train Loss: 110.40210723876953 | MSE Test Loss: 100.29598236083984\n",
      "Epoch: 103600 | MSE Train Loss: 110.4020767211914 | MSE Test Loss: 100.29583740234375\n",
      "Epoch: 103700 | MSE Train Loss: 110.40205383300781 | MSE Test Loss: 100.29570007324219\n",
      "Epoch: 103800 | MSE Train Loss: 110.40202331542969 | MSE Test Loss: 100.29556274414062\n",
      "Epoch: 103900 | MSE Train Loss: 110.4020004272461 | MSE Test Loss: 100.29542541503906\n",
      "Epoch: 104000 | MSE Train Loss: 110.40196990966797 | MSE Test Loss: 100.29528045654297\n",
      "Epoch: 104100 | MSE Train Loss: 110.40193939208984 | MSE Test Loss: 100.29513549804688\n",
      "Epoch: 104200 | MSE Train Loss: 110.40191650390625 | MSE Test Loss: 100.29499816894531\n",
      "Epoch: 104300 | MSE Train Loss: 110.40190887451172 | MSE Test Loss: 100.29486083984375\n",
      "Epoch: 104400 | MSE Train Loss: 110.40188598632812 | MSE Test Loss: 100.29471588134766\n",
      "Epoch: 104500 | MSE Train Loss: 110.4018325805664 | MSE Test Loss: 100.29458618164062\n",
      "Epoch: 104600 | MSE Train Loss: 110.40181732177734 | MSE Test Loss: 100.29444122314453\n",
      "Epoch: 104700 | MSE Train Loss: 110.40178680419922 | MSE Test Loss: 100.2942886352539\n",
      "Epoch: 104800 | MSE Train Loss: 110.40176391601562 | MSE Test Loss: 100.29415893554688\n",
      "Epoch: 104900 | MSE Train Loss: 110.40174102783203 | MSE Test Loss: 100.29402160644531\n",
      "Epoch: 105000 | MSE Train Loss: 110.4017105102539 | MSE Test Loss: 100.29389953613281\n",
      "Epoch: 105100 | MSE Train Loss: 110.40170288085938 | MSE Test Loss: 100.29375457763672\n",
      "Epoch: 105200 | MSE Train Loss: 110.40167236328125 | MSE Test Loss: 100.29360961914062\n",
      "Epoch: 105300 | MSE Train Loss: 110.40164947509766 | MSE Test Loss: 100.29347229003906\n",
      "Epoch: 105400 | MSE Train Loss: 110.40162658691406 | MSE Test Loss: 100.29332733154297\n",
      "Epoch: 105500 | MSE Train Loss: 110.40159606933594 | MSE Test Loss: 100.29319763183594\n",
      "Epoch: 105600 | MSE Train Loss: 110.40155792236328 | MSE Test Loss: 100.29305267333984\n",
      "Epoch: 105700 | MSE Train Loss: 110.40155029296875 | MSE Test Loss: 100.29290771484375\n",
      "Epoch: 105800 | MSE Train Loss: 110.4015121459961 | MSE Test Loss: 100.29277038574219\n",
      "Epoch: 105900 | MSE Train Loss: 110.40148162841797 | MSE Test Loss: 100.29264068603516\n",
      "Epoch: 106000 | MSE Train Loss: 110.40147399902344 | MSE Test Loss: 100.2925033569336\n",
      "Epoch: 106100 | MSE Train Loss: 110.40144348144531 | MSE Test Loss: 100.29237365722656\n",
      "Epoch: 106200 | MSE Train Loss: 110.40142059326172 | MSE Test Loss: 100.29222869873047\n",
      "Epoch: 106300 | MSE Train Loss: 110.40140533447266 | MSE Test Loss: 100.29208374023438\n",
      "Epoch: 106400 | MSE Train Loss: 110.40137481689453 | MSE Test Loss: 100.29195404052734\n",
      "Epoch: 106500 | MSE Train Loss: 110.40135955810547 | MSE Test Loss: 100.29180908203125\n",
      "Epoch: 106600 | MSE Train Loss: 110.40132904052734 | MSE Test Loss: 100.29167175292969\n",
      "Epoch: 106700 | MSE Train Loss: 110.40131378173828 | MSE Test Loss: 100.29154205322266\n",
      "Epoch: 106800 | MSE Train Loss: 110.40129852294922 | MSE Test Loss: 100.29141998291016\n",
      "Epoch: 106900 | MSE Train Loss: 110.40127563476562 | MSE Test Loss: 100.29127502441406\n",
      "Epoch: 107000 | MSE Train Loss: 110.4012451171875 | MSE Test Loss: 100.29113006591797\n",
      "Epoch: 107100 | MSE Train Loss: 110.4012222290039 | MSE Test Loss: 100.29100036621094\n",
      "Epoch: 107200 | MSE Train Loss: 110.40119934082031 | MSE Test Loss: 100.29086303710938\n",
      "Epoch: 107300 | MSE Train Loss: 110.40117645263672 | MSE Test Loss: 100.29072570800781\n",
      "Epoch: 107400 | MSE Train Loss: 110.4011459350586 | MSE Test Loss: 100.29058837890625\n",
      "Epoch: 107500 | MSE Train Loss: 110.40111541748047 | MSE Test Loss: 100.29044342041016\n",
      "Epoch: 107600 | MSE Train Loss: 110.40111541748047 | MSE Test Loss: 100.29032135009766\n",
      "Epoch: 107700 | MSE Train Loss: 110.40109252929688 | MSE Test Loss: 100.2901840209961\n",
      "Epoch: 107800 | MSE Train Loss: 110.40106964111328 | MSE Test Loss: 100.29004669189453\n",
      "Epoch: 107900 | MSE Train Loss: 110.40104675292969 | MSE Test Loss: 100.28990936279297\n",
      "Epoch: 108000 | MSE Train Loss: 110.40101623535156 | MSE Test Loss: 100.2897720336914\n",
      "Epoch: 108100 | MSE Train Loss: 110.4010009765625 | MSE Test Loss: 100.28964233398438\n",
      "Epoch: 108200 | MSE Train Loss: 110.40097045898438 | MSE Test Loss: 100.28948974609375\n",
      "Epoch: 108300 | MSE Train Loss: 110.40095520019531 | MSE Test Loss: 100.28936004638672\n",
      "Epoch: 108400 | MSE Train Loss: 110.40093994140625 | MSE Test Loss: 100.28924560546875\n",
      "Epoch: 108500 | MSE Train Loss: 110.40093231201172 | MSE Test Loss: 100.28910064697266\n",
      "Epoch: 108600 | MSE Train Loss: 110.4009017944336 | MSE Test Loss: 100.2889633178711\n",
      "Epoch: 108700 | MSE Train Loss: 110.40087127685547 | MSE Test Loss: 100.28883361816406\n",
      "Epoch: 108800 | MSE Train Loss: 110.4008560180664 | MSE Test Loss: 100.2886962890625\n",
      "Epoch: 108900 | MSE Train Loss: 110.40082550048828 | MSE Test Loss: 100.28855895996094\n",
      "Epoch: 109000 | MSE Train Loss: 110.40080261230469 | MSE Test Loss: 100.28842163085938\n",
      "Epoch: 109100 | MSE Train Loss: 110.4007797241211 | MSE Test Loss: 100.28828430175781\n",
      "Epoch: 109200 | MSE Train Loss: 110.40076446533203 | MSE Test Loss: 100.28814697265625\n",
      "Epoch: 109300 | MSE Train Loss: 110.40074157714844 | MSE Test Loss: 100.28803253173828\n",
      "Epoch: 109400 | MSE Train Loss: 110.4007339477539 | MSE Test Loss: 100.28789520263672\n",
      "Epoch: 109500 | MSE Train Loss: 110.40071868896484 | MSE Test Loss: 100.28776550292969\n",
      "Epoch: 109600 | MSE Train Loss: 110.40069580078125 | MSE Test Loss: 100.28763580322266\n",
      "Epoch: 109700 | MSE Train Loss: 110.40067291259766 | MSE Test Loss: 100.28749084472656\n",
      "Epoch: 109800 | MSE Train Loss: 110.40064239501953 | MSE Test Loss: 100.287353515625\n",
      "Epoch: 109900 | MSE Train Loss: 110.400634765625 | MSE Test Loss: 100.28721618652344\n",
      "Epoch: 110000 | MSE Train Loss: 110.40061950683594 | MSE Test Loss: 100.28707885742188\n",
      "Epoch: 110100 | MSE Train Loss: 110.40059661865234 | MSE Test Loss: 100.28697204589844\n",
      "Epoch: 110200 | MSE Train Loss: 110.40058135986328 | MSE Test Loss: 100.28682708740234\n",
      "Epoch: 110300 | MSE Train Loss: 110.40056610107422 | MSE Test Loss: 100.28669738769531\n",
      "Epoch: 110400 | MSE Train Loss: 110.40052795410156 | MSE Test Loss: 100.28656005859375\n",
      "Epoch: 110500 | MSE Train Loss: 110.40052032470703 | MSE Test Loss: 100.28643035888672\n",
      "Epoch: 110600 | MSE Train Loss: 110.40049743652344 | MSE Test Loss: 100.28628540039062\n",
      "Epoch: 110700 | MSE Train Loss: 110.40046691894531 | MSE Test Loss: 100.28616333007812\n",
      "Epoch: 110800 | MSE Train Loss: 110.40046691894531 | MSE Test Loss: 100.2860336303711\n",
      "Epoch: 110900 | MSE Train Loss: 110.40044403076172 | MSE Test Loss: 100.28590393066406\n",
      "Epoch: 111000 | MSE Train Loss: 110.40042114257812 | MSE Test Loss: 100.28578186035156\n",
      "Epoch: 111100 | MSE Train Loss: 110.4004135131836 | MSE Test Loss: 100.28563690185547\n",
      "Epoch: 111200 | MSE Train Loss: 110.400390625 | MSE Test Loss: 100.2854995727539\n",
      "Epoch: 111300 | MSE Train Loss: 110.40037536621094 | MSE Test Loss: 100.2853775024414\n",
      "Epoch: 111400 | MSE Train Loss: 110.40036010742188 | MSE Test Loss: 100.28524017333984\n",
      "Epoch: 111500 | MSE Train Loss: 110.40033721923828 | MSE Test Loss: 100.28511047363281\n",
      "Epoch: 111600 | MSE Train Loss: 110.40032196044922 | MSE Test Loss: 100.28496551513672\n",
      "Epoch: 111700 | MSE Train Loss: 110.40029907226562 | MSE Test Loss: 100.28484344482422\n",
      "Epoch: 111800 | MSE Train Loss: 110.40029907226562 | MSE Test Loss: 100.28472900390625\n",
      "Epoch: 111900 | MSE Train Loss: 110.40027618408203 | MSE Test Loss: 100.28459167480469\n",
      "Epoch: 112000 | MSE Train Loss: 110.40025329589844 | MSE Test Loss: 100.28446197509766\n",
      "Epoch: 112100 | MSE Train Loss: 110.4002456665039 | MSE Test Loss: 100.2843246459961\n",
      "Epoch: 112200 | MSE Train Loss: 110.40020751953125 | MSE Test Loss: 100.28419494628906\n",
      "Epoch: 112300 | MSE Train Loss: 110.40019989013672 | MSE Test Loss: 100.2840576171875\n",
      "Epoch: 112400 | MSE Train Loss: 110.40018463134766 | MSE Test Loss: 100.28392028808594\n",
      "Epoch: 112500 | MSE Train Loss: 110.40017700195312 | MSE Test Loss: 100.28380584716797\n",
      "Epoch: 112600 | MSE Train Loss: 110.400146484375 | MSE Test Loss: 100.28367614746094\n",
      "Epoch: 112700 | MSE Train Loss: 110.400146484375 | MSE Test Loss: 100.28356170654297\n",
      "Epoch: 112800 | MSE Train Loss: 110.4001235961914 | MSE Test Loss: 100.2834243774414\n",
      "Epoch: 112900 | MSE Train Loss: 110.40010070800781 | MSE Test Loss: 100.28327941894531\n",
      "Epoch: 113000 | MSE Train Loss: 110.40008544921875 | MSE Test Loss: 100.28316497802734\n",
      "Epoch: 113100 | MSE Train Loss: 110.40007781982422 | MSE Test Loss: 100.28302764892578\n",
      "Epoch: 113200 | MSE Train Loss: 110.40005493164062 | MSE Test Loss: 100.28289794921875\n",
      "Epoch: 113300 | MSE Train Loss: 110.4000473022461 | MSE Test Loss: 100.28276824951172\n",
      "Epoch: 113400 | MSE Train Loss: 110.40001678466797 | MSE Test Loss: 100.28263092041016\n",
      "Epoch: 113500 | MSE Train Loss: 110.40003204345703 | MSE Test Loss: 100.28252410888672\n",
      "Epoch: 113600 | MSE Train Loss: 110.4000015258789 | MSE Test Loss: 100.28239440917969\n",
      "Epoch: 113700 | MSE Train Loss: 110.39998626708984 | MSE Test Loss: 100.2822494506836\n",
      "Epoch: 113800 | MSE Train Loss: 110.39997100830078 | MSE Test Loss: 100.2821273803711\n",
      "Epoch: 113900 | MSE Train Loss: 110.39995574951172 | MSE Test Loss: 100.28199768066406\n",
      "Epoch: 114000 | MSE Train Loss: 110.39994049072266 | MSE Test Loss: 100.28186798095703\n",
      "Epoch: 114100 | MSE Train Loss: 110.3999252319336 | MSE Test Loss: 100.28173065185547\n",
      "Epoch: 114200 | MSE Train Loss: 110.39990997314453 | MSE Test Loss: 100.28160095214844\n",
      "Epoch: 114300 | MSE Train Loss: 110.39988708496094 | MSE Test Loss: 100.28148651123047\n",
      "Epoch: 114400 | MSE Train Loss: 110.3998794555664 | MSE Test Loss: 100.28135681152344\n",
      "Epoch: 114500 | MSE Train Loss: 110.39987182617188 | MSE Test Loss: 100.2812271118164\n",
      "Epoch: 114600 | MSE Train Loss: 110.39984893798828 | MSE Test Loss: 100.2811050415039\n",
      "Epoch: 114700 | MSE Train Loss: 110.39984130859375 | MSE Test Loss: 100.28097534179688\n",
      "Epoch: 114800 | MSE Train Loss: 110.39982604980469 | MSE Test Loss: 100.2808609008789\n",
      "Epoch: 114900 | MSE Train Loss: 110.39981079101562 | MSE Test Loss: 100.28070831298828\n",
      "Epoch: 115000 | MSE Train Loss: 110.39979553222656 | MSE Test Loss: 100.28058624267578\n",
      "Epoch: 115100 | MSE Train Loss: 110.3997802734375 | MSE Test Loss: 100.28047180175781\n",
      "Epoch: 115200 | MSE Train Loss: 110.39978790283203 | MSE Test Loss: 100.28034210205078\n",
      "Epoch: 115300 | MSE Train Loss: 110.3997573852539 | MSE Test Loss: 100.28021240234375\n",
      "Epoch: 115400 | MSE Train Loss: 110.39974975585938 | MSE Test Loss: 100.28009033203125\n",
      "Epoch: 115500 | MSE Train Loss: 110.39974212646484 | MSE Test Loss: 100.27995300292969\n",
      "Epoch: 115600 | MSE Train Loss: 110.39971923828125 | MSE Test Loss: 100.27983856201172\n",
      "Epoch: 115700 | MSE Train Loss: 110.39971160888672 | MSE Test Loss: 100.2796859741211\n",
      "Epoch: 115800 | MSE Train Loss: 110.39969635009766 | MSE Test Loss: 100.27957153320312\n",
      "Epoch: 115900 | MSE Train Loss: 110.3996810913086 | MSE Test Loss: 100.2794418334961\n",
      "Epoch: 116000 | MSE Train Loss: 110.39967346191406 | MSE Test Loss: 100.27932739257812\n",
      "Epoch: 116100 | MSE Train Loss: 110.3996810913086 | MSE Test Loss: 100.27921295166016\n",
      "Epoch: 116200 | MSE Train Loss: 110.399658203125 | MSE Test Loss: 100.2790756225586\n",
      "Epoch: 116300 | MSE Train Loss: 110.39965057373047 | MSE Test Loss: 100.2789535522461\n",
      "Epoch: 116400 | MSE Train Loss: 110.39962005615234 | MSE Test Loss: 100.27882385253906\n",
      "Epoch: 116500 | MSE Train Loss: 110.39961242675781 | MSE Test Loss: 100.27870178222656\n",
      "Epoch: 116600 | MSE Train Loss: 110.39959716796875 | MSE Test Loss: 100.27857208251953\n",
      "Epoch: 116700 | MSE Train Loss: 110.39958190917969 | MSE Test Loss: 100.27845001220703\n",
      "Epoch: 116800 | MSE Train Loss: 110.3995590209961 | MSE Test Loss: 100.27833557128906\n",
      "Epoch: 116900 | MSE Train Loss: 110.39956665039062 | MSE Test Loss: 100.27820587158203\n",
      "Epoch: 117000 | MSE Train Loss: 110.3995590209961 | MSE Test Loss: 100.27806854248047\n",
      "Epoch: 117100 | MSE Train Loss: 110.39954376220703 | MSE Test Loss: 100.2779541015625\n",
      "Epoch: 117200 | MSE Train Loss: 110.3995361328125 | MSE Test Loss: 100.27781677246094\n",
      "Epoch: 117300 | MSE Train Loss: 110.39952087402344 | MSE Test Loss: 100.27769470214844\n",
      "Epoch: 117400 | MSE Train Loss: 110.39950561523438 | MSE Test Loss: 100.27757263183594\n",
      "Epoch: 117500 | MSE Train Loss: 110.39949798583984 | MSE Test Loss: 100.27745056152344\n",
      "Epoch: 117600 | MSE Train Loss: 110.39948272705078 | MSE Test Loss: 100.27733612060547\n",
      "Epoch: 117700 | MSE Train Loss: 110.39949035644531 | MSE Test Loss: 100.27721405029297\n",
      "Epoch: 117800 | MSE Train Loss: 110.39948272705078 | MSE Test Loss: 100.2770767211914\n",
      "Epoch: 117900 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27698516845703\n",
      "Epoch: 118000 | MSE Train Loss: 110.39946746826172 | MSE Test Loss: 100.27694702148438\n",
      "Epoch: 118100 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27693176269531\n",
      "Epoch: 118200 | MSE Train Loss: 110.39946746826172 | MSE Test Loss: 100.27692413330078\n",
      "Epoch: 118300 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27690124511719\n",
      "Epoch: 118400 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27690124511719\n",
      "Epoch: 118500 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27690124511719\n",
      "Epoch: 118600 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27690124511719\n",
      "Epoch: 118700 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 118800 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 118900 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119000 | MSE Train Loss: 110.39945983886719 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 119100 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119200 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119300 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119400 | MSE Train Loss: 110.39945220947266 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119500 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119600 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119700 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 119800 | MSE Train Loss: 110.3994369506836 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 119900 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120000 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27689361572266\n",
      "Epoch: 120100 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120200 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120300 | MSE Train Loss: 110.39944458007812 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 120900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 121900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 122300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 122800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 122900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 123400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27688598632812\n",
      "Epoch: 123500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 123900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 124000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 124100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27687072753906\n",
      "Epoch: 124800 | MSE Train Loss: 110.3994369506836 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 124900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 125700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 125900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 126000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 126100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 126200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.2768783569336\n",
      "Epoch: 126300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 126900 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127000 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127100 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127200 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127400 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127500 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127600 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127700 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127800 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 127900 | MSE Train Loss: 110.39942169189453 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128000 | MSE Train Loss: 110.39942169189453 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128100 | MSE Train Loss: 110.39942169189453 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 128200 | MSE Train Loss: 110.3994140625 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128300 | MSE Train Loss: 110.39942932128906 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128400 | MSE Train Loss: 110.3994140625 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128500 | MSE Train Loss: 110.3994140625 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128600 | MSE Train Loss: 110.3994140625 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128700 | MSE Train Loss: 110.39940643310547 | MSE Test Loss: 100.27686309814453\n",
      "Epoch: 128800 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 128900 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129000 | MSE Train Loss: 110.3994140625 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129100 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129200 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129300 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129400 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129500 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129600 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 129700 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 129800 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 129900 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 130000 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 130100 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684783935547\n",
      "Epoch: 130200 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 130300 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 130400 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 130500 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 130600 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 130700 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 130800 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 130900 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131000 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.27684020996094\n",
      "Epoch: 131100 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131200 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131300 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131400 | MSE Train Loss: 110.39939880371094 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131500 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131600 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131700 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131800 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 131900 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132000 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132100 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132200 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132300 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132400 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132500 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132600 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132700 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132800 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 132900 | MSE Train Loss: 110.3993911743164 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 133000 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 133100 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 133200 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.2768325805664\n",
      "Epoch: 133300 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133400 | MSE Train Loss: 110.39938354492188 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133500 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133600 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133700 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133800 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 133900 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 134000 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27682495117188\n",
      "Epoch: 134100 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134200 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134300 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134400 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134500 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134600 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134700 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134800 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 134900 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 135000 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135100 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135200 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135300 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 135400 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135500 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135600 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135700 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135800 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 135900 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136000 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 136100 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 136200 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136300 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136400 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136500 | MSE Train Loss: 110.39937591552734 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 136600 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136700 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27681732177734\n",
      "Epoch: 136800 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 136900 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137000 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137100 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137200 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137300 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137400 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137500 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137600 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137700 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137800 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 137900 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 138000 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 138100 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138200 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138300 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138400 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138500 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680969238281\n",
      "Epoch: 138600 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138700 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138800 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 138900 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139000 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139100 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139200 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139300 | MSE Train Loss: 110.39935302734375 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139400 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139500 | MSE Train Loss: 110.39936065673828 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139600 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139700 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139800 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 139900 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140000 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140100 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140200 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140300 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140400 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140500 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140600 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27680206298828\n",
      "Epoch: 140700 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 140800 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 140900 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 141000 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141100 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141200 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141300 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 141400 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 141500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141600 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27679443359375\n",
      "Epoch: 141700 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141800 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 141900 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142000 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142100 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142200 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142300 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142400 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142600 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142700 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142800 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 142900 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143000 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143100 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143200 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143300 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143400 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143600 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143700 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143800 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 143900 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144000 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144100 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144200 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 144300 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 144400 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144500 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144600 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144700 | MSE Train Loss: 110.39934539794922 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144800 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 144900 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145000 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145100 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145200 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 145300 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 145400 | MSE Train Loss: 110.39933776855469 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 145500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 145600 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145700 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145800 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 145900 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146000 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146100 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146200 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 146300 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27678680419922\n",
      "Epoch: 146400 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146600 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146700 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 146800 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 146900 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147000 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147100 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 147200 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 147300 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677917480469\n",
      "Epoch: 147400 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147500 | MSE Train Loss: 110.39933013916016 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147600 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147700 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147800 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 147900 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148000 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148100 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148200 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148300 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148400 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148500 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148600 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148700 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148800 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 148900 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149000 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149100 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149200 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149300 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149400 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149500 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149600 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149700 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149800 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 149900 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 150000 | MSE Train Loss: 110.39932250976562 | MSE Test Loss: 100.27677154541016\n",
      "Epoch: 150100 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150200 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150300 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150400 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150500 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150600 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150700 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150800 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 150900 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151000 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151100 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151200 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151300 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151400 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151500 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151600 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151700 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151800 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.2767562866211\n",
      "Epoch: 151900 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152000 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152100 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152200 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152300 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152400 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152500 | MSE Train Loss: 110.39929962158203 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152600 | MSE Train Loss: 110.39929962158203 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152700 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674865722656\n",
      "Epoch: 152800 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 152900 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153000 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153100 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153200 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153300 | MSE Train Loss: 110.39930725097656 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153400 | MSE Train Loss: 110.39929962158203 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153500 | MSE Train Loss: 110.39929962158203 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153600 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153700 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153800 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 153900 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154000 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154100 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154200 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154300 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154400 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154500 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154600 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154700 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154800 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 154900 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.27674102783203\n",
      "Epoch: 155000 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155100 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155200 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155300 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155400 | MSE Train Loss: 110.3992919921875 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155500 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155600 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155700 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155800 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 155900 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156000 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156100 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156200 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 156300 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156400 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156500 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156600 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156700 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156800 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 156900 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 157000 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157100 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157200 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157300 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 157400 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 157500 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157600 | MSE Train Loss: 110.39928436279297 | MSE Test Loss: 100.2767333984375\n",
      "Epoch: 157700 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157800 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 157900 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158000 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158100 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158200 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158300 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158400 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158500 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158600 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158700 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158800 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.27672576904297\n",
      "Epoch: 158900 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159000 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159100 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159200 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159300 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159400 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159500 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159600 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159700 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159800 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 159900 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160000 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160100 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160200 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160300 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160400 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160500 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160600 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160700 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160800 | MSE Train Loss: 110.3992691040039 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 160900 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161000 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161100 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161200 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161300 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161400 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161500 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161600 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161700 | MSE Train Loss: 110.39926147460938 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161800 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 161900 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 162000 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.2767105102539\n",
      "Epoch: 162100 | MSE Train Loss: 110.39923858642578 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162200 | MSE Train Loss: 110.39923858642578 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162300 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162400 | MSE Train Loss: 110.39924621582031 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162500 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162600 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162700 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162800 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 162900 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 163000 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 163100 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 163200 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163300 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163400 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163500 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163600 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163700 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163800 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 163900 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 164000 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 164100 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 164200 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 164300 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27670288085938\n",
      "Epoch: 164400 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 164500 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 164600 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 164700 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 164800 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 164900 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165000 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165100 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165200 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165300 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165400 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165500 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 165600 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27669525146484\n",
      "Epoch: 165700 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 165800 | MSE Train Loss: 110.39922332763672 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 165900 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166000 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166100 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166200 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166300 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166400 | MSE Train Loss: 110.39923095703125 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166500 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166600 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166700 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166800 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 166900 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167000 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167100 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167200 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167300 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167400 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667999267578\n",
      "Epoch: 167500 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 167600 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 167700 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 167800 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 167900 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168000 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168100 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168200 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168300 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168400 | MSE Train Loss: 110.39921569824219 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168500 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168600 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168700 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168800 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 168900 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169000 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169100 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169200 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169300 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169400 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169500 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169600 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169700 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169800 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 169900 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 170000 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170100 | MSE Train Loss: 110.39920806884766 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 170200 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170300 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170400 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170500 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170600 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 170700 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 170800 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 170900 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 171000 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27667236328125\n",
      "Epoch: 171100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171500 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171800 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 171900 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172500 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 172600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172800 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 172900 | MSE Train Loss: 110.39920043945312 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 173500 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 173600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 173700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 173800 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 173900 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 174000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 174100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 174300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174500 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 174600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174800 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 174900 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 175000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 175100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 175200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27666473388672\n",
      "Epoch: 175300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 175400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27665710449219\n",
      "Epoch: 175500 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 175600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 175700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 175800 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 175900 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176400 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176500 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176600 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176700 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176800 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 176900 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177000 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177100 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177200 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177300 | MSE Train Loss: 110.3991928100586 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177400 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177500 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177600 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664947509766\n",
      "Epoch: 177700 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 177800 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 177900 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178000 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178100 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178200 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178300 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178400 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178500 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178600 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178700 | MSE Train Loss: 110.399169921875 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178800 | MSE Train Loss: 110.399169921875 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 178900 | MSE Train Loss: 110.399169921875 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 179000 | MSE Train Loss: 110.399169921875 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 179100 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179200 | MSE Train Loss: 110.39917755126953 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179300 | MSE Train Loss: 110.39916229248047 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179400 | MSE Train Loss: 110.39916229248047 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179500 | MSE Train Loss: 110.39916229248047 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 179600 | MSE Train Loss: 110.39916229248047 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179700 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 179800 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 179900 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 180000 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 180100 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27664184570312\n",
      "Epoch: 180200 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180300 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180400 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180500 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180600 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180700 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180800 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 180900 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181000 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181100 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181200 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181300 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181400 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181500 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181600 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.2766342163086\n",
      "Epoch: 181700 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 181800 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 181900 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 182000 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 182100 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 182200 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27662658691406\n",
      "Epoch: 182300 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182400 | MSE Train Loss: 110.39915466308594 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182500 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182600 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182700 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182800 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 182900 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183000 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183100 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183200 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183300 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183400 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183500 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 183600 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 183700 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183800 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 183900 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184000 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184100 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184200 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184300 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184400 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184500 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184600 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184700 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.27661895751953\n",
      "Epoch: 184800 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 184900 | MSE Train Loss: 110.3991470336914 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 185000 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 185100 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 185200 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.276611328125\n",
      "Epoch: 185300 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 185400 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 185500 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 185600 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 185700 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 185800 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 185900 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186000 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186100 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186200 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186300 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186400 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186500 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186600 | MSE Train Loss: 110.39913940429688 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186700 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 186800 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 186900 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 187000 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27660369873047\n",
      "Epoch: 187100 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187200 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187300 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187400 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187500 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187600 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187700 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187800 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 187900 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188000 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188100 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188200 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188300 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188400 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188500 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.2765884399414\n",
      "Epoch: 188600 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.2765884399414\n",
      "Epoch: 188700 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188800 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27659606933594\n",
      "Epoch: 188900 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189000 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189100 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189200 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189300 | MSE Train Loss: 110.39912414550781 | MSE Test Loss: 100.2765884399414\n",
      "Epoch: 189400 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189500 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.2765884399414\n",
      "Epoch: 189600 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189700 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189800 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 189900 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190000 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190100 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190200 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190300 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190400 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190500 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190600 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190700 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190800 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 190900 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191000 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191100 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191200 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191300 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191400 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191500 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27657318115234\n",
      "Epoch: 191600 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191700 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191800 | MSE Train Loss: 110.39910888671875 | MSE Test Loss: 100.27658081054688\n",
      "Epoch: 191900 | MSE Train Loss: 110.39910125732422 | MSE Test Loss: 100.27657318115234\n",
      "Epoch: 192000 | MSE Train Loss: 110.39910125732422 | MSE Test Loss: 100.27657318115234\n",
      "Epoch: 192100 | MSE Train Loss: 110.39910125732422 | MSE Test Loss: 100.27657318115234\n",
      "Epoch: 192200 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192300 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192400 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192500 | MSE Train Loss: 110.39910125732422 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192600 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192700 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192800 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 192900 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193000 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193100 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193200 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193300 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193400 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193500 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193600 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193700 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193800 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 193900 | MSE Train Loss: 110.39909362792969 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194000 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194100 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194200 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194300 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194400 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194500 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194600 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194700 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194800 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 194900 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 195000 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27656555175781\n",
      "Epoch: 195100 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655792236328\n",
      "Epoch: 195200 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655792236328\n",
      "Epoch: 195300 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655792236328\n",
      "Epoch: 195400 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 195500 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 195600 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 195700 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 195800 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 195900 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196000 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196100 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196200 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196300 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196400 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196500 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196600 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196700 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196800 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 196900 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197000 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197100 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197200 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27654266357422\n",
      "Epoch: 197300 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27654266357422\n",
      "Epoch: 197400 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27654266357422\n",
      "Epoch: 197500 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197600 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197700 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27655029296875\n",
      "Epoch: 197800 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 197900 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198000 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198100 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198200 | MSE Train Loss: 110.39907836914062 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198300 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198400 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198500 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198600 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198700 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198800 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 198900 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199000 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199100 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199200 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27654266357422\n",
      "Epoch: 199300 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27654266357422\n",
      "Epoch: 199400 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199500 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199600 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199700 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199800 | MSE Train Loss: 110.39908599853516 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 199900 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200000 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200100 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200200 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200300 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200400 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200500 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200600 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200700 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 200800 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 200900 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201000 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201100 | MSE Train Loss: 110.3990707397461 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201200 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201300 | MSE Train Loss: 110.39906311035156 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201400 | MSE Train Loss: 110.39906311035156 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201500 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201600 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201700 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201800 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27653503417969\n",
      "Epoch: 201900 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202000 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202100 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202200 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202300 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202400 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202500 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.27651977539062\n",
      "Epoch: 202600 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 202700 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 202800 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 202900 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203000 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203100 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203200 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203300 | MSE Train Loss: 110.39905548095703 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203400 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203500 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203600 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203700 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203800 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 203900 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204000 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204100 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204200 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204300 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204400 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204500 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204600 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204700 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204800 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 204900 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 205000 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 205100 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 205200 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 205300 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 205400 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 205500 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 205600 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 205700 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 205800 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 205900 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 206000 | MSE Train Loss: 110.39904022216797 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 206100 | MSE Train Loss: 110.39904022216797 | MSE Test Loss: 100.2765121459961\n",
      "Epoch: 206200 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206300 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206400 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 206500 | MSE Train Loss: 110.3990478515625 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206600 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206700 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206800 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27650451660156\n",
      "Epoch: 206900 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207000 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207100 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207200 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207300 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207400 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207500 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207600 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207700 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207800 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 207900 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 208000 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 208100 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 208200 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27649688720703\n",
      "Epoch: 208300 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 208400 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 208500 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 208600 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 208700 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.2764892578125\n",
      "Epoch: 208800 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 208900 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209000 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209100 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209200 | MSE Train Loss: 110.39903259277344 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209300 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209400 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209500 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209600 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209700 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209800 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 209900 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210000 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210100 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210200 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210300 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210400 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210500 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210600 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210700 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210800 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 210900 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211000 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211100 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211200 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211300 | MSE Train Loss: 110.3990249633789 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211400 | MSE Train Loss: 110.39901733398438 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211500 | MSE Train Loss: 110.39901733398438 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211600 | MSE Train Loss: 110.39901733398438 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211700 | MSE Train Loss: 110.39901733398438 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211800 | MSE Train Loss: 110.39901733398438 | MSE Test Loss: 100.27648162841797\n",
      "Epoch: 211900 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212000 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212100 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212200 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212300 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 212400 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212500 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 212600 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212700 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.27647399902344\n",
      "Epoch: 212800 | MSE Train Loss: 110.39900970458984 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 212900 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213000 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213100 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213200 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213300 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213400 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213500 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213600 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213700 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213800 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 213900 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214000 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214100 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214200 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214300 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214400 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 214500 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 214600 | MSE Train Loss: 110.39900207519531 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 214700 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.2764663696289\n",
      "Epoch: 214800 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 214900 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215000 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215100 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215200 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215300 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215400 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215500 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215600 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215700 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215800 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 215900 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216000 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216100 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216200 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216300 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216400 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216500 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216600 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216700 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216800 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 216900 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217000 | MSE Train Loss: 110.39899444580078 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217100 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 217200 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217300 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217400 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217500 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27645111083984\n",
      "Epoch: 217600 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 217700 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 217800 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 217900 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218000 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218100 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218200 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218300 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218400 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218500 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218600 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218700 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218800 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 218900 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219000 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219100 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219200 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219300 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219400 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219500 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219600 | MSE Train Loss: 110.39897918701172 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219700 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 219800 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 219900 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220000 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220100 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220200 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220300 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220400 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27644348144531\n",
      "Epoch: 220500 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 220600 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 220700 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 220800 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 220900 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221000 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221100 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221200 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221300 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221400 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221500 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221600 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221700 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221800 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 221900 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 222000 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 222100 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 222200 | MSE Train Loss: 110.39897155761719 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 222300 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 222400 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 222500 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 222600 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 222700 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 222800 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 222900 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 223000 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 223100 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 223200 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223300 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 223400 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223500 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223600 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223700 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223800 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 223900 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224000 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224100 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224200 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224300 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 224400 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224500 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 224600 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224700 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 224800 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27643585205078\n",
      "Epoch: 224900 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225000 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225100 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225200 | MSE Train Loss: 110.39896392822266 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225300 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225400 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225500 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225600 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225700 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225800 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 225900 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226000 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226100 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226200 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226300 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226400 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226500 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226600 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226700 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226800 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 226900 | MSE Train Loss: 110.39895629882812 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 227000 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 227100 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227200 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27642059326172\n",
      "Epoch: 227300 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227400 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227500 | MSE Train Loss: 110.3989486694336 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227600 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227700 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227800 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 227900 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228000 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228100 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228200 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228300 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228400 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228500 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 228600 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 228700 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 228800 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 228900 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229000 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229100 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27641296386719\n",
      "Epoch: 229200 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229300 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229400 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229500 | MSE Train Loss: 110.39893341064453 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229600 | MSE Train Loss: 110.39893341064453 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229700 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229800 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 229900 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230000 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230100 | MSE Train Loss: 110.39894104003906 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230200 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230300 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230400 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230500 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 230600 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 230700 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 230800 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 230900 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 231000 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 231100 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 231200 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 231300 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 231400 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 231500 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 231600 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27640533447266\n",
      "Epoch: 231700 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 231800 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 231900 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 232000 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 232100 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 232200 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 232300 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 232400 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27639770507812\n",
      "Epoch: 232500 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 232600 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 232700 | MSE Train Loss: 110.3989028930664 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 232800 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 232900 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233000 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233100 | MSE Train Loss: 110.39891815185547 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 233200 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233300 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233400 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233500 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.2763900756836\n",
      "Epoch: 233600 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 233700 | MSE Train Loss: 110.3989028930664 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 233800 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 233900 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234000 | MSE Train Loss: 110.3989028930664 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234100 | MSE Train Loss: 110.39891052246094 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234200 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234300 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234400 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234500 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234600 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234700 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234800 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 234900 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 235000 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27638244628906\n",
      "Epoch: 235100 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235200 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235300 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235400 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235500 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235600 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235700 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235800 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 235900 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236000 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236100 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236200 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236300 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236400 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236500 | MSE Train Loss: 110.39889526367188 | MSE Test Loss: 100.27637481689453\n",
      "Epoch: 236600 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 236700 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 236800 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 236900 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237000 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237100 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237200 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237300 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 237400 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 237500 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 237600 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237700 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 237800 | MSE Train Loss: 110.39888000488281 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 237900 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238000 | MSE Train Loss: 110.39888000488281 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238100 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 238200 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238300 | MSE Train Loss: 110.39888763427734 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 238400 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 238500 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 238600 | MSE Train Loss: 110.39888000488281 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238700 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238800 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 238900 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 239000 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 239100 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 239200 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 239300 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763671875\n",
      "Epoch: 239400 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 239500 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 239600 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 239700 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 239800 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 239900 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 240000 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 240100 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 240200 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 240300 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 240400 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 240500 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 240600 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 240700 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 240800 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 240900 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 241000 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241100 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 241200 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.27635955810547\n",
      "Epoch: 241300 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241400 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241500 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241600 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241700 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241800 | MSE Train Loss: 110.39887237548828 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 241900 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242000 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242100 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242200 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242300 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242400 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242500 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242600 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242700 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242800 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 242900 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 243000 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243100 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 243200 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243300 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243400 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243500 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243600 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243700 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243800 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 243900 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244000 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.2763442993164\n",
      "Epoch: 244100 | MSE Train Loss: 110.39886474609375 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244200 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244300 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244400 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244500 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244600 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244700 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244800 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 244900 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 245000 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 245100 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 245200 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 245300 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 245400 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 245500 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 245600 | MSE Train Loss: 110.39884185791016 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 245700 | MSE Train Loss: 110.39884185791016 | MSE Test Loss: 100.27633666992188\n",
      "Epoch: 245800 | MSE Train Loss: 110.39884185791016 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 245900 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 246000 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 246100 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 246200 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 246300 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 246400 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632141113281\n",
      "Epoch: 246500 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632141113281\n",
      "Epoch: 246600 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 246700 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 246800 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 246900 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247000 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247100 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247200 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247300 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247400 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 247500 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247600 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247700 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 247800 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 247900 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 248000 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 248100 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 248200 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248300 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248400 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248500 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248600 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248700 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 248800 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 248900 | MSE Train Loss: 110.39884948730469 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249000 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249100 | MSE Train Loss: 110.39884185791016 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249200 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249300 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249400 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249500 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249600 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249700 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249800 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 249900 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 250000 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 250100 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 250200 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 250300 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 250400 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 250500 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 250600 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27632904052734\n",
      "Epoch: 250700 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 250800 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 250900 | MSE Train Loss: 110.39883422851562 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251000 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251100 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251200 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251300 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251400 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251500 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251600 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251700 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251800 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 251900 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252000 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252100 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252200 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252300 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252400 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252500 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252600 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252700 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 252800 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27631378173828\n",
      "Epoch: 252900 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253000 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253100 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253200 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253300 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253400 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253500 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253600 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253700 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253800 | MSE Train Loss: 110.3988265991211 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 253900 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254000 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254100 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254200 | MSE Train Loss: 110.39881896972656 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254300 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254400 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254500 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254600 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254700 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254800 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27630615234375\n",
      "Epoch: 254900 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255000 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255100 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255200 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255300 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255400 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255500 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255600 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255700 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255800 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 255900 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 256000 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 256100 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 256200 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 256300 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629852294922\n",
      "Epoch: 256400 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 256500 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 256600 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 256700 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 256800 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 256900 | MSE Train Loss: 110.39881134033203 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257000 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257100 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257200 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257300 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257400 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257500 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257600 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257700 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27629089355469\n",
      "Epoch: 257800 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 257900 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258000 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258100 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258200 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258300 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258400 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258500 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258600 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258700 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258800 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 258900 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259000 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259100 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259200 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259300 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259400 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259500 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259600 | MSE Train Loss: 110.39879608154297 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259700 | MSE Train Loss: 110.39878845214844 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259800 | MSE Train Loss: 110.39878845214844 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 259900 | MSE Train Loss: 110.39878845214844 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 260000 | MSE Train Loss: 110.39878845214844 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 260100 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 260200 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.27628326416016\n",
      "Epoch: 260300 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260400 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260500 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260600 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260700 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260800 | MSE Train Loss: 110.39878845214844 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 260900 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261000 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261100 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261200 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261300 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261400 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261500 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261600 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261700 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261800 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 261900 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262000 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262100 | MSE Train Loss: 110.3987808227539 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262200 | MSE Train Loss: 110.39877319335938 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262300 | MSE Train Loss: 110.39877319335938 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262400 | MSE Train Loss: 110.39877319335938 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262500 | MSE Train Loss: 110.39877319335938 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262600 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 262700 | MSE Train Loss: 110.39877319335938 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 262800 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 262900 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.2762680053711\n",
      "Epoch: 263000 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263100 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263200 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263300 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263400 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263500 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263600 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263700 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263800 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 263900 | MSE Train Loss: 110.39876556396484 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 264000 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 264100 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27626037597656\n",
      "Epoch: 264200 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264300 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264400 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264500 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264600 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264700 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264800 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 264900 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265000 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265100 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265200 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265300 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265400 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265500 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265600 | MSE Train Loss: 110.39875793457031 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265700 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265800 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 265900 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 266000 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 266100 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 266200 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.2762451171875\n",
      "Epoch: 266300 | MSE Train Loss: 110.39875030517578 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 266400 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 266500 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 266600 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27625274658203\n",
      "Epoch: 266700 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 266800 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 266900 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267000 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267100 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267200 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267300 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267400 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267500 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267600 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267700 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 267800 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27622985839844\n",
      "Epoch: 267900 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268000 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268100 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268200 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268300 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268400 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268500 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 268600 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268700 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268800 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 268900 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269000 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 269100 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.27623748779297\n",
      "Epoch: 269200 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269300 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269400 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269500 | MSE Train Loss: 110.39874267578125 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269600 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269700 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269800 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 269900 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270000 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27622985839844\n",
      "Epoch: 270100 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270200 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270300 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270400 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270500 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270600 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270700 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270800 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 270900 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 271000 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 271100 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 271200 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 271300 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 271400 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 271500 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 271600 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 271700 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 271800 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 271900 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272000 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272100 | MSE Train Loss: 110.39872741699219 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272200 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272300 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272400 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 272500 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.2762222290039\n",
      "Epoch: 272600 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272700 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 272800 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 272900 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27621459960938\n",
      "Epoch: 273000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273200 | MSE Train Loss: 110.39871215820312 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273300 | MSE Train Loss: 110.39871215820312 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273400 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273500 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273600 | MSE Train Loss: 110.39871215820312 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273700 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273800 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 273900 | MSE Train Loss: 110.39871978759766 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274200 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274800 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 274900 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 275000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 275100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 275200 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 275300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 275400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 275500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 275600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 275700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 275800 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 275900 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 276000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 276100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 276200 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 276300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 276400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 276500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 276600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 276700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 276800 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619934082031\n",
      "Epoch: 276900 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277200 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 277700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 277800 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 277900 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278200 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 278600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 278700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 278800 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 278900 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 279000 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 279100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279200 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27619171142578\n",
      "Epoch: 279300 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279400 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279500 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279600 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279700 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279800 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 279900 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280000 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280100 | MSE Train Loss: 110.3987045288086 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280200 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280300 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280400 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280500 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280600 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280700 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280800 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 280900 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281000 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281100 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281200 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281300 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281400 | MSE Train Loss: 110.39869689941406 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 281500 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27618408203125\n",
      "Epoch: 281600 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 281700 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 281800 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 281900 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282000 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282100 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282200 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282300 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282400 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282500 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282600 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282700 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282800 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 282900 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 283000 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 283100 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 283200 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 283300 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616882324219\n",
      "Epoch: 283400 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 283500 | MSE Train Loss: 110.39868927001953 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 283600 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 283700 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 283800 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 283900 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27616119384766\n",
      "Epoch: 284000 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284100 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284200 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284300 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284400 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284500 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284600 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284700 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284800 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 284900 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285000 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285100 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285200 | MSE Train Loss: 110.39867401123047 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285300 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285400 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285500 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 285600 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 285700 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 285800 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 285900 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286000 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286100 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286200 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286300 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286400 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286500 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286600 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286700 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 286800 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.27615356445312\n",
      "Epoch: 286900 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287000 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287100 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287200 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287300 | MSE Train Loss: 110.3986587524414 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287400 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287500 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287600 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287700 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 287800 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 287900 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 288000 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 288100 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288200 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288300 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.2761459350586\n",
      "Epoch: 288400 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288500 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288600 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288700 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288800 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 288900 | MSE Train Loss: 110.39865112304688 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289000 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289100 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289200 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289300 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 289400 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 289500 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 289600 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289700 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289800 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 289900 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613830566406\n",
      "Epoch: 290000 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290100 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290200 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290300 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290400 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290500 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290600 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290700 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290800 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 290900 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291000 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291100 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291200 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291300 | MSE Train Loss: 110.39864349365234 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291400 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291500 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.27613067626953\n",
      "Epoch: 291600 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 291700 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 291800 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 291900 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 292000 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 292100 | MSE Train Loss: 110.39863586425781 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 292200 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 292300 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.276123046875\n",
      "Epoch: 292400 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 292500 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 292600 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 292700 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 292800 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 292900 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293000 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293100 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293200 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293300 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293400 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293500 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293600 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 293700 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 293800 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 293900 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 294000 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 294100 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 294200 | MSE Train Loss: 110.39862823486328 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294300 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294400 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27611541748047\n",
      "Epoch: 294500 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294600 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294700 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294800 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 294900 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295000 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295100 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295200 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295300 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295400 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295500 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295600 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295700 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 295800 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 295900 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 296000 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 296100 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296200 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296300 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296400 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296500 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296600 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296700 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296800 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 296900 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297000 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297100 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297200 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297300 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297400 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297500 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297600 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297700 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297800 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 297900 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 298000 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.2761001586914\n",
      "Epoch: 298100 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 298200 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 298300 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298400 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298500 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298600 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298700 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298800 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 298900 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299000 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299100 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299200 | MSE Train Loss: 110.39862060546875 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299300 | MSE Train Loss: 110.39861297607422 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299400 | MSE Train Loss: 110.39860534667969 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299500 | MSE Train Loss: 110.39860534667969 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299600 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27610778808594\n",
      "Epoch: 299700 | MSE Train Loss: 110.39860534667969 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 299800 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 299900 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300000 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300100 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300200 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300300 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300400 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300500 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300600 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 300700 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 300800 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 300900 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301000 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301100 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301200 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301300 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301400 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301500 | MSE Train Loss: 110.39859008789062 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301600 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301700 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301800 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27608489990234\n",
      "Epoch: 301900 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302000 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 302100 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302200 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 302300 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 302400 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27609252929688\n",
      "Epoch: 302500 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302600 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302700 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302800 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 302900 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303000 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303100 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303200 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303300 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303400 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303500 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303600 | MSE Train Loss: 110.3985824584961 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303700 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303800 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 303900 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304000 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304100 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304200 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 304300 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304400 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304500 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304600 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304700 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304800 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 304900 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305000 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305100 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305200 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305300 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 305400 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305500 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27607727050781\n",
      "Epoch: 305600 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 305700 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 305800 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 305900 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306000 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306100 | MSE Train Loss: 110.39857482910156 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306200 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306300 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306400 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306500 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306600 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306700 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306800 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 306900 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 307000 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 307100 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 307200 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 307300 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606201171875\n",
      "Epoch: 307400 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606201171875\n",
      "Epoch: 307500 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606201171875\n",
      "Epoch: 307600 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 307700 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 307800 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27606964111328\n",
      "Epoch: 307900 | MSE Train Loss: 110.39856719970703 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308000 | MSE Train Loss: 110.3985595703125 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308100 | MSE Train Loss: 110.39855194091797 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308200 | MSE Train Loss: 110.39855194091797 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308300 | MSE Train Loss: 110.39855194091797 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308400 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308500 | MSE Train Loss: 110.3985595703125 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308600 | MSE Train Loss: 110.3985595703125 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308700 | MSE Train Loss: 110.3985595703125 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308800 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 308900 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 309000 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 309100 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27605438232422\n",
      "Epoch: 309200 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309300 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309400 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309500 | MSE Train Loss: 110.39855194091797 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309600 | MSE Train Loss: 110.39855194091797 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309700 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309800 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 309900 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310000 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310100 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310200 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310300 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310400 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310500 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310600 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310700 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310800 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 310900 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27604675292969\n",
      "Epoch: 311000 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311100 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311200 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311300 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311400 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311500 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311600 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311700 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311800 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 311900 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312000 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312100 | MSE Train Loss: 110.39852905273438 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312200 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312300 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312400 | MSE Train Loss: 110.39854431152344 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312500 | MSE Train Loss: 110.39852905273438 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 312600 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 312700 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 312800 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 312900 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313000 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 313100 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 313200 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313300 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313400 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313500 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313600 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603912353516\n",
      "Epoch: 313700 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 313800 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 313900 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 314000 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314100 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314200 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314300 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314400 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314500 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314600 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 314700 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 314800 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 314900 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.27603149414062\n",
      "Epoch: 315000 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315100 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315200 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315300 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315400 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315500 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315600 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315700 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315800 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 315900 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316000 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316100 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316200 | MSE Train Loss: 110.39852142333984 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316300 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316400 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316500 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316600 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316700 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.2760238647461\n",
      "Epoch: 316800 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 316900 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317000 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317100 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317200 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317300 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317400 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317500 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317600 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317700 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317800 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 317900 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 318000 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 318100 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 318200 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27601623535156\n",
      "Epoch: 318300 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318400 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318500 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318600 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318700 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318800 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 318900 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319000 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319100 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319200 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319300 | MSE Train Loss: 110.39851379394531 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319400 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319500 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319600 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319700 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319800 | MSE Train Loss: 110.39849853515625 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 319900 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 320000 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27600860595703\n",
      "Epoch: 320100 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320200 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320300 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320400 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320500 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320600 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320700 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320800 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 320900 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321000 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321100 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321200 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321300 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321400 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 321500 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 321600 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 321700 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 321800 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 321900 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.2760009765625\n",
      "Epoch: 322000 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322100 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322200 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322300 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322400 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322500 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322600 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322700 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322800 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 322900 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323000 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323100 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323200 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323300 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323400 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323500 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323600 | MSE Train Loss: 110.39849090576172 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323700 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323800 | MSE Train Loss: 110.39848327636719 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 323900 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27599334716797\n",
      "Epoch: 324000 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324100 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27598571777344\n",
      "Epoch: 324200 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324300 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324400 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324500 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324600 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324700 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324800 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 324900 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325000 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325100 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325200 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325300 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325400 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325500 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325600 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325700 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325800 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 325900 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326000 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326100 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326200 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326300 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326400 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326500 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326600 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 326700 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 326800 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 326900 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327000 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327100 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327200 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327300 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327400 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327500 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327600 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327700 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327800 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 327900 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328000 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328100 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328200 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 328300 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.2759780883789\n",
      "Epoch: 328400 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328500 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328600 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328700 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328800 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 328900 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329000 | MSE Train Loss: 110.39846801757812 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329100 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329200 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329300 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329400 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329500 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329600 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329700 | MSE Train Loss: 110.3984603881836 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329800 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 329900 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330000 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330100 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330200 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330300 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330400 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330500 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330600 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330700 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330800 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 330900 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 331000 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27597045898438\n",
      "Epoch: 331100 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331200 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331300 | MSE Train Loss: 110.39845275878906 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331400 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331500 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331600 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 331700 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27596282958984\n",
      "Epoch: 331800 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27596282958984\n",
      "Epoch: 331900 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27596282958984\n",
      "Epoch: 332000 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27596282958984\n",
      "Epoch: 332100 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332200 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332300 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332400 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332500 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332600 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332700 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332800 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 332900 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333000 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333100 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333200 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333300 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333400 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333500 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 333600 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333700 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27595520019531\n",
      "Epoch: 333800 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 333900 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334000 | MSE Train Loss: 110.39844512939453 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334100 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334200 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334300 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334400 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334500 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334600 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334700 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334800 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 334900 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27594757080078\n",
      "Epoch: 335000 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335100 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335200 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335300 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335400 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335500 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335600 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335700 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335800 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 335900 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336000 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336100 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336200 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336300 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336400 | MSE Train Loss: 110.3984375 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336500 | MSE Train Loss: 110.39842224121094 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336600 | MSE Train Loss: 110.39842224121094 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336700 | MSE Train Loss: 110.39842224121094 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336800 | MSE Train Loss: 110.39842224121094 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 336900 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337000 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337100 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337200 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337300 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337400 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337500 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337600 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337700 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 337800 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 337900 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 338000 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 338100 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338200 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338300 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338400 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338500 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338600 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593994140625\n",
      "Epoch: 338700 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338800 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 338900 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339000 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339100 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339200 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339300 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339400 | MSE Train Loss: 110.3984146118164 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339500 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27593231201172\n",
      "Epoch: 339600 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27592468261719\n",
      "Epoch: 339700 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27592468261719\n",
      "Epoch: 339800 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 339900 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27592468261719\n",
      "Epoch: 340000 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340100 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340200 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340300 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340400 | MSE Train Loss: 110.39839935302734 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340500 | MSE Train Loss: 110.39840698242188 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340600 | MSE Train Loss: 110.39839935302734 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340700 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340800 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 340900 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341000 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341100 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341200 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341300 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341400 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 341500 | MSE Train Loss: 110.39839935302734 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 341600 | MSE Train Loss: 110.39839935302734 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 341700 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 341800 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 341900 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 342000 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27591705322266\n",
      "Epoch: 342100 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342200 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 342300 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 342400 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342500 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342600 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342700 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342800 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 342900 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.27590942382812\n",
      "Epoch: 343000 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343100 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343200 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343300 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343400 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343500 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343600 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343700 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343800 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 343900 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 344000 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 344100 | MSE Train Loss: 110.39839172363281 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 344200 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344300 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344400 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344500 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344600 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344700 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344800 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 344900 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345000 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 345100 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345200 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345300 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345400 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345500 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345600 | MSE Train Loss: 110.39837646484375 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345700 | MSE Train Loss: 110.39837646484375 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345800 | MSE Train Loss: 110.39837646484375 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 345900 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.2759017944336\n",
      "Epoch: 346000 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 346100 | MSE Train Loss: 110.39838409423828 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 346200 | MSE Train Loss: 110.39837646484375 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 346300 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 346400 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27589416503906\n",
      "Epoch: 346500 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 346600 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 346700 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 346800 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 346900 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347000 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347100 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347200 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347300 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347400 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347500 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347600 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347700 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347800 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 347900 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348000 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348100 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348200 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 348300 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348400 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348500 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 348600 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 348700 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27588653564453\n",
      "Epoch: 348800 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 348900 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349000 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349100 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349200 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 349300 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 349400 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349500 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 349600 | MSE Train Loss: 110.39835357666016 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349700 | MSE Train Loss: 110.39835357666016 | MSE Test Loss: 100.27587127685547\n",
      "Epoch: 349800 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 349900 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350000 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350100 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350200 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350300 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350400 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350500 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350600 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350700 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350800 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 350900 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351000 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351100 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351200 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351300 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351400 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351500 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351600 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351700 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351800 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 351900 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 352000 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 352100 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 352200 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 352300 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 352400 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 352500 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 352600 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 352700 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 352800 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 352900 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 353000 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 353100 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 353200 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 353300 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 353400 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 353500 | MSE Train Loss: 110.39836120605469 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 353600 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 353700 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 353800 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 353900 | MSE Train Loss: 110.39834594726562 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 354000 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.2758560180664\n",
      "Epoch: 354100 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 354200 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 354300 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354400 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354500 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354600 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354700 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354800 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 354900 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355000 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355100 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355200 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355300 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355400 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355500 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355600 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355700 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355800 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 355900 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 356000 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 356100 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356200 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356300 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356400 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 356500 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356600 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356700 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356800 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 356900 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 357000 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 357100 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 357200 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584838867188\n",
      "Epoch: 357300 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 357400 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 357500 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27584075927734\n",
      "Epoch: 357600 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 357700 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 357800 | MSE Train Loss: 110.39833068847656 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 357900 | MSE Train Loss: 110.39832305908203 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358000 | MSE Train Loss: 110.39832305908203 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358100 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358200 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358300 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358400 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358500 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358600 | MSE Train Loss: 110.39832305908203 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358700 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358800 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 358900 | MSE Train Loss: 110.39832305908203 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359000 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359100 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359200 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 359300 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359400 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359500 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 359600 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 359700 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 359800 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 359900 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 360000 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360100 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360200 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360300 | MSE Train Loss: 110.3983154296875 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360400 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360500 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 360600 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 360700 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 360800 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 360900 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 361000 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 361100 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 361200 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 361300 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581787109375\n",
      "Epoch: 361400 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27582550048828\n",
      "Epoch: 361500 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 361600 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 361700 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 361800 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 361900 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362000 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362100 | MSE Train Loss: 110.39830780029297 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362200 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362300 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362400 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 362500 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27581024169922\n",
      "Epoch: 362600 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 362700 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 362800 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 362900 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 363000 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363100 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27580261230469\n",
      "Epoch: 363200 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363300 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363400 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363500 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363600 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363700 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363800 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 363900 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364000 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364100 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364200 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364300 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364400 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364500 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364600 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364700 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364800 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 364900 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365000 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365100 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365200 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365300 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365400 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365500 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365600 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 365700 | MSE Train Loss: 110.3982925415039 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365800 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 365900 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366000 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366100 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 366200 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366300 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366400 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366500 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366600 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366700 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366800 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 366900 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 367000 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27579498291016\n",
      "Epoch: 367100 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367200 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367300 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367400 | MSE Train Loss: 110.39828491210938 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367500 | MSE Train Loss: 110.39826965332031 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367600 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 367700 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367800 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27578735351562\n",
      "Epoch: 367900 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368000 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368100 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368200 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368300 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368400 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368500 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 368600 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 368700 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 368800 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 368900 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369000 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369100 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369200 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369300 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369400 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369500 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369600 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369700 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369800 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 369900 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.2757797241211\n",
      "Epoch: 370000 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370100 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370200 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370300 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370400 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370500 | MSE Train Loss: 110.39825439453125 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370600 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 370700 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 370800 | MSE Train Loss: 110.39826202392578 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 370900 | MSE Train Loss: 110.39825439453125 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371000 | MSE Train Loss: 110.39825439453125 | MSE Test Loss: 100.27577209472656\n",
      "Epoch: 371100 | MSE Train Loss: 110.39825439453125 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371200 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371300 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371400 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371500 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371600 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371700 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371800 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 371900 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372000 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372200 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372400 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372600 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372700 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372800 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 372900 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373000 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373200 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373300 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373400 | MSE Train Loss: 110.39824676513672 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373600 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373700 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373800 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 373900 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 374000 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 374100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 374200 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 374300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.2757568359375\n",
      "Epoch: 374400 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 374500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 374600 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 374700 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 374800 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 374900 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 375000 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.2757568359375\n",
      "Epoch: 375100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.2757568359375\n",
      "Epoch: 375200 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.2757568359375\n",
      "Epoch: 375300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 375400 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.2757568359375\n",
      "Epoch: 375500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 375600 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 375700 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 375800 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 375900 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376000 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376200 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376400 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376600 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376700 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376800 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 376900 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377000 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377100 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377200 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 377400 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27576446533203\n",
      "Epoch: 377500 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377600 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377700 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377800 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 377900 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378000 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378100 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378200 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378300 | MSE Train Loss: 110.39823150634766 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378400 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 378500 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 378600 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 378700 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 378800 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 378900 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 379000 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379100 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379200 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379300 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379400 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379500 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379600 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379700 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379800 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574920654297\n",
      "Epoch: 379900 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 380000 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 380100 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 380200 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 380300 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 380400 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.27574157714844\n",
      "Epoch: 380500 | MSE Train Loss: 110.3982162475586 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 380600 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 380700 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 380800 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 380900 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381000 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381100 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381200 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381300 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381400 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381500 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381600 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381700 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381800 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 381900 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 382000 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 382100 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382200 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382300 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382400 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382500 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382600 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382700 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382800 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 382900 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383000 | MSE Train Loss: 110.39820861816406 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383100 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 383200 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383300 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.2757339477539\n",
      "Epoch: 383400 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383500 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383600 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383700 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383800 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 383900 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384000 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384100 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384200 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384300 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384400 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384500 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384600 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384700 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384800 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 384900 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 385000 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 385100 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571105957031\n",
      "Epoch: 385200 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 385300 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571868896484\n",
      "Epoch: 385400 | MSE Train Loss: 110.39820098876953 | MSE Test Loss: 100.27571105957031\n",
      "Epoch: 385500 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571105957031\n",
      "Epoch: 385600 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571105957031\n",
      "Epoch: 385700 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27571105957031\n",
      "Epoch: 385800 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 385900 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386000 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386100 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386200 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386300 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386400 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386500 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386600 | MSE Train Loss: 110.398193359375 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386700 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386800 | MSE Train Loss: 110.39818572998047 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 386900 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387000 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387100 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387200 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387300 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387400 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387500 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387600 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387700 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387800 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 387900 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388000 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388100 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388200 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388300 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388400 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388500 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388600 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388700 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388800 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 388900 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 389000 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 389100 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 389200 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 389300 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 389400 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 389500 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27570343017578\n",
      "Epoch: 389600 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 389700 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 389800 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 389900 | MSE Train Loss: 110.39817810058594 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 390000 | MSE Train Loss: 110.3981704711914 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 390100 | MSE Train Loss: 110.3981704711914 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 390200 | MSE Train Loss: 110.3981704711914 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 390300 | MSE Train Loss: 110.3981704711914 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 390400 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 390500 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 390600 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 390700 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 390800 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27569580078125\n",
      "Epoch: 390900 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391000 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391100 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391200 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391300 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391400 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568817138672\n",
      "Epoch: 391500 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 391600 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 391700 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 391800 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 391900 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392000 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392100 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392200 | MSE Train Loss: 110.39815521240234 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392300 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392400 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392500 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392600 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392700 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392800 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 392900 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393000 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393100 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393200 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393300 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393400 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 393500 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393600 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 393700 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 393800 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27568054199219\n",
      "Epoch: 393900 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 394000 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394100 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394200 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394300 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394400 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394500 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394600 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 394700 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 394800 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 394900 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395000 | MSE Train Loss: 110.39814758300781 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395100 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395200 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395300 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395400 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27567291259766\n",
      "Epoch: 395500 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 395600 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 395700 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 395800 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 395900 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396000 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396100 | MSE Train Loss: 110.39813995361328 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396200 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396300 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396400 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.27566528320312\n",
      "Epoch: 396500 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 396600 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 396700 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 396800 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 396900 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397000 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397100 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397200 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397300 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397400 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397500 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397600 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397700 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397800 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 397900 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398000 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398100 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398200 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398300 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398400 | MSE Train Loss: 110.39813232421875 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398500 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398600 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398700 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.2756576538086\n",
      "Epoch: 398800 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 398900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399000 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399100 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399200 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399300 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399400 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399500 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399600 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399700 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 399800 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 399900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 400000 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 400100 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 400200 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27565002441406\n",
      "Epoch: 400300 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400400 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400500 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400600 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400700 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400800 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 400900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401000 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401100 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401200 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 401300 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401400 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401500 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401600 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401700 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401800 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 401900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 402000 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402100 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402200 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402300 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402400 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402500 | MSE Train Loss: 110.39810180664062 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 402600 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 402700 | MSE Train Loss: 110.39812469482422 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 402800 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 402900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 403000 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 403100 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 403200 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 403300 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403400 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403500 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403600 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403700 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403800 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 403900 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.27564239501953\n",
      "Epoch: 404000 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404100 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404200 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404300 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404400 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404500 | MSE Train Loss: 110.39810943603516 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404600 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404700 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404800 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 404900 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.275634765625\n",
      "Epoch: 405000 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405100 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405200 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405300 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405400 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405500 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405600 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405700 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405800 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 405900 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406000 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406100 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406200 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406300 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406400 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406500 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406600 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406700 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406800 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 406900 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 407000 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 407100 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 407200 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27562713623047\n",
      "Epoch: 407300 | MSE Train Loss: 110.3980941772461 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407400 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407500 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407600 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407700 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407800 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 407900 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.27561950683594\n",
      "Epoch: 408000 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408100 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408200 | MSE Train Loss: 110.39808654785156 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408300 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408400 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408500 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408600 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408700 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408800 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 408900 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409000 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409100 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409200 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409300 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409400 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409500 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409600 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409700 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409800 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.2756118774414\n",
      "Epoch: 409900 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410000 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410100 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410200 | MSE Train Loss: 110.39807891845703 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410300 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410400 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410500 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410600 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410700 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410800 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 410900 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411000 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411100 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411200 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411300 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411400 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411500 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411600 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411700 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411800 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 411900 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 412000 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 412100 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412200 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412300 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412400 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412500 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27560424804688\n",
      "Epoch: 412600 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412700 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412800 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 412900 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413000 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413100 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413200 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413300 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413400 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413500 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413600 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413700 | MSE Train Loss: 110.3980712890625 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413800 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 413900 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414000 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414100 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414200 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414300 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414400 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414500 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414600 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414700 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414800 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 414900 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 415000 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 415100 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27559661865234\n",
      "Epoch: 415200 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415300 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415400 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415500 | MSE Train Loss: 110.39805603027344 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415600 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415700 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415800 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 415900 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416000 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416100 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416200 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416300 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416400 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416500 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416600 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416700 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416800 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 416900 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417000 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417100 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417200 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417300 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417400 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417500 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27558135986328\n",
      "Epoch: 417600 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 417700 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 417800 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 417900 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418000 | MSE Train Loss: 110.39804077148438 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418100 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418200 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418300 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418400 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418500 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418600 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418700 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418800 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 418900 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419000 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419100 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419200 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419300 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419400 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419500 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419600 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419700 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419800 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 419900 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420000 | MSE Train Loss: 110.39803314208984 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420100 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420200 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420300 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420400 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420500 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420600 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420700 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420800 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 420900 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 421000 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 421100 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 421200 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27555847167969\n",
      "Epoch: 421300 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27556610107422\n",
      "Epoch: 421400 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27555847167969\n",
      "Epoch: 421500 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 421600 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 421700 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 421800 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 421900 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422000 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422100 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422200 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422300 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422400 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422500 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422600 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422700 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422800 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 422900 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423000 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423100 | MSE Train Loss: 110.39801788330078 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423200 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423300 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423400 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27554321289062\n",
      "Epoch: 423500 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 423600 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 423700 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 423800 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 423900 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 424000 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 424100 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424200 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424300 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.2755355834961\n",
      "Epoch: 424400 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424500 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424600 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424700 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424800 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 424900 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425000 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425100 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425200 | MSE Train Loss: 110.39801025390625 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425300 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425400 | MSE Train Loss: 110.39800262451172 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425500 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425600 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425700 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425800 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 425900 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426000 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426100 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426200 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426300 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426400 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426500 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426600 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426700 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426800 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 426900 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552795410156\n",
      "Epoch: 427000 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 427100 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427200 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427300 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427400 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427500 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427600 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427700 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427800 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 427900 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 428000 | MSE Train Loss: 110.39799499511719 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 428100 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428200 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428300 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428400 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27552032470703\n",
      "Epoch: 428500 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428600 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428700 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428800 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 428900 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429000 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429100 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429200 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429300 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429400 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429500 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429600 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429700 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429800 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 429900 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430000 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430100 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430200 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430300 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430400 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430500 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430600 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430700 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430800 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 430900 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431000 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431100 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431200 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431300 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431400 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431500 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431600 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431700 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431800 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 431900 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432000 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 432100 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432200 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432300 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432400 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432500 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432600 | MSE Train Loss: 110.39797973632812 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432700 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432800 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 432900 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 433000 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 433100 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433200 | MSE Train Loss: 110.39798736572266 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433300 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433400 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433500 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433600 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 433700 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 433800 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 433900 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 434000 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 434100 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27550506591797\n",
      "Epoch: 434200 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.2755126953125\n",
      "Epoch: 434300 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434400 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434500 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434600 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434700 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434800 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 434900 | MSE Train Loss: 110.3979721069336 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435000 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435100 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435200 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435300 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435400 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435500 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435600 | MSE Train Loss: 110.39796447753906 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435700 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435800 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 435900 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 436000 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 436100 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27549743652344\n",
      "Epoch: 436200 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436300 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436400 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436500 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436600 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436700 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436800 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 436900 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.2754898071289\n",
      "Epoch: 437000 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.2754898071289\n",
      "Epoch: 437100 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.2754898071289\n",
      "Epoch: 437200 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437300 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437400 | MSE Train Loss: 110.39795684814453 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437500 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437600 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437700 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437800 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 437900 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 438000 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 438100 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 438200 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27548217773438\n",
      "Epoch: 438300 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 438400 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 438500 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27546691894531\n",
      "Epoch: 438600 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 438700 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 438800 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 438900 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 439000 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 439100 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 439200 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27546691894531\n",
      "Epoch: 439300 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 439400 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 439500 | MSE Train Loss: 110.39794158935547 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 439600 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27546691894531\n",
      "Epoch: 439700 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27546691894531\n",
      "Epoch: 439800 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 439900 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27547454833984\n",
      "Epoch: 440000 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440100 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440200 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440300 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440400 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440500 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440600 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440700 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440800 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 440900 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27546691894531\n",
      "Epoch: 441000 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441100 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441200 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441300 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441400 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441500 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441600 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441700 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 441800 | MSE Train Loss: 110.3979263305664 | MSE Test Loss: 100.27545928955078\n",
      "Epoch: 441900 | MSE Train Loss: 110.39791107177734 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442000 | MSE Train Loss: 110.39791107177734 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442100 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442200 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442300 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442400 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442500 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442600 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442700 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442800 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 442900 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443000 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443100 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443200 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443300 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443400 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443500 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443600 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443700 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 443800 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 443900 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444000 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 444100 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444200 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444300 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444400 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27545166015625\n",
      "Epoch: 444500 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444600 | MSE Train Loss: 110.39790344238281 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444700 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444800 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 444900 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445000 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445100 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 445200 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445300 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445400 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445500 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 445600 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27544403076172\n",
      "Epoch: 445700 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 445800 | MSE Train Loss: 110.39789581298828 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 445900 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446000 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446100 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446200 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446300 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446400 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446500 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446600 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446700 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446800 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 446900 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447000 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447100 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447200 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447300 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447400 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27543640136719\n",
      "Epoch: 447500 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 447600 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 447700 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 447800 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 447900 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448000 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448100 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448200 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448300 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448400 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 448500 | MSE Train Loss: 110.39788818359375 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 448600 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 448700 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 448800 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 448900 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449000 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449100 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449200 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449300 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 449400 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 449500 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 449600 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449700 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449800 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 449900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450000 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450100 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450200 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450300 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450400 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450500 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450600 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450700 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450800 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 450900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451000 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451100 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451200 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451300 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451400 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451500 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451600 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451700 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451800 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 451900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 452000 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452100 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452200 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452300 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 452400 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 452500 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 452600 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452700 | MSE Train Loss: 110.39788055419922 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452800 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 452900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453000 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453100 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453200 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453300 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 453400 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 453500 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453600 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542877197266\n",
      "Epoch: 453700 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453800 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 453900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 454000 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454100 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27542114257812\n",
      "Epoch: 454200 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454300 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454400 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454500 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454600 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454700 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454800 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 454900 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455000 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455100 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455200 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 455300 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455400 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 455500 | MSE Train Loss: 110.39785766601562 | MSE Test Loss: 100.2754135131836\n",
      "Epoch: 455600 | MSE Train Loss: 110.39787292480469 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455700 | MSE Train Loss: 110.39785766601562 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455800 | MSE Train Loss: 110.39785766601562 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 455900 | MSE Train Loss: 110.39785766601562 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456000 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456100 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456200 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456300 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27539825439453\n",
      "Epoch: 456400 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27539825439453\n",
      "Epoch: 456500 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27539825439453\n",
      "Epoch: 456600 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456700 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456800 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 456900 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 457000 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 457100 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 457200 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 457300 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.27540588378906\n",
      "Epoch: 457400 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 457500 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 457600 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 457700 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 457800 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 457900 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458000 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458100 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458200 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458300 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458400 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458500 | MSE Train Loss: 110.3978500366211 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458600 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458700 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458800 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 458900 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459000 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459100 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459200 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459300 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459400 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459500 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459600 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459700 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459800 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 459900 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460000 | MSE Train Loss: 110.39784240722656 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460100 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460200 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460300 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 460400 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 460500 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460600 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.275390625\n",
      "Epoch: 460700 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 460800 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 460900 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461000 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461100 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461200 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461300 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461400 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461500 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461600 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461700 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461800 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 461900 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462000 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462100 | MSE Train Loss: 110.39783477783203 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462200 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462300 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462400 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462500 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462600 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462700 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 462800 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 462900 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463000 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463100 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463200 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463300 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463400 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463500 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463600 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463700 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463800 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 463900 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464000 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464100 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464200 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464300 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 464400 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 464500 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464600 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.27537536621094\n",
      "Epoch: 464700 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 464800 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 464900 | MSE Train Loss: 110.39781951904297 | MSE Test Loss: 100.2753677368164\n",
      "Epoch: 465000 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465100 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465200 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465300 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465400 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465500 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465600 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465700 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465800 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 465900 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466000 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466100 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466200 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466300 | MSE Train Loss: 110.39781188964844 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466400 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466500 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466600 | MSE Train Loss: 110.3978042602539 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466700 | MSE Train Loss: 110.3978042602539 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466800 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 466900 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 467000 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27535247802734\n",
      "Epoch: 467100 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467200 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467300 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467400 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467600 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467700 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467800 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 467900 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 468000 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 468100 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 468200 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 468300 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 468400 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 468500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 468600 | MSE Train Loss: 110.39779663085938 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 468700 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 468800 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 468900 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469000 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469100 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469200 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 469300 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27534484863281\n",
      "Epoch: 469400 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469600 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469700 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469800 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 469900 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470000 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470100 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470200 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470300 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470400 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470600 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470700 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470800 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 470900 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471000 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471100 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471200 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471300 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471400 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471600 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471700 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471800 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 471900 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 472000 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 472100 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27533721923828\n",
      "Epoch: 472200 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472300 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472400 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472500 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472600 | MSE Train Loss: 110.39778137207031 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472700 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472800 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 472900 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473000 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473100 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473200 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473300 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473400 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473500 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473600 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473700 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473800 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 473900 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 474000 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 474100 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474200 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27532196044922\n",
      "Epoch: 474300 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474400 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474500 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474600 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474700 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474800 | MSE Train Loss: 110.39777374267578 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 474900 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475000 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475100 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475200 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475300 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475400 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475500 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475600 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475700 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475800 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 475900 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476000 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476100 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476200 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476300 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476400 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476500 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476600 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476700 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476800 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 476900 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477000 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477100 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477200 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477300 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477400 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27531433105469\n",
      "Epoch: 477500 | MSE Train Loss: 110.39775848388672 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 477600 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 477700 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 477800 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 477900 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 478000 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478100 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478200 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 478300 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478400 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27530670166016\n",
      "Epoch: 478500 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478600 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478700 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478800 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 478900 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479000 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479100 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 479200 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479300 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479400 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 479500 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479600 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479700 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 479800 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 479900 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 480000 | MSE Train Loss: 110.39773559570312 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 480100 | MSE Train Loss: 110.39773559570312 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 480200 | MSE Train Loss: 110.39773559570312 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480300 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480400 | MSE Train Loss: 110.39773559570312 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480500 | MSE Train Loss: 110.39775085449219 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480600 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480700 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 480900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 481000 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481100 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481200 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481300 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481400 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481500 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481600 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481700 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 481900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482000 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482100 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482200 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482300 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.27529907226562\n",
      "Epoch: 482400 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482500 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482600 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482700 | MSE Train Loss: 110.39774322509766 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 482900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483000 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483100 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483200 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483300 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483400 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483500 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483600 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483700 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 483900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 484000 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 484100 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 484200 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.2752914428711\n",
      "Epoch: 484300 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27528381347656\n",
      "Epoch: 484400 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27528381347656\n",
      "Epoch: 484500 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27528381347656\n",
      "Epoch: 484600 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 484700 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 484800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27528381347656\n",
      "Epoch: 484900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485000 | MSE Train Loss: 110.39772033691406 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485100 | MSE Train Loss: 110.39772033691406 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485200 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485300 | MSE Train Loss: 110.39772033691406 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485400 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485500 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485600 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485700 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485800 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 485900 | MSE Train Loss: 110.3977279663086 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486000 | MSE Train Loss: 110.39772033691406 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486100 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486200 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486300 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486400 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486500 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486600 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486700 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486800 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 486900 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 487000 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 487100 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 487200 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487300 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487400 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 487500 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487600 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487700 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487800 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 487900 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 488000 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 488100 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 488200 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 488300 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27527618408203\n",
      "Epoch: 488400 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 488500 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 488600 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 488700 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.2752685546875\n",
      "Epoch: 488800 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 488900 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489000 | MSE Train Loss: 110.39771270751953 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489100 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489200 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489300 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489400 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489500 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489600 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489700 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489800 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 489900 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490000 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490100 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490200 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490300 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490400 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490500 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.27526092529297\n",
      "Epoch: 490600 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 490700 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 490800 | MSE Train Loss: 110.397705078125 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 490900 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491000 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491100 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491200 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491300 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491400 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491500 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491600 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491700 | MSE Train Loss: 110.39769744873047 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491800 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 491900 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492000 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492100 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492200 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492300 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492400 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492500 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492600 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492700 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492800 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 492900 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493000 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493100 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493200 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493300 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493400 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493500 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493600 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493700 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493800 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.2752456665039\n",
      "Epoch: 493900 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494000 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494100 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494200 | MSE Train Loss: 110.39768981933594 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494300 | MSE Train Loss: 110.3976821899414 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494400 | MSE Train Loss: 110.3976821899414 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494500 | MSE Train Loss: 110.3976821899414 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494600 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494700 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494800 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 494900 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495000 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495100 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495200 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495300 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495400 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495500 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495600 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495700 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 495800 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 495900 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27521514892578\n",
      "Epoch: 496000 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 496100 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27521514892578\n",
      "Epoch: 496200 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27523803710938\n",
      "Epoch: 496300 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496400 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496500 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496600 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496700 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496800 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 496900 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497000 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 497100 | MSE Train Loss: 110.39766693115234 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 497200 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497300 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497400 | MSE Train Loss: 110.39767456054688 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497500 | MSE Train Loss: 110.39766693115234 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497600 | MSE Train Loss: 110.39766693115234 | MSE Test Loss: 100.27522277832031\n",
      "Epoch: 497700 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 497800 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 497900 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498000 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498100 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498200 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498300 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498400 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498500 | MSE Train Loss: 110.39765930175781 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498600 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498700 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498800 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 498900 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499000 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499100 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499200 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499300 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499400 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499500 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499600 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499700 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499800 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n",
      "Epoch: 499900 | MSE Train Loss: 110.39765167236328 | MSE Test Loss: 100.27520751953125\n"
     ]
    }
   ],
   "source": [
    "#Set number of epochs\n",
    "epochs = 500000\n",
    "\n",
    "#Create lists to track loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ###Training\n",
    "    model_0.train()\n",
    "    y_pred = model_0(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ###Testing\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_0(x_test)\n",
    "        test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MSE Train Loss: {loss} | MSE Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f916fcdfb20>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and Test Loss Curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[  2.1829,   1.1574, 100.5793, 401.0872,  60.2897]])), ('linear.bias', tensor([2.8002]))])\n"
     ]
    }
   ],
   "source": [
    "# Find the model's learned parameters\n",
    "print(model_0.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "#Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_0(x_test)\n",
    "\n",
    "plt.hist(y_preds[:,0].numpy(),100,histtype='step')\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Predicted Energy Distribution')\n",
    "plt.savefig(\"linregdist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "#Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  y_preds_200GeV = model_0(features(data_tensor_200GeV))\n",
    "  y_preds_100GeV = model_0(features(data_tensor_100GeV))\n",
    "  y_preds_50GeV = model_0(features(data_tensor_50GeV))\n",
    "  y_preds_10GeV = model_0(features(data_tensor_10GeV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.523889, 51.044907, 100.06893, 197.53485)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_preds = norm.fit(y_preds_10GeV)[0], norm.fit(y_preds_50GeV)[0], norm.fit(y_preds_100GeV)[0], norm.fit(y_preds_200GeV)[0]\n",
    "true_peaks = [10,50,100,200]\n",
    "peak_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABq/ElEQVR4nO3deXhM1/8H8Pdkm+yJyB4RiaVKIhJL7bsQtdUu1VJLt9ipol9bW2uLKqW+LVG1q70U0dpSFFmQhAiCkEQI2feZ8/vDz3w7EmRiklnyfj3PPI+5c+bO+7qJ+Tjn3nMkQggBIiIiIj1loOkARERERBWJxQ4RERHpNRY7REREpNdY7BAREZFeY7FDREREeo3FDhEREek1FjtERESk11jsEBERkV5jsUNERER6jcUOEVWIDRs2QCKR4OLFi6W+fvv2bUgkEmzYsKFyg5XD3LlzIZFIlLatXr1aJ7ITEWCk6QBEVDW5uLjg7NmzqF27tqajvNLo0aPRvXt3pW2rV6+Gvb09RowYoZlQRFRmLHaISCOkUilatGih6RgvlZubC3Nzc9SoUQM1atTQdBwiKicOYxGRRpQ2jPVsuCgmJgZDhw6FjY0NnJycMHLkSGRkZCi9XwiB1atXo3HjxjAzM0O1atUwYMAA3Lp1S6ldaGgo+vTpgxo1asDU1BR16tTBRx99hEePHim1e/bZERERGDBgAKpVq6bodXp+GKtWrVqIiYnByZMnIZFIIJFIUKtWLWRnZ8PW1hYfffRRqcdraGiIb7755nX/6ohIRSx2iEjr9O/fH/Xq1cOuXbswffp0bNmyBZMmTVJq89FHH2HixIno0qUL9u7di9WrVyMmJgatWrXCgwcPFO1u3ryJli1bYs2aNTh69Chmz56Nf/75B23atEFRUVGJz+7Xrx/q1KmDnTt34scffyw13549e+Dl5QU/Pz+cPXsWZ8+exZ49e2BpaYmRI0di8+bNJYqz1atXw8TEBCNHjlTD3xARqUQQEVWAkJAQAUBcuHCh1NcTEhIEABESEqLYNmfOHAFALFmyRKntp59+KkxNTYVcLhdCCHH27FkBQCxdulSpXWJiojAzMxPTpk0r9TPlcrkoKioSd+7cEQDEvn37Snz27NmzS7zv2Wv/1rBhQ9G+ffsSbW/evCkMDAzE8uXLFdvy8vJE9erVxQcffFBqLiKqWOzZISKt07t3b6XnjRo1Qn5+PlJTUwEAv//+OyQSCYYNG4bi4mLFw9nZGb6+vjhx4oTivampqfj444/h7u4OIyMjGBsbw8PDAwBw9erVEp/dv3//18ru5eWFnj17YvXq1RBCAAC2bNmCtLQ0jB079rX2TUTlwwuUiUjrVK9eXem5VCoFAOTl5QEAHjx4ACEEnJycSn2/l5cXAEAulyMgIABJSUmYNWsWfHx8YGFhAblcjhYtWij2928uLi6vnX/ChAno3LkzQkNDERAQgB9++AEtW7aEv7//a++biFTHYoeIdI69vT0kEglOnz6tKIT+7dm26OhoXLp0CRs2bMDw4cMVr9+4ceOF+35+Pp3y6NSpE7y9vbFq1SpYWloiIiICmzZteu39ElH5sNghIp3Ts2dPLFq0CPfv38egQYNe2O5Z4fJ8QbR27drXziCVSkvtGXpm/Pjx+Pjjj5GRkQEnJycMHDjwtT+TiMqHxQ4RVai//voLt2/fLrG9QYMG5d5n69at8eGHH+KDDz7AxYsX0a5dO1hYWCA5ORlhYWHw8fHBJ598gvr166N27dqYPn06hBCws7PDgQMHEBoa+hpH9JSPjw+2bduG7du3w8vLC6ampvDx8VG8PmzYMMyYMQOnTp3Cf/7zH5iYmLz2ZxJR+bDYIaIK9fnnn5e6PSEh4bX2u3btWrRo0QJr167F6tWrIZfL4erqitatW6N58+YAAGNjYxw4cAATJkzARx99BCMjI3Tp0gXHjh1DzZo1X+vz582bh+TkZIwZMwZZWVnw8PBQKurMzMzQq1cvbNq0CR9//PFrfRYRvR6JeHa7ABERqU1hYSFq1aqFNm3aYMeOHZqOQ1SlsWeHiEiNHj58iLi4OISEhODBgweYPn26piMRVXksdoiI1OjgwYP44IMP4OLigtWrV/N2cyItwGEsIiIi0mucQZmIiIj0GosdIiIi0mssdoiIiEiv8QJlPF0/JykpCVZWVmqZKp6IiIgqnhACWVlZcHV1hYHBi/tvWOwASEpKgru7u6ZjEBERUTkkJiaiRo0aL3ydxQ4AKysrAE//sqytrTWchoiIiMoiMzMT7u7uiu/xF2Gxg/8tFmhtbc1ih4iISMe86hIUXqBMREREeo3FDhEREek1FjtERESk11jsEBERkV5jsUNERER6jcUOERER6TUWO0RERKTXWOwQERGRXmOxQ0RERHqNxQ4RERHpNY0WOwsXLkSzZs1gZWUFR0dH9O3bF3FxcUpthBCYO3cuXF1dYWZmhg4dOiAmJkapTUFBAcaNGwd7e3tYWFigd+/euHfvXmUeChEREWkpjRY7J0+eRHBwMM6dO4fQ0FAUFxcjICAAOTk5ijZLlizBsmXLsGrVKly4cAHOzs7o2rUrsrKyFG0mTpyIPXv2YNu2bQgLC0N2djZ69uwJmUymicMiIiIiLSIRQghNh3jm4cOHcHR0xMmTJ9GuXTsIIeDq6oqJEyfi888/B/C0F8fJyQmLFy/GRx99hIyMDDg4OODXX3/F4MGDAQBJSUlwd3fHoUOH0K1bt1d+bmZmJmxsbJCRkcGFQImIiNRIJhc4eT0Vneo7qX3fZf3+1qprdjIyMgAAdnZ2AICEhASkpKQgICBA0UYqlaJ9+/Y4c+YMACA8PBxFRUVKbVxdXeHt7a1o87yCggJkZmYqPYiIiEi9UjLy0WdVGEZuuIilR+Mgk2umf0Vrih0hBCZPnow2bdrA29sbAJCSkgIAcHJSrgadnJwUr6WkpMDExATVqlV7YZvnLVy4EDY2NoqHu7u7ug+HiIioSvsuNA6tFv2J6KSnHQor/7qBNov/wuHo5ErPojXFztixY3H58mVs3bq1xGsSiUTpuRCixLbnvazNjBkzkJGRoXgkJiaWPzgREREpyOQCwZvD8d2fN/B8R05KRj4+2RRR6QWPVhQ748aNw/79+3H8+HHUqFFDsd3Z2RkASvTQpKamKnp7nJ2dUVhYiCdPnrywzfOkUimsra2VHkRERPR6HmTmI+inczh4pfSRlWe1z7wDsZU6pKXRYkcIgbFjx2L37t3466+/4OnpqfS6p6cnnJ2dERoaqthWWFiIkydPolWrVgCAJk2awNjYWKlNcnIyoqOjFW2IiIioYp28/hCBK07jn4THL20nACRn5OP8K9qpk1GlfVIpgoODsWXLFuzbtw9WVlaKHhwbGxuYmZlBIpFg4sSJWLBgAerWrYu6detiwYIFMDc3R1BQkKLtqFGjMGXKFFSvXh12dnaYOnUqfHx80KVLF00eHhERkd4rlsmxNPQ61py4CQBwszXF/fT8V74vNevVbdRFo8XOmjVrAAAdOnRQ2h4SEoIRI0YAAKZNm4a8vDx8+umnePLkCd566y0cPXoUVlZWivbLly+HkZERBg0ahLy8PHTu3BkbNmyAoaFhZR0KERFRlZOUnofxWyNx8c7TS0mGtaiJrm86Y3jI+Ve+19HKtKLjKWjVPDuawnl2iIiIVPPXtQeYvOMS0nOLYCk1wqL+PujZyBUyuUCbxX8hJSMfpRUYEgDONqYI+7wTDA1efrPRq+jkPDtERESk3Ypkcsw/GIuRGy4iPbcIPm42ODi+DXo2cgUAGBpIMKdXAwBPC5t/e/Z8Tq8Gr13oqILFDhEREZVJ4uNcDPzxLH46nQAAGNGqFn77pCU8qlsotevu7YI1w/zhbKM8VOVsY4o1w/zR3dul0jIDGr5mh4iIiHTDkZgUfLbzEjLzi2FtaoQlA3zR3dv5he27e7ugawNnnE94jNSsfDhamaK5p12l9ug8w2KHiIiIXqigWIZFf1xDyN+3AQC+7rZYNdQP7nbmr3yvoYEELWtXr+CEr8Zih4iIiEp1Ny0XwVsicOX+07Urx7T1xGfd6sPESLeugmGxQ0RERCUcupKMz3+7jKyCYtiaG+PbAb7o0kD9K5dXBhY7REREpJBfJMPXB2Ox6dxdAEATj2pYOdQPrrZmGk5Wfix2iIiICACQ8CgHwZsjEJv8dKXyTzrUxuSu9WBsqFvDVs9jsUNERETYF3UfM3dfQU6hDHYWJlg2yBcd3nDUdCy1YLFDRERUheUXyTB3fwy2XUgEADT3tMP3Q/xKzJGjy1jsEBERVVE3UrMQvDkScQ+yIJEA4zrWwfjOdWGk48NWz2OxQ0REVAXtCr+H/+yNRl6RDPaWUnw3uDHa1LXXdKwKwWKHiIioCsktLMbsfTH4LfweAKBV7er4bkjjSl2FvLKx2CEiIqoi4lKyELwlAjdSs2EgASZ2qYfgjnU0soRDZWKxQ0REpOeEENhxMRFz9scgv0gORyspVgzx04qlHCoDix0iIiI9ll1QjP/suYK9UUkAgLZ17bF8cGPYW0o1nKzysNghIiLSU7FJmRi7JQK3HuXA0ECCKQH18HG72jDQ82Gr57HYISIi0jNCCGz+5y6+/D0WhcVyuNiY4vuhfmhWy07T0TSCxQ4REZEeycwvwozdV3DwcjIAoFN9R3w70Bd2FiYaTqY5LHaIiIj0xJV7GRi7NQJ30nJhZCDBtO5vYHQbryo3bPU8FjtEREQ6TgiBX87cxoJD11Aok8PN1gwrg/zgX7OapqNpBRY7REREOiwjtwjTdl3CkZgHAICABk74ZoAvbMyNNZxMe7DYISIi0lFRiekYuyUC957kwdhQgpk93sSIVrUgkVTtYavnsdghIiLSMUIIrAtLwKI/rqFYLlDTzhyrgvzQqIatpqNpJRY7REREOiQ9txBTd17CsaupAIAePs5Y1L8RrE05bPUiLHaIiIh0RPidxxi3JRJJGfkwMTLArJ4NMOytmhy2egUWO0RERFpOLhdYe+oWvj0aB5lcwNPeAquC/NDQ1UbT0XQCix0iIiItlpZdgMk7LuHk9YcAgN6+rljQzweWUn6FlxX/poiIiLTUP7fSMH5bJB5kFkBqZIC5vRtiSDN3DlupiMUOERGRlpHJBVYfv4Hlx65DLoDaDhb44V1/1He21nQ0ncRih4iISIs8zCrApO1RCLvxCADQz98NX/XxhgWHrcrNQJMffurUKfTq1Quurq6QSCTYu3ev0usSiaTUxzfffKNo06FDhxKvDxkypJKPhIiI6PWdufEIgStOI+zGI5gZG+KbAY2wbFBjFjqvSaN/ezk5OfD19cUHH3yA/v37l3g9OTlZ6fkff/yBUaNGlWg7ZswYfPnll4rnZmZmFROYiIioAsjkAiv+jMfKv+IhBFDPyRI/BPmjrpOVpqPpBY0WO4GBgQgMDHzh687OzkrP9+3bh44dO8LLy0tpu7m5eYm2REREuuBBZj4mbIvEuVuPAQCDm7pjbu+GMDMx1HAy/aHRYSxVPHjwAAcPHsSoUaNKvLZ582bY29ujYcOGmDp1KrKysl66r4KCAmRmZio9iIiIKtup6w/RY8VpnLv1GOYmhvhucGMsHtCIhY6a6cwg4C+//AIrKyv069dPafu7774LT09PODs7Izo6GjNmzMClS5cQGhr6wn0tXLgQ8+bNq+jIREREpSqWybEs9DpWn7gJAHjTxRo/BPnBy8FSw8n0k0QIITQdAnh6MfKePXvQt2/fUl+vX78+unbtipUrV750P+Hh4WjatCnCw8Ph7+9fapuCggIUFBQonmdmZsLd3R0ZGRmwtuZtfUREVHGSM/IwfmskLtx+AgB4962amNWzAUyN2ZujqszMTNjY2Lzy+1snenZOnz6NuLg4bN++/ZVt/f39YWxsjPj4+BcWO1KpFFKpVN0xiYiIXuqvaw8wZcclPMktgqXUCIv6+6BnI1dNx9J7OlHsrFu3Dk2aNIGvr+8r28bExKCoqAguLi6VkIyIiOjVimRyfHMkDv89dQsA4O1mjVVD/VHL3kLDyaoGjRY72dnZuHHjhuJ5QkICoqKiYGdnh5o1awJ42kW1c+dOLF26tMT7b968ic2bN6NHjx6wt7dHbGwspkyZAj8/P7Ru3brSjoOIiOhF7j3JxbitkYi8mw4AGNGqFmb0qA+pEYetKotGi52LFy+iY8eOiueTJ08GAAwfPhwbNmwAAGzbtg1CCAwdOrTE+01MTPDnn39ixYoVyM7Ohru7O95++23MmTMHhob8ISIiIs06GpOCqTsvITO/GFamRvhmQCN09+bIQ2XTmguUNamsFzgRERGVRWGxHAv/uIqQv28DAHzdbbFqqB/c7cw1G0zP6NUFykRERLriblouxm6NwOV7GQCA0W08Ma17fZgY6czUdnqHxQ4REZGaHLqSjM9/u4ysgmLYmBlj6UBfdGngpOlYVR6LHSIioteUXyTD/INX8eu5OwCAJh7V8P1QP7jZcq1GbcBih4iI6DUkPMrB2C0RiEl6uvTQx+1rY0pAPRgbcthKW7DYISIiKqf9l5IwY9dl5BTKYGdhgqWDfNHxDUdNx6LnsNghIiJSUX6RDPMOxGLr+bsAgOa17PD9UD8425hqOBmVhsUOERGRCm6kZmPslghcS8mCRAKM7VgHEzrXhRGHrbQWix0iIqIy2hV+D//ZG428IhnsLU3w3WA/tKlrr+lY9AosdoiIiF4ht7AYs/fF4LfwewCAVrWr47vBjeFozWErXcBih4iI6CWuP8hC8OYIxKdmw0ACTOhcD2M71YGhgUTT0aiMWOwQERGVQgiBnRfvYfb+aOQXyeFoJcWKIX5oWbu6pqORiljsEBERPSenoBhf7LmCvVFJAIC2de2xfHBj2FtKNZyMyoPFDhER0b/EJmVi7JYI3HqUA0MDCSZ3rYdP2teGAYetdBaLHSIiIjwdttpy/i7mHYhFYbEcztamWBnkh2a17DQdjV4Tix0iIqrysvKLMGP3Ffx+ORkA0PENBywd1Bh2FiYaTkbqwGKHiIiqtOj7GQjeEoE7abkwMpBgWvc3MLqNF4et9AiLHSIiqpKEENh49g7mH7yKQpkcbrZmWBnkB/+a1TQdjdSMxQ4REVU5GXlF+Py3yzgckwIA6NrACd8MaARbcw5b6SMWO0REVKVEJaZj7JYI3HuSB2NDCWYEvokPWteCRMJhK33FYoeIiKoEIQTWhSVg8eFrKJIJuNuZYdVQf/i622o6GlUwFjtERKT30nMLMXXnJRy7mgoACPR2xqL+jWBjZqzhZFQZWOwQEZFeC7/zGOO2RCIpIx8mhgaY1fNNDGvhwWGrKoTFDhER6SW5XOC/p2/hmyNxkMkFalU3x6ogf3i72Wg6GlUylYqduLg4bN26FadPn8bt27eRm5sLBwcH+Pn5oVu3bujfvz+kUq4bQkREmpWWXYApOy/hRNxDAEAvX1cseMcbVqYctqqKJEII8apGkZGRmDZtGk6fPo1WrVqhefPmcHNzg5mZGR4/fozo6GicPn0amZmZmDZtGiZOnKhTRU9mZiZsbGyQkZEBa2trTcchIqLXcD7hMcZtjcCDzAJIjQwwt3dDDGnmzmErPVTW7+8y9ez07dsXU6dOxfbt22Fn9+I1Qs6ePYvly5dj6dKlmDlzpuqpiYiIykkuF1h94gaWhV6HXABeDhb4Icgfb7rwP7FVXZl6dgoLC2FiUvaJllRtr2ns2SEi0m0PswoweUcUTsc/AgD083PDV329YSHlpan6rKzf3wZl2ZmJiQm+++47pKWllenDdanQISIi3XbmxiP0+P40Tsc/gqmxAb4Z0AjLBjdmoUMKZSp2AGDevHlwc3PDoEGDcPToUZShQ4iIiKjCyOQCy0Ov4911/+BhVgHqOVniwNg2GNjUXdPRSMuUudhJSUnBunXr8PjxYwQGBsLDwwNz5sxBQkJCReYjIiIqITUzH+/+fA4r/oyHEMCgpjWwL7gN6jpZaToaaaEyXbPzvNu3byMkJAQbN25EYmIiOnTogNGjR+Odd97RqbuwnuE1O0REuuPU9YeYtD0KaTmFMDcxxPx3vPGOXw1NxyINUOs1O8+rVasW5s2bh4SEBBw+fBhOTk4YNWoUXF1dVdrPqVOn0KtXL7i6ukIikWDv3r1Kr48YMQISiUTp0aJFC6U2BQUFGDduHOzt7WFhYYHevXvj3r175TksIiLSYsUyOb45cg3DQ84jLacQ9Z2tcGBcGxY69ErlKnaUdmBgAIlEAiEE5HK5Su/NycmBr68vVq1a9cI23bt3R3JysuJx6NAhpdcnTpyIPXv2YNu2bQgLC0N2djZ69uwJmUxWruMhIiLtk5yRh6Cf/sEPx29CCCDorZrYG9watR0sNR2NdEC5LlW/c+cONmzYgA0bNiAxMRHt2rXDTz/9hP79+6u0n8DAQAQGBr60jVQqhbOzc6mvZWRkYN26dfj111/RpUsXAMCmTZvg7u6OY8eOoVu3birlISIi7XP8Wiom74jCk9wiWEqNsLCfD3r5qjaSQFVbmYud/Px87Nq1C+vXr8fJkyfh4uKC4cOHY+TIkfDy8qqwgCdOnICjoyNsbW3Rvn17zJ8/H46OjgCA8PBwFBUVISAgQNHe1dUV3t7eOHPmzAuLnYKCAhQUFCieZ2ZmVlh+IiIqnyKZHN8eicPaU7cAAN5u1lg11B+17C00nIx0TZmLHWdnZ+Tn56Nnz544cOAAunXrBgOD1x4Fe6nAwEAMHDgQHh4eSEhIwKxZs9CpUyeEh4dDKpUiJSUFJiYmqFatmtL7nJyckJKS8sL9Lly4EPPmzavQ7EREVH730/MwbksEIu6mAwBGtKqFGT3qQ2pkqNlgpJPKXOzMnj0b77//Puzt7Ssyj5LBgwcr/uzt7Y2mTZvCw8MDBw8eRL9+/V74PiHES9dAmTFjBiZPnqx4npmZCXd3zstARKQNQmMfYOrOS8jIK4KVqRG+GdAI3b1dNB2LdFiZi51/Fwfp6en47bffcPPmTXz22Wews7NDREQEnJyc4ObmViFBAcDFxQUeHh6Ij48H8LS3qbCwEE+ePFHq3UlNTUWrVq1euB+pVKqTt8gTEemzwmI5Fv1xDev/fjp/m28NG6wK8oe7nbmGk5GuU3kc6vLly6hXrx4WL16Mb7/9Funp6QCAPXv2YMaMGerOpyQtLQ2JiYlwcXla4Tdp0gTGxsYIDQ1VtElOTkZ0dPRLix0iItIuiY9zMfDHM4pCZ1QbT+z8uBULHVILle/Gmjx5MkaMGIElS5bAyup/M1UGBgYiKChIpX1lZ2fjxo0biucJCQmIioqCnZ0d7OzsMHfuXPTv3x8uLi64ffs2Zs6cCXt7e7zzzjsAABsbG4waNQpTpkxB9erVYWdnh6lTp8LHx0dxdxYREWm3P64kY9quy8jKL4aNmTG+HeiLrg2cNB2L9IjKxc6FCxewdu3aEtvd3NxeelFwaS5evIiOHTsqnj8bKhs+fDjWrFmDK1euYOPGjUhPT4eLiws6duyI7du3KxVZy5cvh5GREQYNGoS8vDx07twZGzZsgKEhL2IjItJm+UUyLDh0FRvP3gEA+Ne0xcogf7jZmmk4GekblYsdU1PTUm/VjouLg4ODg0r76tChw0sXFD1y5EiZ8qxcuRIrV65U6bOJiEhzbj/KQfCWCMQkPf0++ai9F6YGvAFjw4q9y5eqJpV/qvr06YMvv/wSRUVFAACJRIK7d+9i+vTpKk8qSEREVc/+S0nouTIMMUmZsLMwQcgHzTAj8E0WOlRhVF4INDMzEz169EBMTAyysrLg6uqKlJQUtGzZEocOHYKFhe5N9sSFQImIKl5+kQzzDsRi6/m7AIDmtezw/VA/ONuYajgZ6aqyfn+rPIxlbW2NsLAw/PXXX4iIiIBcLoe/vz8vCCYiohe6+TAbwZsjcC0lCxIJMLZjHUzoXBdG7M2hSqByz44+Ys8OEVHF2RN5D1/siUZuoQz2liZYPrgx2tZV7RpPotKovWcnLy8Pf/75J3r27Ang6SzE/15fytDQEF999RVMTdkdSUREQF6hDLP3RWNn+D0AQEuv6lgxpDEcrfk9QZWrzMXOxo0b8fvvvyuKnVWrVqFhw4YwM3t6i+C1a9fg6uqKSZMmVUxSIiLSGdcfZCF4cwTiU7MhkQATOtfFuE51YWjw4qV8iCpKmYudzZs3lyhktmzZoljxfNOmTfjhhx9Y7BARVWFCCOwMv4fZ+6KRXySHg5UUK4Y0RqvalbeuItHzynxl2PXr11GvXj3Fc1NTU6VVz5s3b47Y2Fj1piMiIp2RU1CMyTsuYdpvl5FfJEfbuvb4Y0JbFjqkcWXu2cnIyICR0f+aP3z4UOl1uVyudA0PERFVHVeTMxG8JQK3HubAQAJMCXgDn7SvDQMOW5EWKHOxU6NGDURHR+ONN94o9fXLly+jRo0aagtGRETaTwiBrecTMfdADAqL5XC2NsX3Q/3Q3NNO09GIFMo8jNWjRw/Mnj0b+fn5JV7Ly8vDvHnz8Pbbb6s1HBERaa+s/CKM3xaFmXuuoLBYjo5vOODQhLYsdEjrlHmenQcPHqBx48YwMTHB2LFjUa9ePUgkEly7dg2rVq1CcXExIiMj4eSkeyvVcp4dIiLVRN/PwNgtEbidlgsjAwk+6/YGxrT14rAVVSq1z7Pj5OSEM2fO4JNPPsH06dMVC3hKJBJ07doVq1ev1slCh4iIyk4IgV/P3cHXv19FoUwON1szfD/UD008qmk6GtELqbRchKenJw4fPozHjx/jxo0bAIA6derAzo5dlkRE+i4jrwjTd13GH9EpAIAubzrh24GNYGtuouFkRC+n8tpYAGBnZ4fmzZurOwsREWmpS4npGLs1AomP82BsKMH0wDcxsnUtSCQctiLtV6YLlD/++GMkJiaWaYfbt2/H5s2bXysUERFpByEE1oUlYMCPZ5D4OA/udmb47eNWGNXGk4UO6Ywy9ew4ODjA29sbrVq1Qu/evdG0aVO4urrC1NQUT548QWxsLMLCwrBt2za4ubnhv//9b0XnJiKiCpaeW4ipOy/j2NUHAIBAb2cs6t8INmbGGk5GpJoy342VmpqKdevWYdu2bYiOjlZ6zcrKCl26dMGHH36IgICACglakXg3FhGRsvA7TzB+ayTup+fBxNAA/+n5Jt5r4cHeHNIqZf3+LnOx82/p6em4c+cO8vLyYG9vj9q1a+v0LwCLHSKip+RygZ9O38I3R+JQLBeoVd0cq4L84e1mo+loRCWo/dbzf7O1tYWtrW15sxERkRZ6nFOIKTuicDzu6XJAvXxdseAdb1iZctiKdFu5ih0iItIv5xMeY/zWSKRk5kNqZIA5vRpiaHN3ne61J3qGxQ4RURUmlwusOXkTy0KvQyYX8HKwwA9B/njThUP6pD9Y7BARVVGPsgswaXsUTsc/AgD083PDV329YSHlVwPpF/5EExFVQWduPsKEbVF4mFUAU2MDfNnHGwOb1OCwFemlMq96/szcuXNx586dishCREQVTCYX+O7YdQz7+R88zCpAXUdL7B/bBoOa8voc0l8qFzsHDhxA7dq10blzZ2zZsgX5+fkVkYuIiNQsNTMf7637B98di4dcAIOa1sD+sW1Qz8lK09GIKpTKxU54eDgiIiLQqFEjTJo0CS4uLvjkk09w4cKFishHRERqcDr+IXp8fxpnbqbB3MQQywb5YskAX5iZGGo6GlGFU7nYAYBGjRph+fLluH//PtavX4/79++jdevW8PHxwYoVK5CRkaHunEREVA7FMjm+PRKH99efx6PsQtR3tsL+sW3Qz7+GpqMRVZpyFTvPyOVyFBYWoqCgAEII2NnZYc2aNXB3d8f27dvVlZGIiMohOSMPQT/9g1XHb0AIIOitmtgb3Bp1HC01HY2oUpWr2AkPD8fYsWPh4uKCSZMmwc/PD1evXsXJkydx7do1zJkzB+PHj1d3ViIiKqPjcanoseI0zt9+DEupEb4f6ocF7/jA1JjDVlT1qLw2VqNGjXD16lUEBARgzJgx6NWrFwwNlX95Hj58CCcnJ8jlcrWGrShcG4uI9EWRTI5vj8Zh7clbAICGrtb4IcgftewtNJyMSP3K+v2tcs/OwIEDcfv2bRw8eBB9+/YtUegAgIODQ5kKnVOnTqFXr15wdXWFRCLB3r17Fa8VFRXh888/h4+PDywsLODq6or3338fSUlJSvvo0KEDJBKJ0mPIkCGqHhYRkc67n56HwWvPKgqd4S09sOuTVix0qMpTeVLBWbNmqe3Dc3Jy4Ovriw8++AD9+/dXei03NxcRERGYNWsWfH198eTJE0ycOBG9e/fGxYsXldqOGTMGX375peK5mZmZ2jISEemC0NgHmLrzEjLyimBlaoQl/Rsh0MdF07GItILKxc7kyZNL3S6RSGBqaoo6deqgT58+sLOze+W+AgMDERgYWOprNjY2CA0NVdq2cuVKNG/eHHfv3kXNmjUV283NzeHs7KzCURAR6YfCYjkWH76GdWEJAADfGjZYOdQfNaubazgZkfZQudiJjIxEREQEZDIZ3njjDQghEB8fD0NDQ9SvXx+rV6/GlClTEBYWhgYNGqg1bEZGBiQSCWxtbZW2b968GZs2bYKTkxMCAwMxZ84cWFlxkiwi0m+Jj3MxdmskLiWmAwBGtvbE9MD6MDF6rRttifSOysXOs16bkJAQxcVAmZmZGDVqFNq0aYMxY8YgKCgIkyZNwpEjR9QWND8/H9OnT0dQUJDSRUjvvvsuPD094ezsjOjoaMyYMQOXLl0q0Sv0bwUFBSgoKFA8z8zMVFtOIqLKcDg6GZ/9dhlZ+cWwMTPGtwN90bWBk6ZjEWklle/GcnNzQ2hoaIlem5iYGAQEBOD+/fuIiIhAQEAAHj16VPYgEgn27NmDvn37lnitqKgIAwcOxN27d3HixImXXnEdHh6Opk2bIjw8HP7+/qW2mTt3LubNm1diO+/GIiJtV1Asw4KDV/HL2adrFPrVtMXKoX6oUY3DVlT1VNjdWBkZGUhNTS2x/eHDh4oeEltbWxQWFqq661IVFRVh0KBBSEhIQGho6CuLEX9/fxgbGyM+Pv6FbWbMmIGMjAzFIzExUS1ZiYgq0u1HOei/5oyi0PmovRd2fNSShQ7RK5RrGGvkyJFYunQpmjVrBolEgvPnz2Pq1KmKXpnz58+jXr16rx3uWaETHx+P48ePo3r16q98T0xMDIqKiuDi8uK7EKRSKaRS6WvnIyKqLAcuJWHG7ivILihGNXNjLBvUGB3rO2o6FpFOULnYWbt2LSZNmoQhQ4aguLj46U6MjDB8+HAsX74cAFC/fn38/PPPr9xXdnY2bty4oXiekJCAqKgo2NnZwdXVFQMGDEBERAR+//13yGQypKSkAADs7OxgYmKCmzdvYvPmzejRowfs7e0RGxuLKVOmwM/PD61bt1b10IiItE5+kQxf/h6LLf/cBQA0q1UN3w/1g4sNp9ggKiuVrtmRyWQICwuDj48PTExMcOvWLQghULt2bVhaqr7WyokTJ9CxY8cS24cPH465c+fC09Oz1PcdP34cHTp0QGJiIoYNG4bo6GhkZ2fD3d0db7/9NubMmVOmW9+f4QzKRKSNbj7MRvDmCFxLyYJEAgR3qIOJXerCyJB3WxEBZf/+VvkCZVNTU1y9evWFhYguYrFDRNpmT+Q9fLEnGrmFMlS3MMF3QxqjbV0HTcci0ipl/f5WeRjLx8cHt27d0qtih4hIW+QVyjBnfzR2XLwHAGjpVR0rhjSGo7WphpMR6S6Vi5358+dj6tSp+Oqrr9CkSRNYWCivucKeESKi8ol/kIXgLRG4/iAbEgkwvlNdjO9cF4YGEk1HI9JpKg9jGRj8b6xYIvnfL6AQAhKJBDKZTH3pKgmHsYhI03ZeTMSsfdHIL5LDwUqKFYMbo1Ude03HItJqFTaMdfz48dcKRkRE/5NTUIxZ+6KxO+I+AKBtXXssG9QYDlacHoNIXVQudtq3b18ROYiIqpxrKZkI3hyBmw9zYCABJneth0871IEBh62I1Kpc9y+ePn0aw4YNQ6tWrXD//tP/jfz6668ICwtTazgiIn0khMDW83fRZ9XfuPkwB87Wptj2YUuM7VSXhQ5RBVC52Nm1axe6desGMzMzREREKBbUzMrKwoIFC9QekIhIn2TlF2H8tijM2H0FBcVydHjDAYcmtEVzz7LPDUZEqlG52Pn666/x448/4qeffoKxsbFie6tWrRAREaHWcERE+iT6fgZ6rQzDgUtJMDSQYEZgfawf3gx2Fiaajkak11S+ZicuLg7t2rUrsd3a2hrp6enqyEREpFeEENh07g6++v0qCmVyuNqYYmWQP5p4VNN0NKIqQeVix8XFBTdu3ECtWrWUtoeFhcHLy0tduYiI9EJmfhGm77qMQ1eeru3X5U0nfDuwEWzN2ZtDVFlULnY++ugjTJgwAevXr4dEIkFSUhLOnj2LqVOnYvbs2RWRkYhIJ11KTMfYrRFIfJwHY0MJPu9eH6PaeCrNUUZEFU/lYmfatGnIyMhAx44dkZ+fj3bt2kEqlWLq1KkYO3ZsRWQkItIpQgiE/H0bC/+4iiKZQI1qZlgV5I/G7raajkZUJak8g/Izubm5iI2NhVwuR4MGDcq16rm24AzKRKQu6bmF+Oy3ywiNfQAA6N7QGYsHNIKNmfEr3klEqqqwGZSfMTc3R9OmTcv7diIivRNx9wnGbYnE/fQ8mBga4Iu338T7LT04bEWkYSoXOzk5OVi0aBH+/PNPpKamQi6XK71+69YttYUjItIFcrnAz2G3sORwHIrlAh7VzfFDkD+83Ww0HY2IUI5iZ/To0Th58iTee+89uLi48H8sRFSlPc4pxNSdl/DXtVQAQM9GLljYzwdWphy2ItIWKhc7f/zxBw4ePIjWrVtXRB4iIp1x4fZjjN8aieSMfJgYGWBur4YY2tyd/wkk0jIqFzvVqlWDnR2nNSeiqksuF1hz8iaWhV6HTC7gZW+BH971x5suvMGBSBupvFzEV199hdmzZyM3N7ci8hARabVH2QUYHnIe3xyJg0wu8I6fGw6Ma8NCh0iLqdyzs3TpUty8eRNOTk6oVauW0vpYALg+FhHprbM30zBhWyRSswpgamyAL3t7Y2DTGhy2ItJyKhc7ffv2rYAYRETaSyYXWPXXDaz48zrkAqjraIkf3vVHPScrTUcjojIo96SC+oSTChLRi6Rm5WPitiicuZkGABjYpAbm9WkIc5NyT1NGRGpS1u/vMl+zc/78echkMsXz52ukgoIC7NixoxxRiYi0U1j8I/RYcRpnbqbB3MQQywb54puBvix0iHRMmYudli1bIi0tTfHcxsZGaQLB9PR0DB06VL3piIg0oFgmx9KjcXhv/T94lF2I+s5W2D+2Dfr519B0NCIqhzL/9+T5npzSRr84IkZEui4lIx/jt0XifMJjAMDQ5jUxp1cDmBobajgZEZWXWvtieUcCEemyE3GpmLzjEh7nFMLCxBAL+zdCb19XTcciotfEgWciqvKKZHIsPXodP568CQBo6GqNVUH+8LS30HAyIlIHlYqd2NhYpKSkAHg6ZHXt2jVkZ2cDAB49eqT+dEREFSwpPQ/jtkYi/M4TAMD7LT0ws8ebHLYi0iNlvvXcwMAAEomk1Otynm2XSCRKd2zpCt56TlQ1HYt9gKm/XUJ6bhGspEZYPKARevi4aDoWEZVRWb+/y9yzk5CQoJZgRESaVlgsx5LD1/Bz2NN/1xrVsMGqof6oWd1cw8mIqCKUudjx8PCoyBxERJUi8XEuxm6NxKXEdADAyNaemB5YHyZGKi8VSEQ6QqO/3adOnUKvXr3g6uoKiUSCvXv3Kr0uhMDcuXPh6uoKMzMzdOjQATExMUptCgoKMG7cONjb28PCwgK9e/fGvXv3KvEoiEgbyeQCZ2+mYV/UfZy9mQaZXOBwdAp6fH8alxLTYW1qhP++1wSzezVgoUOk5zR6N1ZOTg58fX3xwQcfoH///iVeX7JkCZYtW4YNGzagXr16+Prrr9G1a1fExcXByurpmjQTJ07EgQMHsG3bNlSvXh1TpkxBz549ER4eDkNDXmBIVBUdjk7GvAOxSM7IV2wzNzFEbuHTawr9atpi5VA/1KjGYSuiqkBr1saSSCTYs2ePYqFRIQRcXV0xceJEfP755wCe9uI4OTlh8eLF+Oijj5CRkQEHBwf8+uuvGDx4MAAgKSkJ7u7uOHToELp161amz+YFykT643B0Mj7ZFIEX/cPWtYEjVr/bBMaG7M0h0nVqXxursiUkJCAlJQUBAQGKbVKpFO3bt8eZM2cAAOHh4SgqKlJq4+rqCm9vb0UbIqo6ZHKBeQdiX1joAED0/UwYcAJUoipFa4udZ/P5ODk5KW13cnJSvJaSkgITExNUq1bthW1KU1BQgMzMTKUHEem+8wmPlYauSpOcka9YCoKIqoYyXbPj5+dX5qUgIiIiXivQ857/3Gfz+bzMq9osXLgQ8+bNU0s+ItIeqVkvL3RUbUdE+qFMPTt9+/ZFnz590KdPH3Tr1g03b96EVCpFhw4d0KFDB5iamuLmzZtlvkamLJydnQGgRA9NamqqorfH2dkZhYWFePLkyQvblGbGjBnIyMhQPBITE9WWm4g05/qD7DK1c7QyreAkRKRNytSzM2fOHMWfR48ejfHjx+Orr74q0UadRYOnpyecnZ0RGhoKPz8/AEBhYSFOnjyJxYsXAwCaNGkCY2NjhIaGYtCgQQCA5ORkREdHY8mSJS/ct1QqhVQqVVtWItKsvEIZ5u6PwfaLL/83SALA2cYUzT3tKicYEWkFlW8937lzJy5evFhi+7Bhw9C0aVOsX7++zPvKzs7GjRs3FM8TEhIQFRUFOzs71KxZExMnTsSCBQtQt25d1K1bFwsWLIC5uTmCgoIAADY2Nhg1ahSmTJmC6tWrw87ODlOnToWPjw+6dOmi6qERkQ6Kf5CF4C0RuP4gGxIJ0MPbGQevpEACKF2o/Gxge06vBjA04AXKRFWJysWOmZkZwsLCULduXaXtYWFhMDVVrWv44sWL6Nixo+L55MmTAQDDhw/Hhg0bMG3aNOTl5eHTTz/FkydP8NZbb+Ho0aOKOXYAYPny5TAyMsKgQYOQl5eHzp07Y8OGDZxjh6gK2HkxEbP3xSCvSAYHKylWDG6MVnXs0auUeXacbUwxp1cDdPfm2ldEVY3K8+wsWrQIc+fOxejRo9GiRQsAwLlz57B+/XrMnj0b06dPr5CgFYnz7BDplpyCYszaF43dEfcBAG3q2GP54MZwsPrf8LRMLnA+4TFSs/LhaPV06Io9OkT6pazf3+WaVHDHjh1YsWIFrl69CgB48803MWHCBMV1M7qGxQ6R7riWkongzRG4+TAHBhJgctd6+LRDHRiwkCGqciq02NE3LHaItJ8QAtsvJGLO/hgUFMvhZC3F90P88JZXdU1HIyINKev3d7nWxkpPT8dvv/2GW7duYerUqbCzs0NERAScnJzg5uZW7tBERKXJLijGzN1XsP9SEgCgfT0HLBvki+qWvKuSiF5N5WLn8uXL6NKlC2xsbHD79m2MHj0adnZ22LNnD+7cuYONGzdWRE4iqqJikjIwdkskEh7lwNBAgs+6vYEP23px2IqIykzl5SImT56MESNGID4+Xunuq8DAQJw6dUqt4Yio6hJC4Ndzd/DO6jNIeJQDVxtT7PioBT5uX5uFDhGpROWenQsXLmDt2rUltru5ub10PSoiorLKzC/CjF1XcPBKMgCgy5uO+GaAL6pZmGg4GRHpIpWLHVNT01IXzoyLi4ODg4NaQhFR1XX5XjrGbonE3ce5MDKQYHpgfYxq41nm9fmIiJ6n8jBWnz598OWXX6KoqAjA04U67969i+nTp6N///5qD0hEVYMQAuvDEtB/zRncfZyLGtXM8NsnrTC6rRcLHSJ6LSrfep6ZmYkePXogJiYGWVlZcHV1RUpKClq2bIlDhw7BwsKiorJWGN56TqRZGblF+Oy3Szga+wAA0L2hMxYPaAQbM2MNJyMibVZht55bW1sjLCwMf/31FyIiIiCXy+Hv78+1qIioXCLvPsHYLZG4n54HE0MDfPH2m3i/pQd7c4hIbVTu2dm4cSMGDx5cYtXwwsJCbNu2De+//75aA1YG9uwQVT65XGBdWAIWH76GYrmAR3VzrBrqD58aNpqORkQ6osJmUDY0NERycjIcHR2VtqelpcHR0REymax8iTWIxQ5R5XqSU4gpOy/hr2upAIC3G7lgUT8fWJly2IqIyq7ChrGEEKV2L9+7dw82NvwfGRG93MXbjzFuaySSM/JhYmSAOb0aIKh5TQ5bEVGFKXOx4+fnB4lEAolEgs6dO8PI6H9vlclkSEhIQPfu3SskJBHpPrlc4MdTN7H06HXI5AJe9hZYFeSPBq7sTSWiilXmYqdv374AgKioKHTr1g2WlpaK10xMTFCrVi3eek5EpXqUXYDJOy7h1PWHAIC+jV3x9Ts+sJSWa3k+IiKVlPlfmjlz5gAAatWqhSFDhpS4QJmIqDTnbqVh/NZIpGYVwNTYAF/29sbApjU4bEVElUblSQUbNGiAqKioEtv/+ecfXLx4UR2ZiEgPyOQC3/8Zj6CfziE1qwB1HC2xL7gNBjVzZ6FDRJVK5WInODgYiYmJJbbfv38fwcHBaglFRLotNSsf76//B8tCr0MugIFNamD/2NZ4w9lK09GIqApSecA8NjYW/v7+Jbb7+fkhNjZWLaGISHf9feMRJmyLwqPsApgZG2L+O97o519D07GIqApTudiRSqV48OABvLy8lLYnJycr3aFFRFVLsUyO7/+Mx8rjNyAEUN/ZCquC/FHH0fLVbyYiqkAqD2N17doVM2bMQEZGhmJbeno6Zs6cia5du6o1HBHphgeZ+Qj6+R98/9fTQmdoc3fsDW7NQoeItILKXTFLly5Fu3bt4OHhAT8/PwBPb0d3cnLCr7/+qvaARKTdTsSlYvKOS3icUwgLE0Ms6OeDPo3dNB2LiEhB5WLHzc0Nly9fxubNm3Hp0iWYmZnhgw8+wNChQ2FszKneiaqKYpkcS0OvY82JmwCABi7W+OFdf3jaW2g4GRGRsnJdZGNhYYEPP/xQ3VmISEckpedh/NZIXLzzBADwXgsPfPH2mzA1NtRwMiKiklQudjZu3PjS13Vx1XMiKrs/rz7AlJ2XkJ5bBCupERYPaIQePi6ajkVE9EIqr3perVo1pedFRUXIzc2FiYkJzM3N8fjxY7UGrAxc9Zzo1QqL5fjmyDX8dDoBANCohg1WDfVHzermGk5GRFVVha16/uTJkxLb4uPj8cknn+Czzz5TdXdEpAMSH+di3NZIRCWmAwA+aF0L0wPrQ2rEYSsi0n5qmRinbt26WLRoEYYNG4Zr166pY5dEpCWOxKTgs52XkJlfDGtTI3wz0BfdGjprOhYRUZmpbRZAQ0NDJCUlqWt3RKRhBcUyLDx0DRvO3AYA+NW0xcqhfqhRjcNWRKRbVC529u/fr/RcCIHk5GSsWrUKrVu3VlswItKcO2k5GLslElfuP5089MN2Xvis2xswNlR5HlIiIo1Tudjp27ev0nOJRAIHBwd06tQJS5cuVVcuItKQg5eTMX3XZWQVFKOauTGWDvJFp/pOmo5FRFRuKhc7crm8InIQkYblF8nw9cFYbDp3FwDQ1KMaVgb5wcXGTMPJiIhej0p90kVFRfDy8qrU1c1r1aoFiURS4hEcHAwAGDFiRInXWrRoUWn5iPTBrYfZeGf1GUWh82mH2tj2YQsWOkSkF1Tq2TE2NkZBQQEkEklF5SnhwoULkMlkiufR0dHo2rUrBg4cqNjWvXt3hISEKJ6bmJhUWj4iXbcv6j5m7r6CnEIZqluYYNngxmhfz0HTsYiI1EblYaxx48Zh8eLF+Pnnn2FkpLabuV7IwUH5H91Fixahdu3aaN++vWKbVCqFszNvhSVSRV6hDPMOxGDbhUQAQAsvO6wY4gcna1MNJyMiUi+Vq5V//vkHf/75J44ePQofHx9YWCgv+rd79261hXteYWEhNm3ahMmTJyv1Lp04cQKOjo6wtbVF+/btMX/+fDg6Or5wPwUFBSgoKFA8z8zMrLDMRNroRmoWgjdHIu5BFiQSYFynupjQuS4MDSqv15aIqLKoXOzY2tqif//+FZHllfbu3Yv09HSMGDFCsS0wMBADBw6Eh4cHEhISMGvWLHTq1Anh4eGQSqWl7mfhwoWYN29eJaUm0i6/hd/DrL3RyCuSwd5SihVDGqN1HXtNxyIiqjAqr42lSd26dYOJiQkOHDjwwjbJycnw8PDAtm3b0K9fv1LblNaz4+7uzrWxSK/lFhZj1t4Y7Iq4BwBoU8ceywc3hoNV6f8pICLSdmVdG0vlGcI6deqE9PT0Uj+wU6dOqu6uzO7cuYNjx45h9OjRL23n4uICDw8PxMfHv7CNVCqFtbW10oNIn8WlZKH3qr+xK+IeDCTAlK718MvI5ix0iKhKUHkY68SJEygsLCyxPT8/H6dPn1ZLqNKEhITA0dERb7/99kvbpaWlITExES4uLhWWhUhXCCGw/UIi5uyPQUGxHE7WUqwY4ocWXtU1HY2IqNKUudi5fPmy4s+xsbFISUlRPJfJZDh8+DDc3NzUm+7/yeVyhISEYPjw4Up3gGVnZ2Pu3Lno378/XFxccPv2bcycORP29vZ45513KiQLka7ILijGF3uuYF/U0zXr2tdzwLJBvqhuyd4cIqpaylzsNG7cWDFpX2nDVWZmZli5cqVawz1z7Ngx3L17FyNHjlTabmhoiCtXrmDjxo1IT0+Hi4sLOnbsiO3bt8PKyqpCshDpgpikDIzbEolbj3JgaCDB1IA38FE7LxjwbisiqoLKfIHynTt3IISAl5cXzp8/rzT/jYmJCRwdHWFoaFhhQStSWS9wItJ2Qghs+ucuvvo9FoXFcrjYmGLlUD80rWWn6WhERGpX1u/vMvfseHh4AODaWETaKjO/CDN2X8HBy8kAgM71HfHtQF9Us+CM4kRUtal8N9Yvv/yCgwcPKp5PmzYNtra2aNWqFe7cuaPWcERUNlfuZaDn92E4eDkZRgYS/OftN/Hz8KYsdIiIUI5iZ8GCBTAze7o44NmzZ7Fq1SosWbIE9vb2mDRpktoDEtGLCSGw4e8E9F9zBncf58LN1gw7P26J0W29KnUNOyIibabyreeJiYmoU6cOgKczGg8YMAAffvghWrdujQ4dOqg7HxG9QEZuEabtuoQjMQ8AAN0aOmFJf1/YmBtrOBkRkXZRuWfH0tISaWlpAICjR4+iS5cuAABTU1Pk5eWpNx0RlSry7hO8vfI0jsQ8gImhAeb2aoAfhzVhoUNEVAqVe3a6du2K0aNHw8/PD9evX1dM8hcTE4NatWqpOx8R/YsQAj+fTsDiw9dQLBeoaWeOH4L84VPDRtPRiIi0lso9Oz/88ANatmyJhw8fYteuXahe/elMrOHh4Rg6dKjaAxLRU09yCjH6l4uYf+gqiuUCbzdywe/j27DQISJ6BZ1aCLSicJ4d0nYXbz/G+K2RSMrIh4mRAWb3bIB336rJi5CJqEpT+zw7/5aeno7z588jNTVVad4diUSC9957rzy7JKJSyOUCP566iaVHr0MmF/C0t8CqID80dGVvDhFRWalc7Bw4cADvvvsucnJyYGVlpfQ/SxY7ROqTll2AyTsu4eT1hwCAPo1dMf8dH1hKy/V/FCKiKkvlfzWnTJmCkSNHYsGCBTA3N6+ITERV3j+30jB+WyQeZBZAamSAL/s0xKCm7hy2IiIqB5WLnfv372P8+PEsdIgqgEwusPr4DSw/dh1yAdRxtMQPQf54w5kL2xIRlZfKxU63bt1w8eJFeHl5VUQeoirrYVYBJm6PxN83ns5jNaBJDXzZpyHMTThsRUT0OlT+V/Ttt9/GZ599htjYWPj4+MDYWHkSs969e6stHFFV8feNR5iwLQqPsgtgZmyIr/t6o3+TGpqORUSkF1S+9dzA4MVT80gkEshkstcOVdl46zlpikwusOLPeKz8Kx5CAG84WeGHd/1Qx5HDVkREr1Jht57/+1ZzIiq/B5n5mLAtEuduPQYADG3ujjm9GsLU2FDDyYiI9AsvBiDSgJPXH2Ly9iik5RTCwsQQC/r5oE9jN03HIiLSSyovFwEAJ0+eRK9evVCnTh3UrVsXvXv3xunTp9WdjUjvFMvkWHz4GoavP4+0nEK86WKNA+PasNAhIqpAKhc7mzZtQpcuXWBubo7x48dj7NixMDMzQ+fOnbFly5aKyEikF5LS8zDkv+ew5sRNAMB7LTyw59NW8HKw1HAyIiL9pvIFym+++SY+/PBDTJo0SWn7smXL8NNPP+Hq1atqDVgZeIEyVbS/rj3A5B2XkJ5bBCupERb1b4S3G7loOhYRkU4r6/e3yj07t27dQq9evUps7927NxISElTdHZFeK5LJseDQVYzccBHpuUXwcbPB7+PbsNAhIqpEKl+g7O7ujj///BN16tRR2v7nn3/C3d1dbcGIdN29J7kYuyUSUYnpAIAPWtfC9MD6kBrxbisiospUrrWxxo8fj6ioKLRq1QoSiQRhYWHYsGEDVqxYUREZiXTOkZgUfLbzEjLzi2FtaoRvBvqiW0NnTcciIqqSVC52PvnkEzg7O2Pp0qXYsWMHgKfX8Wzfvh19+vRRe0AiXVJYLMfCP64i5O/bAIDG7rZYOdQP7nZcS46ISFNUvkBZH/ECZVKHu2m5GLs1ApfvZQAAxrT1xGfd6sPEqFwzPBAR0StU2AzKFy5cgFwux1tvvaW0/Z9//oGhoSGaNm2qeloiHXfoSjI+/+0ysgqKYWtujKUDfdH5TSdNxyIiIpTjbqzg4GAkJiaW2H7//n0EBwerJRSRrsgvkmHW3mh8ujkCWQXFaOpRDYfGt2WhQ0SkRVTu2YmNjYW/v3+J7X5+foiNjVVLKCJdkPAoB8GbIxCbnAkA+LRDbUzqWg/Ghhy2IiLSJioXO1KpFA8ePICXl5fS9uTkZBgZcakt0n0yucD5hMdIzcqHo5UpmnvawdBAotRmX9R9zNx9BTmFMthZmGD54MZoX89BQ4mJiOhlVK5OunbtihkzZmDfvn2wsbEBAKSnp2PmzJno2rWr2gMSVabD0cmYdyAWyRn5im0uNqaY06sBunu7IL9IhnkHYrD1/NOh3Lc87fD9UD84WZtqKjIREb2Cyndj3b9/H+3atUNaWhr8/PwAAFFRUXByckJoaKhOTizIu7EIeFrofLIpAs//Qjzr05nTqwG2nk9E3IMsSCTAuE51Mb5THRhx2IqISCMqbLkINzc3XL58GUuWLEGDBg3QpEkTrFixAleuXFF7oTN37lxIJBKlh7Pz/yZmE0Jg7ty5cHV1hZmZGTp06ICYmBi1ZqCqQSYXmHcgtkShAwDi/x/zDsQi7kEW7C2l2DTqLUzuWo+FDhGRDijXRTYWFhb48MMP1Z2lVA0bNsSxY8cUzw0N/zfV/pIlS7Bs2TJs2LAB9erVw9dff42uXbsiLi4OVlZWlZKP9MP5hMdKQ1elEQC8Xa2x/oNmcLTisBURka4o139Lf/31V7Rp0waurq64c+cOAGD58uXYt2+fWsMBgJGREZydnRUPB4enF4EKIfDdd9/hiy++QL9+/eDt7Y1ffvkFubm52LJli9pzkH5LzXp5ofPMqDaeLHSIiHSMysXOmjVrMHnyZAQGBuLJkyeQyWQAgGrVquG7775Tdz7Ex8fD1dUVnp6eGDJkCG7dugUASEhIQEpKCgICAhRtpVIp2rdvjzNnzrx0nwUFBcjMzFR6UNVW1gLG2casgpMQEZG6qVzsrFy5Ej/99BO++OILpVvNmzZtiitXrqg13FtvvYWNGzfiyJEj+Omnn5CSkoJWrVohLS0NKSkpAAAnJ+XJ25ycnBSvvcjChQthY2OjeOjiRdWkXs097eBi8+KCR4Knd2U197SrvFBERKQWKhc7CQkJiruw/k0qlSInJ0ctoZ4JDAxE//794ePjgy5duuDgwYMAgF9++UXRRiJRnv9ECFFi2/NmzJiBjIwMxaO0GaGpajE0kGBUm1qlvvbvu7Gen2+HiIi0n8rFjqenJ6Kiokps/+OPP9CgQQN1ZHohCwsL+Pj4ID4+XnFX1vO9OKmpqSV6e54nlUphbW2t9KCqSwiBzf/cwZIj1wEAz9czzjamWDPMH929XTSQjoiIXpfKd2N99tlnCA4ORn5+PoQQOH/+PLZu3YqFCxfi559/roiMCgUFBbh69Sratm0LT09PODs7IzQ0VNHTVFhYiJMnT2Lx4sUVmoP0R1Z+EabvvoKDl5MBAJ3rO2LxgEaIf5D90hmUiYhId6hc7HzwwQcoLi7GtGnTkJubi6CgILi5uWHFihUYMmSIWsNNnToVvXr1Qs2aNZGamoqvv/4amZmZGD58OCQSCSZOnIgFCxagbt26qFu3LhYsWABzc3MEBQWpNQfppyv3MjB2awTupOXCyECC6YH1MaqNJyQSCewtpZqOR0REalKueXbGjBmDMWPG4NGjR5DL5XB0dATwdHZlNzc3tYW7d+8ehg4dikePHsHBwQEtWrTAuXPn4OHhAQCYNm0a8vLy8Omnn+LJkyd46623cPToUc6xQy8lhMAvZ25jwaFrKJTJ4WZrhlVBfvCrWU3T0YiIqAKovFxEaVJSUjB//nz8/PPPyMvLU0euSsXlIqqOjLwifP7bZRyOeXqtV0ADJ3wzwBc25sYaTkZERKpS+3IR6enpePfdd+Hg4ABXV1d8//33kMvlmD17Nry8vHDu3DmsX79eLeGJKkJUYjre/v40DsekwNhQgjm9GmDte01Y6BAR6bkyD2PNnDkTp06dwvDhw3H48GFMmjQJhw8fRn5+Pv744w+0b9++InMSlZsQAuvCErDoj2solgvUtDPHqiA/NKphq+loRERUCcpc7Bw8eBAhISHo0qULPv30U9SpUwf16tWrkFmTidQlPbcQU3dewrGrqQCAt31csLC/D6xN2ZtDRFRVlLnYSUpKUsyj4+XlBVNTU4wePbrCghG9rvA7jzFuSySSMvJhYmSAWT0bYNhbNV856SQREemXMhc7crkcxsb/+9+woaEhLCwsKiQU0euQywX+e/oWvjkSB5lcwNPeAquC/NDQ1UbT0YiISAPKXOwIITBixAhIpU/nH8nPz8fHH39couDZvXu3ehMSqSAtuwBTdl7CibiHAIA+jV0x/x0fWErLNcsCERHpgTJ/AwwfPlzp+bBhw9Qehuh1/HMrDeO3ReJBZgGkRgb4sk9DDGrqzmErIqIqrszFTkhISEXmICo3mVxg9fEbWH7sOuQCqO1ggdXvNsEbzpxckoiIyjmDMpG2eJhVgEnboxB24xEAoL9/DXzVtyHMTfijTURET/EbgXTWmRuPMGF7FB5mFcDM2BBf9fXGgCY1NB2LiIi0DIsd0jkyucCKP+Ox8q94CAG84WSFVUF+qOvEYSsiIiqJxQ7plAeZ+ZiwLRLnbj0GAAxp5o45vRrCzMRQw8mIiEhbsdghnXHq+kNM2h6FtJxCWJgYYkE/H/Rp7KbpWEREpOVY7JDWK5bJsfzYdaw+cRNCAG+6WOOHID94OVhqOhoREekAFjuk1ZIz8jB+ayQu3H4CABjWoib+83YDmBpz2IqIiMqGxQ5prePXUjF5RxSe5BbBSmqEhf190LORq6ZjERGRjmGxQ1qnSCbHt0fisPbULQCAj5sNVgX5waM612IjIiLVsdghrXLvSS7GbY1E5N10AMCIVrUwo0d9SI04bEVEROXDYoe0xtGYFHz222Vk5BXB2tQISwb4oru3s6ZjERGRjmOxQxpXWCzHwj+uIuTv2wAAX3dbrBrqB3c7c80GIyIivcBihzTqblouxm6NwOV7GQCAMW098Vm3+jAxMtBwMiIi0hcsdkhj/riSjGm/XUZWQTFszY3x7QBfdGngpOlYRESkZ1jsUKXLL5JhwaGr2Hj2DgCgiUc1rBzqB1dbMw0nIyIifcRihypVwqMcjN0SgZikTADAJx1qY3LXejA25LAVERFVDBY7VGn2X0rCzN1XkF1QDDsLEywb5IsObzhqOhYREek5FjtU4fKLZJh3IBZbz98FADT3tMP3Q/zgbGOq4WRERFQVsNihCnUjNRtjt0TgWkoWJBJgXMc6GN+5Low4bEVERJWExQ5VmN0R9/CfvdHILZTB3lKK7wY3Rpu69pqORUREVQyLHVK73MJizNkXg53h9wAArWpXx3dDGsPRisNWRERU+VjskFpdf5CF4M0RiE/NhoEEmNilHoI71oGhgUTT0YiIqIpisUNqIYTAzov3MHt/NPKL5HC0kmLFED+0rF1d09GIiKiK0+qrRBcuXIhmzZrBysoKjo6O6Nu3L+Li4pTajBgxAhKJROnRokULDSWumnIKijF5xyVM23UZ+UVytK1rj0MT2rLQISIiraDVPTsnT55EcHAwmjVrhuLiYnzxxRcICAhAbGwsLCwsFO26d++OkJAQxXMTExNNxK2SriZnInhzBG49yoGhgQRTAurh43a1YcBhKyIi0hJaXewcPnxY6XlISAgcHR0RHh6Odu3aKbZLpVI4OztXdrwqTQiBLefvYt6BWBQWy+FiY4rvh/qhWS07TUcjIiJSotXFzvMyMp6ujG1np/yFeuLECTg6OsLW1hbt27fH/Pnz4ej44pl5CwoKUFBQoHiemZlZMYH1VFZ+EWbsvoLfLycDADrVd8S3A31hZ8EeNSIi0j4SIYTQdIiyEEKgT58+ePLkCU6fPq3Yvn37dlhaWsLDwwMJCQmYNWsWiouLER4eDqlUWuq+5s6di3nz5pXYnpGRAWtr6wo7Bn0QfT8DY7dE4HZaLowMJPi8e32MauPJYSsiIqp0mZmZsLGxeeX3t84UO8HBwTh48CDCwsJQo0aNF7ZLTk6Gh4cHtm3bhn79+pXaprSeHXd3dxY7LyGEwMazdzD/4FUUyuRwszXDyiA/+NespuloRERURZW12NGJYaxx48Zh//79OHXq1EsLHQBwcXGBh4cH4uPjX9hGKpW+sNeHSsrIK8L0XZfxR3QKACCggRO+GeALG3NjDScjIiJ6Na0udoQQGDduHPbs2YMTJ07A09Pzle9JS0tDYmIiXFxcKiGh/otKTMfYLRG49yQPxoYSzOzxJka0qgWJhMNWRESkG7S62AkODsaWLVuwb98+WFlZISXlac+CjY0NzMzMkJ2djblz56J///5wcXHB7du3MXPmTNjb2+Odd97RcHrdJoTAurAELD58DUUygZp25lgV5IdGNWw1HY2IiEglWn3Nzot6D0JCQjBixAjk5eWhb9++iIyMRHp6OlxcXNCxY0d89dVXcHd3L/PnlHXMr6pIzy3E1J2XcezqAwBADx9nLOrfCNamHLYiIiLtoRfX7LyqDjMzM8ORI0cqKU3VEH7nCcZtiUBSRj5MjAwwq2cDDHurJoetiIhIZ2l1sUOVRy4X+O/pW/jmSBxkcgFPewusCvJDQ1cbTUcjIiJ6LSx2CI9zCjF5RxROxD0EAPT2dcWCfj6wlPLHg4iIdB+/zaq48wmPMX5rJFIy8yE1MsC83g0xuJk7h62IiEhvsNipouRygdUnbmBZ6HXIBVDbwQI/vOuP+s68QJuIiPQLi50q6GFWASbviMLp+EcAgH7+bviqjzcsOGxFRER6iN9uVcyZm48wYVsUHmYVwMzYEF/2aYiBTct+mz4REZGuYbFTRcjkAiv/isf3f8ZDLoB6Tpb4IcgfdZ2sNB2NiIioQrHYqQJSM/MxYVsUzt5KAwAMbuqOub0bwszEUMPJiIiIKh6LHT13Ov4hJm2PwqPsQpibGGLBOz7o6+em6VhERESVhsWOniqWyfHdsXj8cOIGhADedLHGD0F+8HKw1HQ0IiKiSsViRw8lZ+RhwtYonL/9GADw7ls1MatnA5gac9iKiIiqHhY7OkQmFzif8BipWflwtDJFc087GBooT/53/FoqJu+IwpPcIlhKjbCovw96NnLVUGIiIiLNY7GjIw5HJ2PegVgkZ+QrtrnYmGJOrwbo7u2CIpkc3x6Jw9pTtwAA3m7W+CHIHx7VLTQVmYiISCuw2NEBh6OT8cmmCDy/BnxKRj4+2RSB+e9447fwe4i4mw4AGNGqFmb0qA+pEYetiIiIWOxoOZlcYN6B2BKFDgDFti/2RkMIwMrUCN8MaITu3i6VGZGIiEirsdjRcucTHisNXZVG/P/aVhs+aA53O/NKSkZERKQbDDQdgF4uNevlhc4zn3aozUKHiIioFCx2tJyjlWmZ2rnastAhIiIqDYsdLdfc0w4uNi8ueCR4eldWc0+7ygtFRESkQ1jsaDlDAwk+6eBV6mvPZtiZ06tBifl2iIiI6CkWO1ruwKUkLDl8HQDwfD3jbGOKNcP8efcVERHRS/BuLC2VXyTDl7/HYss/dwE8Hc5aPqgx7j7OfekMykRERKSMxY4WuvkwG8GbI3AtJQsSCTC2Yx1M6FwXRoYGcKtmpul4REREOoXFjpbZE3kPX+yJRm6hDPaWJvhusB/a1LXXdCwiIiKdxWJHS+QVyjBnfzR2XLwHAGhVuzq+G9wYjtZlu/WciIiISsdiRwvEP8jCp5sjEJ+aDQMJMKFzPYztVIfX4xAREakBix0NEkJgZ/g9zN4XjfwiORytpFgxxA8ta1fXdDQiIiK9wWJHQ3IKijFrbzR2R94HALSta4/lgxvD3lKq4WRERET6hcWOBlxNzkTwlgjcepgDQwMJJneth0/a14YBh62IiIjUjsVOJRJCYOv5RMw7EIOCYjmcrU2xMsgPzWpxqQciIqKKwmKnkmTlF2HmnmgcuJQEAOhU3xHfDvSFnYWJhpMRERHpN71ZLmL16tXw9PSEqakpmjRpgtOnT2s6kkL0/Qz0WhmGA5eSYGQgwcwe9fHz+01Z6BAREVUCvSh2tm/fjokTJ+KLL75AZGQk2rZti8DAQNy9e1ejuYQQ2Hj2NvqtPoPbablwszXDjo9b4sN2vD6HiIioskiEEELTIV7XW2+9BX9/f6xZs0ax7c0330Tfvn2xcOHCV74/MzMTNjY2yMjIgLW1tVoyCSEwaXsU9kY9Hbbq2sAJ3wxoBFtz9uYQERGpQ1m/v3W+Z6ewsBDh4eEICAhQ2h4QEIAzZ85oKBUgkUjgV7MajA0lmN2zAf77XhMWOkRERBqg8xcoP3r0CDKZDE5OTkrbnZyckJKSUup7CgoKUFBQoHiemZlZIdneb+mBtnXt4eVgWSH7JyIiolfT+Z6dZyQS5WtghBAltj2zcOFC2NjYKB7u7u4VlomFDhERkWbpfLFjb28PQ0PDEr04qampJXp7npkxYwYyMjIUj8TExMqISkRERBqg88WOiYkJmjRpgtDQUKXtoaGhaNWqVanvkUqlsLa2VnoQERGRftL5a3YAYPLkyXjvvffQtGlTtGzZEv/9739x9+5dfPzxx5qORkRERBqmF8XO4MGDkZaWhi+//BLJycnw9vbGoUOH4OHhoeloREREpGF6Mc/O66qIeXaIiIioYlWZeXaIiIiIXobFDhEREek1FjtERESk11jsEBERkV5jsUNERER6jcUOERER6TUWO0RERKTXWOwQERGRXtOLGZRf17N5FTMzMzWchIiIiMrq2ff2q+ZHZrEDICsrCwDg7u6u4SRERESkqqysLNjY2LzwdS4XAUAulyMpKQlWVlaQSCSvvb/MzEy4u7sjMTFRb5ef0Pdj1PfjA3iM+kDfjw/gMeqDijw+IQSysrLg6uoKA4MXX5nDnh0ABgYGqFGjhtr3a21trZc/uP+m78eo78cH8Bj1gb4fH8Bj1AcVdXwv69F5hhcoExERkV5jsUNERER6jcVOBZBKpZgzZw6kUqmmo1QYfT9GfT8+gMeoD/T9+AAeoz7QhuPjBcpERESk19izQ0RERHqNxQ4RERHpNRY7REREpNdY7BAREZFeY7FTAVavXg1PT0+YmpqiSZMmOH36tKYjlcvChQvRrFkzWFlZwdHREX379kVcXJxSmxEjRkAikSg9WrRooaHEqps7d26J/M7OzorXhRCYO3cuXF1dYWZmhg4dOiAmJkaDiVVTq1atEscnkUgQHBwMQDfP36lTp9CrVy+4urpCIpFg7969Sq+X5ZwVFBRg3LhxsLe3h4WFBXr37o179+5V4lG83MuOsaioCJ9//jl8fHxgYWEBV1dXvP/++0hKSlLaR4cOHUqc2yFDhlTykZTuVeewLD+XunwOAZT6eymRSPDNN98o2mjzOSzL94M2/S6y2FGz7du3Y+LEifjiiy8QGRmJtm3bIjAwEHfv3tV0NJWdPHkSwcHBOHfuHEJDQ1FcXIyAgADk5OQotevevTuSk5MVj0OHDmkocfk0bNhQKf+VK1cUry1ZsgTLli3DqlWrcOHCBTg7O6Nr166K9dS03YULF5SOLTQ0FAAwcOBARRtdO385OTnw9fXFqlWrSn29LOds4sSJ2LNnD7Zt24awsDBkZ2ejZ8+ekMlklXUYL/WyY8zNzUVERARmzZqFiIgI7N69G9evX0fv3r1LtB0zZozSuV27dm1lxH+lV51D4NU/l7p8DgEoHVtycjLWr18PiUSC/v37K7XT1nNYlu8HrfpdFKRWzZs3Fx9//LHStvr164vp06drKJH6pKamCgDi5MmTim3Dhw8Xffr00Vyo1zRnzhzh6+tb6mtyuVw4OzuLRYsWKbbl5+cLGxsb8eOPP1ZSQvWaMGGCqF27tpDL5UII3T9/AMSePXsUz8tyztLT04WxsbHYtm2bos39+/eFgYGBOHz4cKVlL6vnj7E058+fFwDEnTt3FNvat28vJkyYULHh1KC043vVz6U+nsM+ffqITp06KW3TlXMoRMnvB237XWTPjhoVFhYiPDwcAQEBStsDAgJw5swZDaVSn4yMDACAnZ2d0vYTJ07A0dER9erVw5gxY5CamqqJeOUWHx8PV1dXeHp6YsiQIbh16xYAICEhASkpKUrnUyqVon379jp5PgsLC7Fp0yaMHDlSacFbXT9//1aWcxYeHo6ioiKlNq6urvD29tbJ8wo8/d2USCSwtbVV2r5582bY29ujYcOGmDp1qs70SAIv/7nUt3P44MEDHDx4EKNGjSrxmq6cw+e/H7Ttd5ELgarRo0ePIJPJ4OTkpLTdyckJKSkpGkqlHkIITJ48GW3atIG3t7die2BgIAYOHAgPDw8kJCRg1qxZ6NSpE8LDw3ViNtC33noLGzduRL169fDgwQN8/fXXaNWqFWJiYhTnrLTzeefOHU3EfS179+5Feno6RowYodim6+fveWU5ZykpKTAxMUG1atVKtNHF39P8/HxMnz4dQUFBSossvvvuu/D09ISzszOio6MxY8YMXLp0STGUqc1e9XOpb+fwl19+gZWVFfr166e0XVfOYWnfD9r2u8hipwL8+3/NwNMfhOe36ZqxY8fi8uXLCAsLU9o+ePBgxZ+9vb3RtGlTeHh44ODBgyV+cbVRYGCg4s8+Pj5o2bIlateujV9++UVxQaS+nM9169YhMDAQrq6uim26fv5epDznTBfPa1FREYYMGQK5XI7Vq1crvTZmzBjFn729vVG3bl00bdoUERER8Pf3r+yoKinvz6UunkMAWL9+Pd59912YmpoqbdeVc/ii7wdAe34XOYylRvb29jA0NCxRkaamppaobnXJuHHjsH//fhw/fhw1atR4aVsXFxd4eHggPj6+ktKpl4WFBXx8fBAfH6+4K0sfzuedO3dw7NgxjB49+qXtdP38leWcOTs7o7CwEE+ePHlhG11QVFSEQYMGISEhAaGhoUq9OqXx9/eHsbGxTp7b538u9eUcAsDp06cRFxf3yt9NQDvP4Yu+H7Ttd5HFjhqZmJigSZMmJboYQ0ND0apVKw2lKj8hBMaOHYvdu3fjr7/+gqen5yvfk5aWhsTERLi4uFRCQvUrKCjA1atX4eLioug+/vf5LCwsxMmTJ3XufIaEhMDR0RFvv/32S9vp+vkryzlr0qQJjI2NldokJycjOjpaZ87rs0InPj4ex44dQ/Xq1V/5npiYGBQVFenkuX3+51IfzuEz69atQ5MmTeDr6/vKttp0Dl/1/aB1v4tqvdyZxLZt24SxsbFYt26diI2NFRMnThQWFhbi9u3bmo6msk8++UTY2NiIEydOiOTkZMUjNzdXCCFEVlaWmDJlijhz5oxISEgQx48fFy1bthRubm4iMzNTw+nLZsqUKeLEiRPi1q1b4ty5c6Jnz57CyspKcb4WLVokbGxsxO7du8WVK1fE0KFDhYuLi84cnxBCyGQyUbNmTfH5558rbdfV85eVlSUiIyNFZGSkACCWLVsmIiMjFXcileWcffzxx6JGjRri2LFjIiIiQnTq1En4+vqK4uJiTR2WkpcdY1FRkejdu7eoUaOGiIqKUvrdLCgoEEIIcePGDTFv3jxx4cIFkZCQIA4ePCjq168v/Pz8tOIYX3Z8Zf251OVz+ExGRoYwNzcXa9asKfF+bT+Hr/p+EEK7fhdZ7FSAH374QXh4eAgTExPh7++vdKu2LgFQ6iMkJEQIIURubq4ICAgQDg4OwtjYWNSsWVMMHz5c3L17V7PBVTB48GDh4uIijI2Nhaurq+jXr5+IiYlRvC6Xy8WcOXOEs7OzkEqlol27duLKlSsaTKy6I0eOCAAiLi5Oabuunr/jx4+X+nM5fPhwIUTZzlleXp4YO3assLOzE2ZmZqJnz55addwvO8aEhIQX/m4eP35cCCHE3bt3Rbt27YSdnZ0wMTERtWvXFuPHjxdpaWmaPbD/97LjK+vPpS6fw2fWrl0rzMzMRHp6eon3a/s5fNX3gxDa9bso+f/QRERERHqJ1+wQERGRXmOxQ0RERHqNxQ4RERHpNRY7REREpNdY7BAREZFeY7FDREREeo3FDhEREek1FjtEpBKJRIK9e/eWqe3cuXPRuHHjCs2jq9LS0uDo6Ijbt29rLMOqVavQu3dvjX0+UWVhsUOk40aMGAGJRAKJRAJjY2N4eXlh6tSpyMnJea39vqhQSU5OVlotvqL9+/j+/ejevXulZagICxcuRK9evVCrVi2l7bt27UKnTp1QrVo1mJub44033sDIkSMRGRlZpv0WFhbC3t4eX3/99Qs/197eHoWFhRgzZgwuXLhQ6mrVRPqExQ6RHujevTuSk5Nx69YtfP3111i9ejWmTp1arn0JIVBcXPzC152dnSGVSssbtVyeHd+/H1u3bq3QzywsLKywfefl5WHdunUlVrr+/PPPMXjwYDRu3Bj79+9HTEwM/vvf/6J27dqYOXNmmfZtYmKCYcOGYcOGDShtgvyQkBC89957MDExgVQqRVBQEFauXKmW4yLSWmpfgIKIKtXw4cNFnz59lLaNHj1aODs7CyGE+PXXX0WTJk2EpaWlcHJyEkOHDhUPHjxQtH22hs/hw4dFkyZNhLGxsVi/fv0L17wBIPbs2aN4f2Jiohg8eLCoVq2aMDc3F02aNBHnzp0TQggxZ84c4evrq5Rt/fr1on79+kIqlYo33nhD/PDDDyof3/MAiJ9++kn07dtXmJmZiTp16oh9+/YptYmJiRGBgYHCwsJCODo6imHDhomHDx8qXm/fvr0IDg4WkyZNEtWrVxft2rUTQgixb98+UadOHWFqaio6dOggNmzYIACIJ0+eiOzsbGFlZSV27typ9Fn79+8X5ubmL1xQddeuXcLe3l5p29mzZwUAsWLFilLfI5fLS3yGv7+/kEqlwtPTU8ydO1cUFRUJIYS4fPmyACBOnDih9J5Tp04JAErrE504cUKYmJgoLeBIpG/Ys0Okh8zMzFBUVATgaQ/FV199hUuXLmHv3r1ISEjAiBEjSrxn2rRpWLhwIa5evYqAgABMmTIFDRs2VPSkDB48uMR7srOz0b59eyQlJWH//v24dOkSpk2bBrlcXmqun376CV988QXmz5+Pq1evYsGCBZg1axZ++eWX1z7mefPmYdCgQbh8+TJ69OiBd999F48fPwbwdOitffv2aNy4MS5evIjDhw/jwYMHGDRokNI+fvnlFxgZGeHvv//G2rVrcfv2bQwYMAB9+/ZFVFQUPvroI3zxxReK9hYWFhgyZAhCQkKU9hMSEoIBAwbAysqq1KynTp1C06ZNlbZt3boVlpaW+PTTT0t9j0QiUfz5yJEjGDZsGMaPH4/Y2FisXbsWGzZswPz58wEAPj4+aNasWYlc69evR/PmzeHt7a3Y1rRpUxQVFeH8+fOlfi6RXtB0tUVEr+f5no9//vlHVK9eXQwaNKjU9ufPnxcARFZWlhDifz07e/fuVWpXWq+MEMo9O2vXrhVWVlYvXIn5+X24u7uLLVu2KLX56quvRMuWLV96fIaGhsLCwkLp8eWXXypl+s9//qN4np2dLSQSifjjjz+EEELMmjVLBAQEKO03MTFRaTX49u3bi8aNGyu1+fzzz4W3t7fSti+++ELRsyPE079vQ0NDcf/+fSGEEA8fPhTGxsYlelX+rU+fPmLkyJFK27p37y4aNWqktG3p0qVKx/xsdey2bduKBQsWKLX99ddfhYuLi+L5mjVrhIWFheI8Z2VlCQsLC7F27doSeapVqyY2bNjwwrxEuo49O0R64Pfff4elpSVMTU3RsmVLtGvXTnEdRmRkJPr06QMPDw9YWVmhQ4cOAIC7d+8q7eP5noayiIqKgp+fH+zs7F7Z9uHDh0hMTMSoUaNgaWmpeHz99de4efPmS9/bsWNHREVFKT2Cg4OV2jRq1EjxZwsLC1hZWSE1NRUAEB4ejuPHjyt9bv369QFA6bOf/zuIi4tDs2bNlLY1b968xPOGDRti48aNAIBff/0VNWvWRLt27V54PHl5eTA1NS2x/d+9NwAwcuRIREVFYe3atcjJyVFcgxMeHo4vv/xS6XjGjBmD5ORk5ObmAgCGDh0KuVyO7du3AwC2b98OIQSGDBlS4nPNzMwU7yPSR0aaDkBEr69jx45Ys2YNjI2N4erqCmNjYwBATk4OAgICEBAQgE2bNsHBwQF3795Ft27dSlyAa2FhofLnmpmZlbnts6Gtn376CW+99ZbSa4aGhi99r4WFBerUqfPSNs+O+RmJRKL4TLlcjl69emHx4sUl3ufi4qL0Of8mhChRgIhSLvodPXo0Vq1ahenTpyMkJAQffPBBiff9m729PZ48eaK0rW7duggLC0NRUZHiWGxtbWFra4t79+4ptZXL5Zg3bx769etXYt/PiigbGxsMGDAAISEhGDVqlGJozdrausR7Hj9+DAcHhxfmJdJ17Nkh0gPPigEPDw+lL/1r167h0aNHWLRoEdq2bYv69esrejtexcTEBDKZ7KVtGjVqhKioKMW1MS/j5OQENzc33Lp1C3Xq1FF6eHp6lilTefn7+yMmJga1atUq8dkvK/Lq16+PCxcuKG27ePFiiXbDhg3D3bt38f333yMmJgbDhw9/aR4/Pz/ExsYqbRs6dCiys7OxevXqMh1PXFxciWOpU6cODAz+98/6qFGj8Pfff+P333/H33//jVGjRpXY182bN5Gfnw8/P79Xfi6RrmKxQ6THatasCRMTE6xcuRK3bt3C/v378dVXX5XpvbVq1UJCQgKioqLw6NEjFBQUlGgzdOhQODs7o2/fvvj7779x69Yt7Nq1C2fPni11n3PnzsXChQuxYsUKXL9+HVeuXEFISAiWLVv20iwFBQVISUlRejx69KhMxwEAwcHBePz4MYYOHYrz58/j1q1bOHr0KEaOHPnSgu6jjz7CtWvX8Pnnn+P69evYsWMHNmzYAEB5yKlatWro168fPvvsMwQEBKBGjRovzdOtWzfExMQo9e60bNkSU6ZMwZQpUzB58mSEhYXhzp07OHfuHNatWweJRKIoZGbPno2NGzdi7ty5iImJwdWrV7F9+3b85z//Ufqc9u3bo06dOnj//fdRp06dUofWTp8+DS8vL9SuXfuVf49EuorFDpEec3BwwIYNG7Bz5040aNAAixYtwrffflum9/bv3x/du3dHx44d4eDgUOq8NiYmJjh69CgcHR3Ro0cP+Pj4YNGiRS8clho9ejR+/vlnbNiwAT4+Pmjfvj02bNjwyp6dw4cPw8XFRenRpk2bMh0HALi6uuLvv/+GTCZDt27d4O3tjQkTJsDGxkapJ+R5np6e+O2337B79240atQIa9asUdyN9fxcQ6NGjUJhYSFGjhz5yjw+Pj5o2rQpduzYobT922+/xZYtWxAZGYmePXuibt26GDhwIORyOc6ePasYgurWrRt+//13hIaGolmzZmjRogWWLVsGDw+PEp81cuRIPHny5IW5tm7dijFjxrwyM5Euk4jSBqCJiKhU8+fPx48//ojExESl7Zs3b8aECROQlJQEExOTV+7n0KFDmDp1KqKjo19acFWk6OhodO7cGdevX4eNjY1GMhBVBl6gTET0EqtXr0azZs1QvXp1/P333/jmm28wduxYxeu5ublISEjAwoUL8dFHH5Wp0AGAHj16ID4+Hvfv34e7u3tFxX+ppKQkbNy4kYUO6T327BARvcSkSZOwfft2PH78GDVr1sR7772HGTNmwMjo6f8V586di/nz56Ndu3bYt28fLC0tNZyYiJ7HYoeIiIj0Gi9QJiIiIr3GYoeIiIj0GosdIiIi0mssdoiIiEivsdghIiIivcZih4iIiPQaix0iIiLSayx2iIiISK+x2CEiIiK99n9muB2V5ZpQugAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(true_peaks,peak_preds)\n",
    "plt.xlabel('Particle Energy (GeV)')\n",
    "plt.ylabel('Reconstructed Energy (GeV)')\n",
    "plt.plot(np.arange(1,201),np.arange(1,201))\n",
    "plt.title('Linearity')\n",
    "plt.savefig(\"linreg_linearity.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get energy resolution from distribution of predictions\n",
    "def res(preds,energy):\n",
    "    return norm.fit(preds)[1]/energy\n",
    "\n",
    "energy_list = [200,100,50,10]\n",
    "resolutions = res(y_preds_200GeV,200), res(y_preds_100GeV,100), res(y_preds_50GeV,50), res(y_preds_10GeV,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curve fit for energy resolution as a function of energy\n",
    "def f(E,a):\n",
    "    return a/np.sqrt(E)\n",
    "\n",
    "popt, pcov = curve_fit(f, energy_list, resolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.94902746]), array([[0.00073691]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Include gaussian fit in loss fn\n",
    "\n",
    "popt, pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(200),f(range(1,201),popt[0]))\n",
    "plt.scatter(energy_list,resolutions)\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Resolution')\n",
    "plt.title('Energy Resolution')\n",
    "plt.savefig(\"linreg_res.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model_0.state_dict, f=\"/home/dmisra/eic/model_0\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0ad538c402c22927254208aecd010220eb94d12e6a2da33653e1d12172e2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
